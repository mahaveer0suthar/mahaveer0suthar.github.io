<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/mahaveer0suthar.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/mahaveer0suthar.github.io/" rel="alternate" type="text/html" /><updated>2020-02-13T19:38:52+05:30</updated><id>http://localhost:4000/mahaveer0suthar.github.io/feed.xml</id><title type="html">Mahaveer Senior Deep Learning Engineer</title><subtitle>Page of a deep learning's enthuasiast.</subtitle><author><name>Mahaveer Suthar</name><email>mahaveer0suthar@gmail.com</email></author><entry><title type="html">What I talk when I talk about Tensorflow</title><link href="http://localhost:4000/mahaveer0suthar.github.io/tutorial/What-I-talk-about-Tensorflow/" rel="alternate" type="text/html" title="What I talk when I talk about Tensorflow" /><published>2018-11-02T00:00:00+05:30</published><updated>2018-11-02T00:00:00+05:30</updated><id>http://localhost:4000/mahaveer0suthar.github.io/tutorial/What-I-talk-about-Tensorflow</id><content type="html" xml:base="http://localhost:4000/mahaveer0suthar.github.io/tutorial/What-I-talk-about-Tensorflow/">&lt;p&gt;Some of my collegues, as well as many of my readers told me that they had problems using Tensorflow for their projects. Something like this:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Hey Mahaveer, what is the difference between tf.contrib.layers and tf.layers? Oh, and what is the TF-Slim thing? And now we have the godd*** tf.estimator. What are all these for? What are we supposed to use?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To be honest, when I started using Tensorflow, I was in that situation too. Tensorflow was already pretty bulky back then, and to make things even worse, it just kept getting bigger and bigger. If you don’t believe me, just look at the size of the current installation package and compare it to previous versions.&lt;/p&gt;

&lt;p&gt;But wait! I just got an idea. Why not create a series of blog posts about Tensorflow ;)&lt;/p&gt;

&lt;h3 id=&quot;objectives&quot;&gt;Objectives&lt;/h3&gt;
&lt;hr /&gt;
&lt;p&gt;Let’s talk about what we’re gonna focus on in this post. I learned this thing the hardest way, guys. I think I will make a post about what I learned from writing technical blog posts, and one of them is: do talk about the objectives first!&lt;/p&gt;

&lt;p&gt;So, here’s what we will do in this post:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Address some confusing problems of Tensorflow&lt;/li&gt;
  &lt;li&gt;Understand the mostly used Tensorflow modules&lt;/li&gt;
  &lt;li&gt;(Optional) Get out hands dirty with some easy code!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Okay, let’s tackle them one by one!&lt;/p&gt;

&lt;h2 id=&quot;common-problems&quot;&gt;Common Problems&lt;/h2&gt;
&lt;p&gt;Before diving in the details, I think I should list out the most common problems that we might face when using Tensorflow:&lt;/p&gt;
&lt;h3 id=&quot;dont-know-how-to-start&quot;&gt;Don’t know how to start&lt;/h3&gt;

&lt;p&gt;Whether it comes to importing data, creating the model or visualizing the results, we usually get confused. Technically, there are so many ways to do the exact same thing in Tensorflow. And it’s the urge of doing things in the most proper way that drives us crazy.&lt;/p&gt;

&lt;h3 id=&quot;dont-know-what-to-do-when-things-go-wrong&quot;&gt;Don’t know what to do when things go wrong&lt;/h3&gt;

&lt;p&gt;I think this is the problem that a lot of you guys can relate to. In Tensorflow, we must first define the computation graph. Not only doing this way prevents us from modifying the graph when it’s running (sometimes we just want it to be dynamic), but it also does a good job at hiding things from us, which we can’t know what the hell under the hood is causing the trouble. We are the Python guys, we want things to be Pythonic!&lt;/p&gt;

&lt;h2 id=&quot;tensorflows-vocabulary&quot;&gt;Tensorflow’s Vocabulary&lt;/h2&gt;
&lt;p&gt;As I said above, one problem with Tensorflow is that there are a lot of ways to do the exact same thing. Even experienced users find it confusing sometimes.&lt;/p&gt;

&lt;p&gt;Below, I’m gonna list out some “concepts” that are mostly confusing to beginners:&lt;/p&gt;

&lt;h3 id=&quot;low-level-api&quot;&gt;Low-level API&lt;/h3&gt;

&lt;p&gt;It used to be how we did everything in Tensorflow when it first came out. Want to create a fully connected layer? Create some weights, some biases and roll them in!&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with tf.variable_scopes('fc_layer'):
  weights = tf.get_variables('weights', [5, 5, 3, 64],
                             initializer=tf.initializers.truncated_normal(stddev=5e-2))
  biases = tf.get_variables('biases', [64],
                            initializer=tf.zeros_initializer)
  output = tf.nn.conv2d(inputs, weights, [1, 1, 1, 1], padding='SAME')
  output = tf.nn.bias_add(conv, biases)
  output = tf.nn.relu(output)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;tfcontrib&quot;&gt;tf.contrib&lt;/h3&gt;

&lt;p&gt;You are likely to come across tf.contrib.layers a lot. Basically, it’s backed by the large community of Tensorflow (contrib stands for contribution) and it contains experimental implementation, such as a newly introduced layer, a new optimization method, or just wrappers for low-level API, etc. Although they are technically just experimental codes, they actually work very well and will be merged to Tensorflow’s core code in the future.&lt;/p&gt;

&lt;h3 id=&quot;tflayers&quot;&gt;tf.layers&lt;/h3&gt;

&lt;p&gt;As its name is self explained, this is the package for defining layers. We can think of it as an official version of tf.contrib.layers. They basically do the same job: to make defining layers less tiresome. Using tf.contrib.layers or tf.layers to create a conv2d layer like we did above, we now need only one line:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;output = tf.contrib.layers.conv2d(inputs, 64, [5, 5],
                                  weights_initializer=tf.initializers.truncated_normal(stddev=5e-2))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Or with tf.layers:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;output = tf.layers.conv2d(inputs, 64, [5, 5],
                          padding='same',
                          kernel_initializer=tf.initializers.truncated_normal(stddev=5e-2))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I bet you wouldn’t create any layers by hand from now on!&lt;/p&gt;

&lt;h3 id=&quot;tfcontribslim-or-tf-slim&quot;&gt;tf.contrib.slim (or TF-Slim)&lt;/h3&gt;

&lt;p&gt;Okay, this may be the most confusing one. At first, I thought that was the light-weight version of Tensorflow but soon enough, I realized I was wrong. &lt;strong&gt;slim&lt;/strong&gt; only stands for fewer lines to do the same thing (comparing with low-level API). For example, to create not only one, but three conv2d layers, we only need to write one line:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;slim = tf.contrib.slim
output = slim.repeat(inputs, 3, slim.conv2d, 64, [5, 5])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Other than that though, tf.slim can help you build an entire pipeline for training the network, which means they have some functions to help you get the losses, train or evaluate the model, etc.&lt;/p&gt;

&lt;p&gt;Overall, TF-Slim may be a good option for fast experimenting new idea (Tensorflow’s research uses TF-Slim for building the networks). What you need to take into account is, TF-Slim’s codes actually came from tf.contrib (e.g. slim.conv2d is just an alias for tf.contrib.layers.conv2d), so there’s no magic here.&lt;/p&gt;

&lt;h3 id=&quot;tfkeras&quot;&gt;tf.keras&lt;/h3&gt;

&lt;p&gt;This is legend! Keras came out when we had to write everything using low-level API. So technically it was used as the high-level API for Tensorflow. In my opinion, it did help make the community (especially researchers) adopt Tensorflow. And since Keras is officially a module of Tensorflow (from version 1.0 I think), you don’t have to worry about version compatibility any more.&lt;/p&gt;

&lt;p&gt;The great thing about Keras is, it does all the hard tasks for you. So going from idea to result is just a piece of cake. Want to create a network? Just stack up the layers! Want to train it? Just compile and call fit!&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = Sequential()
model.add(Dense(out_size, activation='relu', input_shape=(in_size,)))
# Add more here
model.add(...)

# Compile
model.compile(loss='categorical_crossentropy', optimizer=SGD())

# Train
model.fit(x=inputs, y=labels)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Although Keras is super convenient, especially for those who don’t like to write code, it abstracts so many things from us. François Chollet, the author of Keras, claimed that Keras will act like an interface only, but it does have some constraints which may confuse you sometimes (Want model.fit to compute validation loss after a specific number of batches? It can’t!). You may also have hard time implementing newly introduced deep-learning papers entirely by Keras since they require some minor tweaks within some layer.&lt;/p&gt;

&lt;h3 id=&quot;eager-execution&quot;&gt;Eager Execution&lt;/h3&gt;

&lt;p&gt;As I mentioned earlier, when implementing in Tensorflow, you must first define all the operations to form a graph. It’s not until the graph is finalized (it’s locked, no more in, no more out, no more update) that you can run it to see the results. Because of this, Tensorflow is hard to debug and incapable of creating dynamic graph.&lt;/p&gt;

&lt;p&gt;So Eager Execution came out to help deal with these problems. The name is kind of weird though. I interprete it as “can’t wait to execute”. With the additional 2 new lines, you can now do something like: evaluate the new created variable (which seems trivial but used to be impossible in Tensorflow):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tf.enable_eager_execution()
tf.executing_eagerly()
import tensorflow.contrib.eager as tfe

weights = tfe.Variable(tf.truncated_normal(shape=[2, 3], stddev=5e-2), name='weights')
weights
&amp;lt;tf.Variable 'weights:0' shape=(2, 3) dtype=float32, numpy=
array([[ 0.06691323, -0.01890625, -0.00283119],
       [-0.0536754 ,  0.00109388, -0.04310168]], dtype=float32)&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Rumor has it Eager Execution is gonna be set to default from Tensorflow 2.0. I think this move will please a lot of Tensorflow fans out there. But please bear in mind that at the moment, not everything is gonna work in Eager Execution mode (yet). So while we’re waiting for Tensorflow 2.0 to be released, it’s a good idea to stay updated to the latest news from Tensorflow’s team and Google.&lt;/p&gt;

&lt;h2 id=&quot;optional-lets-play-with-tensors&quot;&gt;(Optional) Let’s play with Tensors!&lt;/h2&gt;
&lt;p&gt;Okay guys, this is an optional section. We’re gonna see if different approaches produce exactly the same results. We’re gonna create a “real” convolution2d layer, including activation functions and regularization terms, by using tf.contrib.layers and tf.layers. We will check the similarity among their results by checking the variables and operations that they created.&lt;/p&gt;

&lt;p&gt;Oh hold on! There’s one more thing I want you to pay attention to. I will write out all the arguments whether some of them have default values. The reason is, the two modules’ conv2d functions set the default values differently for the same terms! For example, padding is set to ‘SAME’ by default in tf.contrib.layers.conv2d, but ‘valid’ in case of tf.layers.conv2d.&lt;/p&gt;

&lt;p&gt;Now we’re ready to move on.&lt;/p&gt;

&lt;h3 id=&quot;tfcontriblayers&quot;&gt;tf.contrib.layers&lt;/h3&gt;

&lt;p&gt;Let’s start with tf.contrib.layers (tf.contrib is technically big and capable of a lot of things, so by saying tf.contrib.layers here seems more appropriate).&lt;/p&gt;

&lt;p&gt;I don’t want to think of the amount of work to achieve the same result by using low-level API. That’s why having any kinds of high-level API will save us a ton of time and effort. Not only researchers, developers do love high-level APIs!&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# The inputs we use is one image of shape (224, 224, 3)
inputs = tf.placeholder(tf.float32, [1, 224, 224, 3])

conv2d = tf.contrib.layers.conv2d(inputs=inputs,
                                  num_outputs=64,
                                  kernel_size=3,
                                  stride=1, 
                                  padding='SAME', 
                                  activation_fn=tf.nn.relu,
                                  weights_initializer=tf.initializers.truncated_normal(stddev=0.01),
                                  weights_regularizer=tf.contrib.layers.l2_regularizer(0.005),
                                  biases_initializer=tf.zeros_initializer())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;tflayers-1&quot;&gt;tf.layers&lt;/h3&gt;

&lt;p&gt;Next, let’s see how we can create a convolution2d layer with tf.layers, an official modules by the core team of Tensorflow ;) Obviously we expect that it can produce the same result, with less or (at least) similar effort.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conv2d = tf.layers.conv2d(inputs=inputs,
                          filters=64,
                          kernel_size=3,
                          strides=1,
                          padding='same',
                          activation=tf.nn.relu,
                          kernel_initializer=tf.initializers.truncated_normal(stddev=0.01),
                          kernel_regularizer=tf.contrib.layers.l2_regularizer(0.005),
                          bias_initializer=tf.zeros_initializer())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It’s time to compare the results. Did both of tf.contrib and tf.layers produce the layers with similar functionality? Did one of them do more than the other?&lt;/p&gt;

&lt;p&gt;First, let’s consider the variables created by above commands. You can use the method &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.global_variables()&lt;/code&gt; to get all variables in the current graph.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Variables created by tf.contrib.layers.conv2d
[&amp;lt;tf.Variable 'Conv/weights:0' shape=(3, 3, 3, 64) dtype=float32_ref&amp;gt;, &amp;lt;tf.Variable 'Conv/biases:0' shape=(64,) dtype=float32_ref&amp;gt;]

# Variables created by tf.contrib.layers.conv2d
[&amp;lt;tf.Variable 'conv2d/kernel:0' shape=(3, 3, 3, 64) dtype=float32_ref&amp;gt;, &amp;lt;tf.Variable 'conv2d/bias:0' shape=(64,) dtype=float32_ref&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Phew, the variable sets are similar. They both created a weights Tensor, and a biases Tensor with the same shape. Notice that their names are slightly different, though.&lt;/p&gt;

&lt;p&gt;Next, let’s check if the two functions generated different sets of operations. The command we can use is 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.get_default_graph().get_operations()&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Operations created by tf.contrib.layers.conv2d
&amp;lt;tf.Operation 'Placeholder' type=Placeholder&amp;gt;, 
&amp;lt;tf.Operation 'Conv/weights/Initializer/truncated_normal/shape' type=Const&amp;gt;, 
&amp;lt;tf.Operation 'Conv/weights/Initializer/truncated_normal/mean' type=Const&amp;gt;, 
&amp;lt;tf.Operation 'Conv/weights/Initializer/truncated_normal/stddev' type=Const&amp;gt;, 
&amp;lt;tf.Operation 'Conv/weights/Initializer/truncated_normal/TruncatedNormal' type=TruncatedNormal&amp;gt;, 
&amp;lt;tf.Operation 'Conv/weights/Initializer/truncated_normal/mul' type=Mul&amp;gt;, 
&amp;lt;tf.Operation 'Conv/weights/Initializer/truncated_normal' type=Add&amp;gt;, 
&amp;lt;tf.Operation 'Conv/weights' type=VariableV2&amp;gt;, 
&amp;lt;tf.Operation 'Conv/weights/Assign' type=Assign&amp;gt;, 
&amp;lt;tf.Operation 'Conv/weights/read' type=Identity&amp;gt;, 
&amp;lt;tf.Operation 'Conv/kernel/Regularizer/l2_regularizer/scale' type=Const&amp;gt;, 
&amp;lt;tf.Operation 'Conv/kernel/Regularizer/l2_regularizer/L2Loss' type=L2Loss&amp;gt;, 
&amp;lt;tf.Operation 'Conv/kernel/Regularizer/l2_regularizer' type=Mul&amp;gt;, 
&amp;lt;tf.Operation 'Conv/biases/Initializer/zeros' type=Const&amp;gt;, 
&amp;lt;tf.Operation 'Conv/biases' type=VariableV2&amp;gt;, 
&amp;lt;tf.Operation 'Conv/biases/Assign' type=Assign&amp;gt;, 
&amp;lt;tf.Operation 'Conv/biases/read' type=Identity&amp;gt;, 
&amp;lt;tf.Operation 'Conv/dilation_rate' type=Const&amp;gt;, 
&amp;lt;tf.Operation 'Conv/Conv2D' type=Conv2D&amp;gt;, 
&amp;lt;tf.Operation 'Conv/BiasAdd' type=BiasAdd&amp;gt;, 
&amp;lt;tf.Operation 'Conv/Relu' type=Relu&amp;gt;

# Operations created by tf.layers.conv2d
&amp;lt;tf.Operation 'Placeholder' type=Placeholder&amp;gt;, 
&amp;lt;tf.Operation 'conv2d/kernel/Initializer/truncated_normal/shape' type=Const&amp;gt;, 
&amp;lt;tf.Operation 'conv2d/kernel/Initializer/truncated_normal/mean' type=Const&amp;gt;, 
&amp;lt;tf.Operation 'conv2d/kernel/Initializer/truncated_normal/stddev' type=Const&amp;gt;, 
&amp;lt;tf.Operation 'conv2d/kernel/Initializer/truncated_normal/TruncatedNormal' type=TruncatedNormal&amp;gt;, 
&amp;lt;tf.Operation 'conv2d/kernel/Initializer/truncated_normal/mul' type=Mul&amp;gt;, 
&amp;lt;tf.Operation 'conv2d/kernel/Initializer/truncated_normal' type=Add&amp;gt;, 
&amp;lt;tf.Operation 'conv2d/kernel' type=VariableV2&amp;gt;, 
&amp;lt;tf.Operation 'conv2d/kernel/Assign' type=Assign&amp;gt;, 
&amp;lt;tf.Operation 'conv2d/kernel/read' type=Identity&amp;gt;, 
&amp;lt;tf.Operation 'conv2d/kernel/Regularizer/l2_regularizer/scale' type=Const&amp;gt;, 
&amp;lt;tf.Operation 'conv2d/kernel/Regularizer/l2_regularizer/L2Loss' type=L2Loss&amp;gt;, 
&amp;lt;tf.Operation 'conv2d/kernel/Regularizer/l2_regularizer' type=Mul&amp;gt;, 
&amp;lt;tf.Operation 'conv2d/bias/Initializer/zeros' type=Const&amp;gt;, 
&amp;lt;tf.Operation 'conv2d/bias' type=VariableV2&amp;gt;, 
&amp;lt;tf.Operation 'conv2d/bias/Assign' type=Assign&amp;gt;, 
&amp;lt;tf.Operation 'conv2d/bias/read' type=Identity&amp;gt;, 
&amp;lt;tf.Operation 'conv2d/dilation_rate' type=Const&amp;gt;, 
&amp;lt;tf.Operation 'conv2d/Conv2D' type=Conv2D&amp;gt;, 
&amp;lt;tf.Operation 'conv2d/BiasAdd' type=BiasAdd&amp;gt;, 
&amp;lt;tf.Operation 'conv2d/Relu' type=Relu&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, what is the verdict? As we can observe above. Using tf.contrib or tf.layers will save us a lot of time and prevent us from headache later on. Moreover, they create absolutely similar things. What does that mean to us? It means that it doesn’t matter what your preferred module is, you can create/re-create any networks or you can even use the weights trained by the code written on the other module.&lt;/p&gt;

&lt;p&gt;But hey, you can’t see that the names are obviously different, can you? You might ask. As long as the shapes and types of variables are the same, mapping the names between the two variable sets is not that painful task. In fact, it’s just no more than 5 lines of code and yeah, you only need to know how to do it. As we addressed the problems earlier in this post, Tensorflow is not hard, it is just kind of confusing.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Oh, that was so long. Thank you guys for reading. Before we say goodbye, let’s take a look at what we did in this post:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;We discussed why Tensorflow may seem confusing&lt;/li&gt;
  &lt;li&gt;We talked about heavily in-use Tensorflow module&lt;/li&gt;
  &lt;li&gt;We checked if different modules produce different results on the same task&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This post is no more than an entry point, some kind of what-I-would-talk-about-when-I-talk-about-Tensorflow (I borrowed that title from Haruki Murakami, check it &lt;a href=&quot;https://www.amazon.com/What-About-Running-Vintage-International-ebook/dp/B0015DWJ8W&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Personally, I am a big fan of learning-by-doing style. Even in the deep-learning field which seems way too deeply academic, it can work out well. In the future posts, I will guide you through how we can accomplish the most common tasks with Tensorflow. Those won’t make you a deep learning guru, but with a solid understanding about how to use the proper tool, and with some practice from your own, what can stop you from building amazing things?&lt;/p&gt;

&lt;p&gt;Okay, I might have exaggerated a little bit, but honestly I hope that I can make something that you guys can benefit from. So, I’m gonna see you soon, in the next blog post of this Tensorflow series.&lt;/p&gt;</content><author><name>Mahaveer Suthar</name><email>mahaveer0suthar@gmail.com</email></author><category term="machine-learning" /><category term="deep-learning" /><category term="keras" /><category term="tensorflow" /><category term="tutorials" /><category term="gpu" /><category term="note" /><summary type="html">Some of my collegues, as well as many of my readers told me that they had problems using Tensorflow for their projects. Something like this:</summary></entry><entry><title type="html">Installing NVIDIA Docker On Ubuntu 16.04</title><link href="http://localhost:4000/mahaveer0suthar.github.io/project/Installing-NVIDIA-Docker-On-Ubuntu-16.04/" rel="alternate" type="text/html" title="Installing NVIDIA Docker On Ubuntu 16.04" /><published>2018-02-04T00:00:00+05:30</published><updated>2018-02-04T00:00:00+05:30</updated><id>http://localhost:4000/mahaveer0suthar.github.io/project/Installing-NVIDIA-Docker-On-Ubuntu-16.04</id><content type="html" xml:base="http://localhost:4000/mahaveer0suthar.github.io/project/Installing-NVIDIA-Docker-On-Ubuntu-16.04/">&lt;p&gt;Hey guys, it has been quite a long while since my last blog post (for almost a year, I guess). Today, I am going to tell you about something that I wish I had known before: NVIDIA Docker.&lt;/p&gt;

&lt;h3 id=&quot;what-is-docker-and-what-is-nvidia-docker&quot;&gt;What is Docker? And what is NVIDIA Docker?&lt;/h3&gt;
&lt;p&gt;When you come across the term &lt;strong&gt;Docker&lt;/strong&gt; somewhere, you may feel (just like I did) a little bit confused about what it is, and why we should consider using it.
That’s totally understandable if you are just machine learning enthusiasts like I was before, which you may not need to care about managing resources, setting up development/testing/production environment or something like that.
But what if you are serious about getting a career in this field, working on some large scale projects with a team? It is likely that you and your guys will end up sharing one “bucky” supermachine (such as DGX-1), rather than using your own machine. Because of that, it will be best if you know about Docker or at least know how to use it!&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;So, what is Docker? I’m not gonna rewrite any kind of long explanation since you can get it from Docker’s page. In short (which is why it may not be absolutely precise), Docker is a software which help you create a virtual Linux/Windows environment called &lt;strong&gt;container&lt;/strong&gt;, which you can then develop and run applications on.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/installing-nvidia-docker/what_is_container.PNG&quot; alt=&quot;what_is_container&quot; /&gt;
(Image captured from Docker’s page)&lt;/p&gt;

&lt;p&gt;“Hey, isn’t it what VM (Virtual Machine) does?” - You may ask. Docker containers and VMs generally can do the same thing. For instance, you want to run Windows application on Ubuntu? Well, you can use Docker to create a Windows container, and VM can do the trick as well.&lt;/p&gt;

&lt;p&gt;To answer to the question above, please take a look at the picture below which I took from Docker’s page. They even show us the difference, don’t they?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/installing-nvidia-docker/docker_vs_vm.PNG&quot; alt=&quot;docker_vs_vm&quot; /&gt;
(Image captured from Docker’s page)&lt;/p&gt;

&lt;p&gt;What we can see from the picture is that, using VM requires creating a full copy of the OS for each application, whereas within Docker, many containers can share the same OS while running their own applications, which is the reason why:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Docker takes less hard disk space&lt;/li&gt;
  &lt;li&gt;Docker takes less RAM to operate&lt;/li&gt;
  &lt;li&gt;Docker containers can boot within seconds&lt;/li&gt;
  &lt;li&gt;Containerized software will always run the same, regardless of the environment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For a more detailed comparison, you can take a look at this thread on StackOverflow: &lt;a href=&quot;https://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine&quot; target=&quot;_blank&quot;&gt;How is Docker different from VM?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Last, but not least, what about NVIDIA Docker???&lt;/p&gt;

&lt;p&gt;Well, it’s a pretty tedious task to get your graphic cards working on virtual machines, and the same thing happens when using Docker too. And then NVIDIA Docker, which is simply a plugin to Docker, came out and turn that task into just a piece of cake! And that’s it, even the name explains itself, right?&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;installing-nvidia-docker&quot;&gt;Installing NVIDIA docker&lt;/h3&gt;
&lt;p&gt;So now you may got some idea about what Docker is, let’s get into the most important part: installing NVIDIA Docker.
As I said in the previous part, NVIDIA docker is just a plugin to docker, which makes GPU accessible from inside docker’s containers. Therefore, installing NVIDIA docker consists of three steps like below:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Installing NVIDIA driver&lt;/li&gt;
  &lt;li&gt;Installing docker&lt;/li&gt;
  &lt;li&gt;Installing NVIDIA docker&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;installing-nvidia-driver&quot;&gt;Installing NVIDIA driver&lt;/h4&gt;
&lt;p&gt;NVIDIA driver is simply the necessary driver to use your GPU. You only need to install NVIDIA driver to your PC in order to use NVIDIA docker, which is a big advantage of docker.
How to install NVIDIA driver depends on what Linux distribution you are using. And NVIDIA has a very detailed step-by-step instructions for all Linux distributions, so I think I should leave this part for you guys :) It’s very easy, don’t panic. My one piece of advice is: the newer the driver is, the better! The reason is that, newer version of CUDA toolkit may not work with old version of NVIDIA driver.
Here is the link to the instructions: &lt;a href=&quot;http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html&quot; target=&quot;_blank&quot;&gt;CUDA Installation Guide&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;installing-docker&quot;&gt;Installing docker&lt;/h4&gt;
&lt;p&gt;Next, we will install docker. Docker has two available editions: Community Edition (CE) and Enterprise Edition (EE). And just like NVIDIA driver, you need to know what Linux distribution you are using to choose the proper installation file.
Below is the installing instructions for Docker Community Edition on Ubuntu (the OS I am using, of course).&lt;/p&gt;

&lt;p&gt;Firstly, you need to remove (if any) old version of docker. If you can assure that this is the first time you install docker on your machine, then you can skip to the next step. Otherwise, you’d better run the following command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-get remove docker docker-engine docker.io
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If docker is not installed on your machine, then apt-get will tell you that. It is totally fine.&lt;/p&gt;

&lt;p&gt;Next, we will install docker. I recommend installing docker using the repository so that when the new version is available, you can get the update from repository easily.&lt;/p&gt;

&lt;p&gt;To installing from repository, we need to set up the Docker repository first. As usual, you may want to update the &lt;em&gt;apt&lt;/em&gt; package index:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-get update
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, install the packages to allow &lt;em&gt;apt&lt;/em&gt; to use a repository through HTTPS:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    software-properties-common
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, add the official GPG key of Docker:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Verify that the command below print out 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-key fingerprint 0EBFCD88
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, tell &lt;em&gt;apt&lt;/em&gt; to use the stable repository by run the command below:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo add-apt-repository \
   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this point, we have finished set up the repository. Next, we will update the &lt;em&gt;apt&lt;/em&gt; package index and install Docker CE:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-get update &amp;amp;&amp;amp; apt-get install docker-ce
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, we will check if Docker is installed correctly by running the well-known hello-world Image:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo docker run hello-world
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Your screen should print out something like below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/installing-nvidia-docker/docker-hello-world.png&quot; alt=&quot;docker-hello-world&quot; /&gt;&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;installing-nvidia-docker-1&quot;&gt;Installing NVIDIA docker&lt;/h4&gt;
&lt;p&gt;In the next step, we will finish our job by installing NVIDIA docker, which is just a plug in of Docker to help container use the GPUs of the host machine.&lt;/p&gt;

&lt;p&gt;First, you need to remove NVIDIA docker 1.0 (if installed):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker volume ls -q -f driver=nvidia-docker | xargs -r -I{} -n1 docker ps -q -a -f volume={} | xargs -r docker rm -f
sudo apt-get purge -y nvidia-docker
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, we will add the necessary repository, then update the &lt;em&gt;apt&lt;/em&gt; package index:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \
  sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/ubuntu16.04/amd64/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We are nearly there, next we will install NVIDIA docker:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install -y nvidia-docker2
sudo pkill -SIGHUP dockerd
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That’s it. We have installed NVIDIA docker. Let’s verify the installation by running the latest CUDA image, which is officially provided by NVIDIA:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If this is the first time you run the command above, you might notice that Docker is trying to download something like below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/installing-nvidia-docker/docker-image-download.png&quot; alt=&quot;docker-image-download&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Remember that Docker somehow works the same way as VM? It means that in order to create Containers, Docker first needs an Image too. And where is it getting the Image from? Well, I will make it clear in the next blog post. After the Image is downloaded and the command is executed, you will see something like that:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/installing-nvidia-docker/nvidia-docker-test.png&quot; alt=&quot;nvidia-docker-test&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The command above should print out the information of your host machine’s GPU(s) (as the nvidia-smi command usually does).&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;Today I have introduced to you NVIDIA Docker, which I personally think that every deep learning developer should know about and get used to. And I hope you all successfully installed NVIDIA Docker. Some of you may not be convinced yet, which is why I am creating another post about some practical use cases as well as some tips on using Docker. So how do we utilize Docker to be more efficient at work? Wait for the next blog post! See you!&lt;/p&gt;</content><author><name>Mahaveer Suthar</name><email>mahaveer0suthar@gmail.com</email></author><category term="machine-learning" /><category term="deep-learning" /><category term="docker" /><category term="nvidia" /><category term="nvidia-docker" /><category term="gpu" /><category term="note" /><summary type="html">Hey guys, it has been quite a long while since my last blog post (for almost a year, I guess). Today, I am going to tell you about something that I wish I had known before: NVIDIA Docker.</summary></entry><entry><title type="html">Creating A Language Translation Model Using Sequence To Sequence Learning Approach</title><link href="http://localhost:4000/mahaveer0suthar.github.io/project/Sequence-To-Sequence/" rel="alternate" type="text/html" title="Creating A Language Translation Model Using Sequence To Sequence Learning Approach" /><published>2017-12-20T00:00:00+05:30</published><updated>2017-12-20T00:00:00+05:30</updated><id>http://localhost:4000/mahaveer0suthar.github.io/project/Sequence-To-Sequence</id><content type="html" xml:base="http://localhost:4000/mahaveer0suthar.github.io/project/Sequence-To-Sequence/">&lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;p&gt;Hello guys. It’s been quite a long while since my last blog post. It may sound like an excuse, but I’ve been struggling with finding a new place to move into. And I had to say, it’s a real problem for a foreigner to find a reasonable apartment in Japan. Luckily, I somehow managed to find one, and I have just moved in for nearly two weeks. Anyway, the toughest time has gone, and now I can get myself back to work, to bring to you guys new interesting (and maybe boring as usual) blog posts on Deep Learning.&lt;/p&gt;

&lt;p&gt;So, in my previous blog post, I told you about how to create a simple text generator by training a Recurrent Neural Network model. What RNNs differ from normal Neural Networks is, instead of computing the output prediction on each input independently, RNNs compute the output of timestep \(t\) using not only the input of timestep \(t\), but also involving the input of previous timesteps (say, timestep \(t-1\), \(t-2\), \(\dots\)).&lt;/p&gt;

&lt;p&gt;As you already saw in my previous post, inputs are actually sequences of characters, and each output was simply the corresponding input shifted by one character to the right. Obviously, you can see that each pair of input sequence and output sequence has the same length. Then, the network was trained using the famous Harry Potter as training dataset and as a result, the trained model could generate some great J.K. Rowling-style paragraphs. If you haven’t read my previous post yet, please take a look at it by the link below (make sure you do before moving on):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://mahaveer0suthar.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/&quot; target=&quot;_blank&quot;&gt;Creating A Text Generator Using Recurrent Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But here comes a big question: What if input sequence and output sequence have different lengths?&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;You know, there are many types of Machine Learning problems out there where input and output sequences don’t necessarily have the exact same length. And in terms of Natural Language Processing (or NLP for short), you are more likely to face problems where their lengths are totally different, not only between each pair of input and output sequence, but also between input sequences themselves! For example, in building a language translation model, each pair of input and output sequence are in different languages, so there’s a big chance that they don’t have the same length. Moreover, I can bet my life that there is no known language where we can create all sentences with the exact same length! Obviously, that is a really big big problem, because the model I showed you in the last post required all the input and output sequences have the same length. Sounds impossible, huh?&lt;/p&gt;

&lt;p&gt;The answer is: NO. Big problems only got smart people attracted, and as a result, solved by not only one, but many solutions. Let’s go back to our problem. A lot of attempts were made, each of them has its own advantages and disadvantages when compared to others. And in today’s post, I will introduce to you one approach which received great attention from NLP community: The Sequence To Sequence Networks (or seq2seq for short), great work by Ilya Sutskever, Oriol Vinyals, Quoc V. Le from Google.&lt;/p&gt;

&lt;p&gt;I will talk briefly about the idea behind seq2seq right below. For ones who want to understand deeply about the state-of-the-art model, please refer to the link to the paper at the end of this post.&lt;/p&gt;

&lt;p&gt;At this point, we have already known the problem we must deal with, that we have input and output sequences of different lengths. To make the problem become more concrete, let’s take a look at the graph below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/sequence-to-sequence/figure.png&quot; alt=&quot;figure&quot; /&gt;&lt;br /&gt;
(Image cut from the original paper of Sequence to Sequence Learning with Neural Networks)&lt;/p&gt;

&lt;p&gt;As illustrated in the graph above, we have “ABC” as the input sequence, and “WXYZ” as the output sequence. Obviously, the lengths of the two sequences are different. So, how does seq2seq approach to solve that problem? The answer is: they create a model which consists of two seperate recurrent neural networks called &lt;strong&gt;Encoder&lt;/strong&gt; and &lt;strong&gt;Decoder&lt;/strong&gt; respectively. To make it easy for you, I drew a simple graph below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/sequence-to-sequence/encode_decode.png&quot; alt=&quot;encode_decode&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As the names of the two networks are somehow self-explained, first, it’s clear that we can’t directly compute the output sequence by using just one network, so we need to use the first network to &lt;strong&gt;encode&lt;/strong&gt; the input sequence into some kind of “middle sequence”, then the other network will decode that sequence into our desire output sequence. So, what does the “middle sequence” look like? Let’s take a look at the next graph below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/sequence-to-sequence/repeated_vector.png&quot; alt=&quot;repeated_vec&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The mystery was revealed! Concretely, what the Encoder actually did is creating a temporary output vector from the input sequence (you can think about that temporary output vector as a sequence with only one timestep). Then, that vector is repeated \(n\) times, with \(n\) is the length of our desire output sequence. Up to this point, you may get all the rest. Yep, the Decoder network acts exact the same way with the network I talked about in the last post. After repeating the output vector from the Encoder \(n\) times, we obtain a sequence with exact the same length with the associated output sequence, we can leave the computation for the Decoder network! And that’s the idea behind seq2seq. It’s not as hard as it might seem, right?&lt;/p&gt;

&lt;p&gt;So we now know about how to output a sequence from an input of different length. But what about the lengths of input sequences? As I mentioned above, the input sequences themselves don’t necessarily have the exact same length, either! Sounds like an other headache, doesn’t it? Fortunately, it’s far more relaxing than the problem above. In fact, all we need to do is just something called: &lt;strong&gt;Zero Padding&lt;/strong&gt;. To make it easier for you to understand, let’s see the image below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/sequence-to-sequence/five_sentences.png&quot; alt=&quot;five_sentences&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here I prepared five sentences (they were actually from a great song of Twenty One Pilots, link provided at Reference) and let’s imagine that they will be the input sequences to our network. As you could see, three sentences are not equal in length. To make them all equal in length, let’s take the length of the longest sentence as the common length, and we only need to add one same word some times to the end of the other two, until they have the same length as the longest one. The added word must not resemble any words in the sentences, since it will cause their meaning to change. I will use the word &lt;strong&gt;ZERO&lt;/strong&gt;, and here’s the result I received:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/sequence-to-sequence/ZERO_added.png&quot; alt=&quot;ZERO_added&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You might get this now. And that’s why it is called &lt;strong&gt;zero padding&lt;/strong&gt;. In fact, what I did above is not exactly zero padding, and we will likely implement it differently. I’ll tell you more in the Implementation section. For now, all I wanted to do is just to help you understand zero padding without any hurt.&lt;/p&gt;

&lt;p&gt;We are half way there! We now know all we need to know about the-state-of-the-art Sequence to Sequence Learning. I can’t help jumping right into Implementation section. Neither can you, right?&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;(You can find the whole source files on my GitHub repository here: &lt;a href=&quot;https://github.com/mahaveer0suthar/seq2seq&quot; target=&quot;_blank&quot;&gt;seq2seq&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;So, now we are here, finally, right in the Implementation section. Working with NLP problems is literally abstract (than what we did in Computer Vision problems, which we could at least have some visualization). Even worse, deep neural network in common is kind of abstract itself, so it seems that thing’s gonna get more complicated here. That’s the reason why I decided not to dig into details in the previous section, but to explain it along with the corresponding part in the code instead so that you won’t find it difficult to understand the abstract terms (at least I think so). And now, let’s get your hands dirty!&lt;/p&gt;

&lt;p&gt;As usual, we will start with the most tedious (and boring) but important task, which is &lt;strong&gt;Data Preparation&lt;/strong&gt;. As you already saw in my previous post, it is a little bit more complicated to prepare language data rather than image data. You will understand why soon.&lt;/p&gt;

&lt;p&gt;Before doing any complicated processing, first we need to read the training data from file. I defined &lt;em&gt;X_data&lt;/em&gt; and &lt;em&gt;y_data&lt;/em&gt; variables to store the input text and the output text, respectively. They are all raw string objects, which means that we must split them into sentences:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text_to_word_sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[::&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_len&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text_to_word_sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_len&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Let’s break it down for a better understanding. I will use the three sentences above as our &lt;em&gt;X_data&lt;/em&gt;, here’s what happened after &lt;em&gt;X_data.split(‘\n’)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/sequence-to-sequence/split_sentences.png&quot; alt=&quot;split_sentences&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The easiest way to split a raw text into sentences is looking for the line break. Of course, there are many other better ways, but let’s make it simple this time. So, from a raw text we now obtained an array of sentences.&lt;/p&gt;

&lt;p&gt;Next, for each sentence in the array, we must then split it into an array of words, or say it in a more proper way, a sequence of words. We will do this by using Keras’ predefined method called &lt;strong&gt;text_to_word_sentence&lt;/strong&gt;, as illustrated below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/sequence-to-sequence/split_words.png&quot; alt=&quot;split_words&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Splitting a sentence into a sequence of words is harder than splitting text into sentences, since there are many ways to seperate words in a sentence, e.g. spaces, commas and so on. So we should not self-implement it but make use of predefined method instead. &lt;strong&gt;text_to_word_sentence&lt;/strong&gt; also helps us remove all the sentence ending marks such as periods or exclamation marks. Quite helpful, isn’t it?&lt;/p&gt;

&lt;p&gt;So, here’s what we received, an array of sequences of words:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/sequence-to-sequence/sequences.png&quot; alt=&quot;sequences&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But wait! There’s one minor change which needs to be made to the input sequences, as mentioned from the paper as follow:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We found
it extremely valuable to reverse the order of the words of the input sentence. So for example, instead
of mapping the sentence a, b, c to the sentence α, β, γ, the LSTM is asked to map c, b, a to α, β, γ,
where α, β, γ is the translation of a, b, c. This way, a is in close proximity to α, b is fairly close to
β, and so on, a fact that makes it easy for SGD to “establish communication” between the input and
the output. We found this simple data transformation to greatly boost the performance of the LSTM.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;If you noticed the graph I drew above, you would have some doubt about the order of the input sequence. Yeah, as you might guess, the order of the input sequence is reversed before going into the network. And that’s the reason why I added &lt;strong&gt;[::-1]&lt;/strong&gt; to reverse the sequence split from the raw text. So, the final input sequences look like below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/sequence-to-sequence/reverse_sequences.png&quot; alt=&quot;reverse_sequences&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Seems like we’re done, right? But sadly, we are only half way there before we can actually have the network train our data. As computers can only understand the gray scale values of pixels in an image, inputting sequences of raw human-alike words will make no sense to computers. For that reason, we need to take a further step, which is converting the raw words into some kind of numeric values. To do that, we need a dictionary to map from a word to its corresponding index value, and another dictionary for the same purpose, but in reverse direction.&lt;/p&gt;

&lt;p&gt;But first, what we need is a vocaburaly set. You can think of vocabulary set as an array which stores all the words in the raw text, but each word only appears once.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FreqDist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;most_common&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FreqDist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;most_common&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In real deep learning projects, especially when we’re dealing with NLP problems, our training data is pretty large in size, which the number of vocabularies may be up to millions. Obviously, that’s too much for our computers to handle. Furthermore, words which appear only a few times (typically once or twice) in the whole text may not have a significant impact on the learning of our network. So, what we do first is to count the frequency which a word appears in the text, then we create the vocabulary set using only 10000 words with highest frequencies (you can change to 20000 or more, but make sure that your machine can handle it).&lt;/p&gt;

&lt;p&gt;The result may look like below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/sequence-to-sequence/vocab_set.png&quot; alt=&quot;vocab_set&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So we just have created the vocabulary set from the input text. In the next step, we will create two dictionaries to map between each word and its index in the vocabulary set, and vice versa.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Creating an array of words from the vocabulary set, we will use this array as index-to-word dictionary
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_ix_to_word&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Adding the word &quot;ZERO&quot; to the beginning of the array
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_ix_to_word&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;insert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ZERO'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Adding the word 'UNK' to the end of the array (stands for UNKNOWN words)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_ix_to_word&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'UNK'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;With the vocabulary set we created above, it’s pretty easy to create an array to store only the words, and eliminate their frequencies of occurrence (we don’t need that information after all). But you may wonder, we were supposed to create some kind of dictionary here in order to convert each index to its associated word, and now what I told you to create is an array. Well, since we want to create the index-to-word dictionary, and we can access any element of an array through its index, it’s better just to create a simple array instead of a dictionary where keys are all indexes! I’m sure you get that now.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Next, we will need to add two special words. As I mentioned earlier, we need a word called &lt;strong&gt;ZERO&lt;/strong&gt; in order to make all sequences have the exact same length, and another word called &lt;strong&gt;UNK&lt;/strong&gt;, which stands for &lt;strong&gt;unknown words&lt;/strong&gt; or &lt;strong&gt;out of vocabulary&lt;/strong&gt; in order to represent words which are not in the vocabulary set. There’s nothing special with the word “UNK”, which we can just append it to the end of the index-to-word array. But I want you to pay attention to the word “ZERO”, &lt;strong&gt;it must be the element of index 0&lt;/strong&gt;! You will understand why as we move on to the next steps.&lt;/p&gt;

&lt;p&gt;So here’s what the index-to-word array looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/sequence-to-sequence/ix_to_word.png&quot; alt=&quot;ix_to_word&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As I told you above, don’t forget to confirm that the word &lt;strong&gt;ZERO&lt;/strong&gt; always be the first element before moving to the next step!&lt;/p&gt;

&lt;p&gt;Our next step is pretty simple which is creating the word-to-index dictionary from the array above, so all we need is just a single line of code!&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Create the word-to-index dictionary from the array created above
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_word_to_ix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ix&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_ix_to_word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Let’s confirm the dictionary we have just created. Once again, make sure the word &lt;strong&gt;ZERO&lt;/strong&gt; is associated with the index 0! After that, we can move on to the next step.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/sequence-to-sequence/word_to_ix.png&quot; alt=&quot;word_to_ix&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So now we got the two dictionaries ready. The next step is pretty simple: we will loop through the sequences and replace every word in each sequence by its corresponding index number. And also remember that we’re only putting 10000 words with highest frequencies into the vocabulary set, which also means that our network will actually learn words from that vocabulary set only. So here comes the question: What happens to the other words and how can we converse them to numeric values? That’s where the word &lt;strong&gt;UNK&lt;/strong&gt; makes sense. It stands for “Unknown words”, or it’s sometimes called &lt;strong&gt;OOV&lt;/strong&gt;, which means “Out Of Vocabulary”. So, for words which are not in the vocabulary set, we will simply assign them as &lt;strong&gt;UNK&lt;/strong&gt;. And as you may guess, they will all have the same index value.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Converting each word to its index value
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentence&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_word_to_ix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_word_to_ix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_word_to_ix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'UNK'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And here’s what we obtained. Obviously, our sequences don’t contain any &lt;strong&gt;ZERO&lt;/strong&gt;, so the converted sequences only contain numbers from \(1\) (which are the associated indexes of words in the sequences).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/sequence-to-sequence/index_sequence.png&quot; alt=&quot;index_sequence&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And now we got an array of sequences which all elements are numeric values instead of raw words. In the next step, we will use Keras’ &lt;strong&gt;pad_sequences&lt;/strong&gt; method to pad zeros into our sequences, so as all the sequences will have a same length. I told you about zero padding above, so there’s not much left to talk here, I think.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_sequences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_max_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'int32'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_sequences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_max_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'int32'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And here’s what our sequences looks like, after zero padded.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/sequence-to-sequence/zero_pad_index_sequence.png&quot; alt=&quot;zero_pad_index_sequence&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you could see from the image above, what &lt;strong&gt;pad_sequences&lt;/strong&gt; method did is just add additional \(0\) to each sequence, to make all the sequences have a same length with the longest one. So it’s very important that the original sequences don’t contain any \(0\). That’s the reason why we must add the word &lt;strong&gt;ZERO&lt;/strong&gt; to the beginning of the index-to-word array, so that the index of every word in the vocabulary set is not \(0\). If we don’t, and have some word with index \(0\) instead, then our network won’t be able to decide whether that \(0\) is padded zero, or index of a particular word. And it will definitely lead to a really bad learning.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;So we now got a new array of sequences which all the lengths are the same. But it still can’t be understand by the network. Concretely, we have to do a final processing step called vectorization:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word_sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word_to_ix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentence&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word_sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Explaining the process of vectorization (especially in terms of NLP) is kind of tedious, so I think it’s better help you guys have a visualization of it. I’m quite sure you will get it just by having a look at the image below. A picture is worth a thousand words!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/sequence-to-sequence/vectorization.png&quot; alt=&quot;vectorization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, we have finished the toughest part and got our training data ready. Phew! You’d better take a break, we all deserve it!&lt;/p&gt;

&lt;p&gt;In the next step, we will create the &lt;strong&gt;encoder&lt;/strong&gt; network. Since we need to compute only a single vector from the input sequences, the &lt;strong&gt;encoder&lt;/strong&gt; network is pretty simple, just a network with a single hidden layer is far from enough.&lt;/p&gt;

&lt;p&gt;But wait! What the heck is &lt;strong&gt;Embedding&lt;/strong&gt;, you may probably ask. In fact, we are supposed to input directly the vectorized array from above step into some kind of recurrent neural network like LSTM or vanilla RNN. But what we’re gonna do is slightly different. We will vectorize only the output sequences, and leaving the zero padded input sequences unchanged. Then, we will put that input sequences into a special layer called &lt;strong&gt;Embedding&lt;/strong&gt; first. Remember that you don’t necessarily use that &lt;strong&gt;Embedding&lt;/strong&gt; layer, instead you can just vectorize the input sequences and put it directly to the LSTM layer. Talking further into Word Embedding is beyond the scope of this post. The reason I use that layer is just to obtain a better result, from the fact that the size of vocabulary set is pretty small. I will definitely talk about Word Embedding in the coming post, I promise. For now, you have two choices, and it’s all on you.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_vocab_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_max_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask_zero&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LSTM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Next, we will create the &lt;strong&gt;decoder&lt;/strong&gt; network, which does the main job. First, we need to repeat the single vector outputted from the &lt;strong&gt;encoder&lt;/strong&gt; network to obtain a sequence which has the same length with the output sequences. The rest is similar to the &lt;strong&gt;encoder&lt;/strong&gt; network, except that the &lt;strong&gt;decoder&lt;/strong&gt; will be more complicated, which we will have two or more hidden layers stacked up. For ones who are not familiar with Recurrent Neural Networks and how to create them using Keras, please refer to my previous post from the link in the beginning of this post.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RepeatVector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_max_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LSTM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_sequences&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TimeDistributed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_vocab_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'softmax'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'categorical_crossentropy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rmsprop'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;So we finally got everything ready. Let’s go ahead and train our models. Due to some limitations of memory, I was able to train 1000 sequences, which means 1 batch at a time (with batch size 1000). I still can’t find another better solution to this probem. If you guys have some ideas about it, please kindly let me know :)&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NB_EPOCH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;i_end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;i_end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_sequences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;process_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i_end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_max_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_word_to_ix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'[INFO] Training model: epoch {}th {}/{} samples'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i_end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_sequences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BATCH_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_epoch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'checkpoint_epoch_{}.hdf5'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;At the time of writing, the model is on its third day of learning and everything seems promising. I will continue to update the result, maybe after letting it learn for four or five more days!&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;So, in today’s blog post, I have talked about the incapability of normal RNN networks to deal with complicated NLP problems, where sequences differ in length. And through a project of creating an English-Finnish Language Translating Model, I also introduced to you a solution to this big problem by using Sequence To Sequence Learning Approach. It’s just a simple experiment, so obviously, there are many places that you can improve. Feel free to play with the model and modify it for your own purposes.&lt;/p&gt;

&lt;p&gt;After all, language modeling is a quite complicated problem, I think, and so is Sequence To Sequence Approach. For that reason, I don’t expect you to fully understand the idea behind it just by reading this blog post (I myself can’t say that I fully understand it, either!). So I recommend you to take not just one look at the paper, but to read it many times to grab a better understanding. And just don’t forget that we can always discuss here to help each other learn better. Hope you all enjoy my post, and I’m gonna see you guys soon, on my next post.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Ilya Sutskever, Oriol Vinyals and Quoc V. Le. &lt;a href=&quot;https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&quot; target=&quot;_blank&quot;&gt;Sequence to Sequence Learning with Neural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Keras Addition RNN (Sequence to Sequence Learning based implementation) &lt;a href=&quot;https://github.com/fchollet/keras/blob/master/examples/addition_rnn.py&quot; target=&quot;_blank&quot;&gt;addition_rnn&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The sentences above were from Heathens, an addicting song of Twenty One Pilots which I kept repeating recently. Watch it here: &lt;a href=&quot;https://www.youtube.com/watch?v=UprcpdwuwCg&quot; target=&quot;_blank&quot;&gt;Heathens&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Mahaveer Suthar</name><email>mahaveer0suthar@gmail.com</email></author><category term="machine-learning" /><category term="deep-learning" /><category term="keras" /><category term="recurrent neural network" /><category term="gpu" /><category term="training" /><category term="RNN" /><category term="LSTM" /><category term="GRU" /><category term="seq2seq" /><category term="translator model" /><summary type="html">Hello guys. It’s been quite a long while since my last blog post. It may sound like an excuse, but I’ve been struggling with finding a new place to move into. And I had to say, it’s a real problem for a foreigner to find a reasonable apartment in Japan. Luckily, I somehow managed to find one, and I have just moved in for nearly two weeks. Anyway, the toughest time has gone, and now I can get myself back to work, to bring to you guys new interesting (and maybe boring as usual) blog posts on Deep Learning.</summary></entry><entry><title type="html">Creating A Text Generator Using Recurrent Neural Network</title><link href="http://localhost:4000/mahaveer0suthar.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/" rel="alternate" type="text/html" title="Creating A Text Generator Using Recurrent Neural Network" /><published>2017-11-14T00:00:00+05:30</published><updated>2017-11-14T00:00:00+05:30</updated><id>http://localhost:4000/mahaveer0suthar.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network</id><content type="html" xml:base="http://localhost:4000/mahaveer0suthar.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/">&lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;p&gt;Hello guys, it’s been another while since my last post, and I hope you’re all doing well with your own projects. I’ve been kept busy with my own stuff, too. And till this point, I got some interesting results which urged me to share to all you guys. Yeah, what I did is creating a Text Generator by training a Recurrent Neural Network Model. Below is a sample which was generated by the trained Model:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;They had no choice but the most recent univerbeen fairly uncomfortable and dangerous as ever. As long as he dived experience that it was 
not uncertain that even Harry had taken in black tail as the train roared and was thin, but Harry, Ron, and Hermione, at the fact that he was in complete disarraying the rest of the class holding him, he should have been able to prove them.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Does it sound similar? Yeah, you may recognize J. K. Rowling’s style in the paragraph above. That’s because I trained the Model using the famous Harry Potter series! Do you feel excited and want to create something of your own? Just keep reading, a lot of fun is waiting ahead, I promise!&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Many of you may know about Recurrent Neural Networks, and many may not, but I’m quite sure that you all heard about Neural Networks. We have already seen how Neural Networks can solve nearly all Machine Learning problems no matter how complicated they are. And because to fully understand how Neural Networks work does require a lot of time for reading and implementing by yourself, and yet I haven’t made any tutorials on them, it’s nearly impossible to write it all in this post. So it’d be better to leave them for some future tutorials and make it easy this time by looking at the picture below instead.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/creating-text-generator-using-recurrent-neural-network/neural_network.png&quot; alt=&quot;neural_network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you could see in the picture above, the main reason why Neural Network can out-perform other learning algorithms is because of the &lt;strong&gt;hidden layers&lt;/strong&gt;. What the hidden layers do is to create a more complicated set of features, which results in a better predicting accuracy. I also mentioned about this in my previous posts: the more complicated and informative the features become, the more likely your Model can learn better and give more precise predictions.&lt;/p&gt;

&lt;p&gt;Despite the outstanding performance that Neural Networks have shown us over the last decade, they still have a big big limitation: they can’t understand the sequence, in which the current state is affected by its previous states. And Recurrent Neural Networks came out as a promising solution for that.&lt;/p&gt;

&lt;p&gt;The explanation of Recurrent Neural Networks such as what they are, how they work, or something like that is quite long and not the main purpose of this post, which I mainly want to guide you to create your own text generator. In fact, there are many guys out there who made some excellent posts on how Recurrent Neural Networks work. You can refer to their post through the links below. Some of them provides their codes too, but they used Theano or Torch for their work, which may hurt a lot if you don’t have experience with those frameworks. To make it easy for you, I tried to re-implement the code using a more relaxing framework called Keras. You can check it out in the Implementation section below.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/&quot; target=&quot;_blank&quot;&gt;Recurrent Neural Networks tutorial by Denny Britz&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot; target=&quot;_blank&quot;&gt;Understanding LSTMs by Colah&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot; target=&quot;_blank&quot;&gt;The Unreasonable Effectiveness of Recurrent Neural Networks by Andrej Karpathy&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And because the fact that there are already many great posts on Recurrent Neural Networks, I will only talk briefly about some points which confused me, and may confuse you too, I think.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;vanilla-rnn&quot;&gt;Vanilla RNN&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/creating-text-generator-using-recurrent-neural-network/vanilla_RNN.png&quot; alt=&quot;vanilla_RNN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The very first basic idea of RNN is to stack one or more hidden layers of previous timesteps, each hidden layer depends on the corresponding input at that timestep and the previous timestep, like below:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_t=f(W_{xh}x_t+W_{hh}h_{t-1})&lt;/script&gt;

&lt;p&gt;The output, on the other hand, is computed using only the associating hidden layer:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_t=softmax(W_{hy}h_t)&lt;/script&gt;

&lt;p&gt;So, with hidden layers of different timesteps, obviously the new tyep of Network can now have ability to “remember”. But it can’t not remember over a long timestep due to a problem called &lt;strong&gt;vanishing gradient&lt;/strong&gt; (I will talk about it in future post), and it can’t decide which information of some timestep is valuable (which it should keep) and which information is not valuable (which it should forget). So an improvement was required. And Long Short-term Memory, or LSTM came out as a potential successor.&lt;/p&gt;

&lt;h3 id=&quot;long-short-term-memory-networks&quot;&gt;Long Short-term Memory Networks&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/creating-text-generator-using-recurrent-neural-network/LSTM.png&quot; alt=&quot;LSTM&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Having seen the limitation of vanilla RNN, now let’s take a look at its successor, the LSTM Networks. The explanations of LSTM in the links above are pretty awesome, but honestly, they confused me a little. Personally I think it would be easier to understand if we begin from what RNNs could accomplish:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;o_t=\sigma(W_o[h_{t-1},x_t]+b_O)&lt;/script&gt;

&lt;p&gt;Comparing to RNN, the equation above is exactly the same with RNN to compute the hidden state at timestep \(t\). But it’s not the &lt;em&gt;actual&lt;/em&gt; hidden state in terms of LSTM, so we name it differently, let’s say \(o_t\). So from here, we will see how LSTM was improved from RNN.&lt;/p&gt;

&lt;p&gt;First, LSTM is given the ability to “forget”, which mean it can decide whether to forget the previous hidden state. All is done by adding &lt;strong&gt;Forget Gate Layer&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_t=\sigma(W_f.\left[h_{t-1}, x_t\right]+b_f)&lt;/script&gt;

&lt;p&gt;In contrast to forget gate layer, to tell the Model whether to update the current state using the previous state, we need to add &lt;strong&gt;Input Gate Layer&lt;/strong&gt; accordingly.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;i_t=\sigma(W_i.[h_{t-1}, x_t]+b_i)&lt;/script&gt;

&lt;p&gt;Next, we will compute the temporal cell state for the current timestep. It looks just like the output of RNN above, except that &lt;strong&gt;tanh&lt;/strong&gt; activation function is used:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tilde{C}_t=tanh(W_C.[h_{t-1},x_t]+b_C)&lt;/script&gt;

&lt;p&gt;And now, we will compute the &lt;em&gt;actual&lt;/em&gt; cell state for current timestep, using the forget gate and input gate above. Intuitively, doing so makes LSTM be able to keep only the necessary information and forget the unnecessary one.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C_t=f_t*C_{t-1}+i_t*\tilde{C}_t&lt;/script&gt;

&lt;p&gt;After we computed the current cell state, we will use it to compute the current hidden state like below:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_t=o_t*tanh(C_t)&lt;/script&gt;

&lt;p&gt;So after all, we now have the hidden state for the current timestep. The rest is similar to vanilla RNN, which is computing the &lt;em&gt;actual&lt;/em&gt; output \(y_t\):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_t=softmax(W_{hy}h_t)&lt;/script&gt;

&lt;p&gt;That’s all I want to tell you about RNNs and LSTMs. I suggest that you read the three articles above for better understanding about how they work. And now let’s jump into the most interesting part (I think so): the Implementation section!&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;As I mentioned earlier in this post, there are quite a lot of excellent posts on how Recurrent Neural Networks work, and those guys also included the implementations for demonstration. Actually, because they wrote code for teaching purpose, reading the codes does help understanding the tutorials a lot. But I must say that it may hurt, especially if you don’t have any experience in Theano or Torch (Denny wrote his code in Theano and Andrej used Torch). I want to make it easy for you, so I will show you how to implement RNN using Keras, an excellent work from François Chollet, which I had a chance to introduced to you in my previous posts.&lt;/p&gt;

&lt;p&gt;If you don’t have Keras installed on your machine, just give the link below a click. The installation only takes 20 minutes (max):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://mahaveer0suthar.github.io/tutorial/Setting-Up-Python-Environment-For-Computer-Vision-And-Machine-Learning/&quot; target=&quot;_blank&quot;&gt;Installing OpenCV &amp;amp; Keras&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, let’s get down to business. For sake of simplicity, I will divide the code into four parts and dig into each part one at a time. Of course I will omit some lines used for importing or argument parsing, etc. You can find the full source file in my GitHub here: &lt;a href=&quot;https://github.com/mahaveer0suthar/text-generator&quot; target=&quot;_blank&quot;&gt;Text Generator&lt;/a&gt;. Now let’s go into the first part: preparing the data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Prepare the training data&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I always try to deal with the most tedious part in the beginning, which is data preparation. Not only because a good data preparation can result in a well learned Model, but this step is also some kind of tricky, which we likely spend a lot of time until it works (especially if you are working with different frameworks).&lt;/p&gt;

&lt;p&gt;We are gonna work with text in this post, so obviously we have to prepare a text file to train our Model. You can go on the internet to grab anything you want such as free text novels &lt;a href=&quot;http://cs.stanford.edu/people/karpathy/char-rnn/&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;, and I recommend the file size is at least 2MB for an acceptable result. In my case, I used the famous Harry Potter series for training (of course I can’t share it here for copyright privacy).&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; 
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DATA_DIR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;chars&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;VOCAB_SIZE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;First, we will read the text file, then split the content into an array which each element is a character, and store it into &lt;em&gt;data&lt;/em&gt; variable. Next, we will create a new array called &lt;em&gt;chars&lt;/em&gt; to store the unique values in &lt;em&gt;data&lt;/em&gt;. For example, your text file contains only the following sentence:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I have a dream.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Then the &lt;em&gt;data&lt;/em&gt; array will look like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; 
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'I'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'h'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'a'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'v'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'e'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'a'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'d'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'e'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'a'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'m'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'.'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And the &lt;em&gt;chars&lt;/em&gt; array will look like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; 
&lt;span class=&quot;n&quot;&gt;chars&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'I'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'h'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'a'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'v'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'e'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'d'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'m'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'.'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;As you could see, every element in &lt;em&gt;char&lt;/em&gt; array only appears once. So the &lt;em&gt;data&lt;/em&gt; array contains all the examples, and the &lt;em&gt;chars&lt;/em&gt; array acts like a features holder, which we then create two dictionaries to map between indexes and characters:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; 
&lt;span class=&quot;n&quot;&gt;ix_to_char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;char_to_ix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ix&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Why do we have to do the mapping anyway? Because it’s better to input numeric training data into the Networks (as well as other learning algorithms). And we also need a different dictionary to convert the numbers back to the original characters. That’s why we created the two dictionaries above.&lt;/p&gt;

&lt;p&gt;After we’ve done the file reading, we will create the actual input for the Network. We’re gonna use Keras to create and train our Network, so we must convert the data into this form: &lt;strong&gt;(number_of_sequences, length_of_sequence, number_of_features)&lt;/strong&gt;. The last dimension is the number of the features, in this case the length of the &lt;em&gt;chars&lt;/em&gt; array above. Next, the length of sequence means how long you want your Model to learn at a time. It’s also the total timesteps of our Networks which I showed you above. The first dimension is the number of sequences, which is easy to achieve by dividing the length of our data by the length of each sequence. Of course we also need to convert each character into the corresponding index number.&lt;/p&gt;

&lt;p&gt;And what about the target sequences? In this post, we only make a simple text generator, so we just need to set the target by shifting the corresponding input sequence by one character. Obviously our target sequence will have the same length with the input sequence. About model that can output target sequences with different length, I will leave for the next post.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; 
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SEQ_LENGTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SEQ_LENGTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VOCAB_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SEQ_LENGTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SEQ_LENGTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VOCAB_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SEQ_LENGTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X_sequence&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SEQ_LENGTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SEQ_LENGTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X_sequence_ix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char_to_ix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input_sequence&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SEQ_LENGTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VOCAB_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SEQ_LENGTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;input_sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_sequence_ix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_sequence&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;y_sequence&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SEQ_LENGTH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SEQ_LENGTH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_sequence_ix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char_to_ix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;target_sequence&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SEQ_LENGTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VOCAB_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SEQ_LENGTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;target_sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_sequence_ix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_sequence&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The code is not difficult to understand at all, but make sure you take a look before moving on.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Create the Network&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So we have done with the data preparation. The rest is some kind of relaxing since we can make use of Keras to help us handle the hardest part: create the Network. We’re gonna use LSTM for its ability to deal with long sequences, you can experiment other Model by changing LSTM to SimpleRNN or GRU. The choice is yours!&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; 
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LSTM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HIDDEN_DIM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VOCAB_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_sequences&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LAYER_NUM&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LSTM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HIDDEN_DIM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_sequences&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TimeDistributed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VOCAB_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'softmax'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;categorical_crossentropy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;rmsprop&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You should have no problem in understand the code above, right? There are only few points that I want to make clear:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;return_sequences=True&lt;/strong&gt; parameter:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We want to have a sequence for the output, not just a single vector as we did with normal Neural Networks, so it’s necessary that we set the &lt;em&gt;return_sequences&lt;/em&gt; to True. Concretely, let’s say we have an input with shape &lt;em&gt;(num_seq, seq_len, num_feature)&lt;/em&gt;. If we don’t set return_sequences=True, our output will have the shape &lt;em&gt;(num_seq, num_feature)&lt;/em&gt;, but if we do, we will obtain the output with shape &lt;em&gt;(num_seq, seq_len, num_feature)&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;TimeDistributed&lt;/strong&gt; wrapper layer:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since we set return_sequences=True in the LSTM layers, the output is now a three-dimension vector. If we input that into the Dense layer, it will raise an error because the Dense layer only accepts two-dimension input. In order to input a three-dimension vector, we need to use a wrapper layer called &lt;em&gt;TimeDistributed&lt;/em&gt;. This layer will help us maintain output’s shape, so that we can achieve a sequence as output in the end.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Train the Network&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the next step, we will train our Network using the data we prepared above. Here we want the Model to generate some texts after each epoch, so we set &lt;em&gt;nb_epoch=1&lt;/em&gt; and put the training into a while loop. We also save the weights after each 10 epochs in order to load it back later, without training the Network again!&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; 
&lt;span class=&quot;n&quot;&gt;nb_epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BATCH_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_epoch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nb_epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;generate_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GENERATE_LENGTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'checkpoint_{}_epoch_{}.hdf5'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HIDDEN_DIM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4. Generate text&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Last but not least, I want to talk a little about the method to generate text. We begin with some random character and use the trained Model to predict the next one. Then we append the predicted character into the input, and have the Model predict the next one, which is the third character. We continue the process until we obtain a sequence with the length we want (500 characters by default). It’s just that simple!&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; 
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generate_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VOCAB_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ix_to_char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VOCAB_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ix_to_char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:])[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ix_to_char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;5. Result&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I created the Network with three LSTM layers, each layer has 700 hidden states, with Dropout ratio 0.3 at the first LSTM layer. I was training the Network on GPU for roughly a day (\(\approx200\) epochs), and here are some paragraphs which were generated by the trained Model:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Yeah, I know, I saw him run off the balls of the Three Broomsticks around the Daily Prophet that we met Potter’s name!” said Hermione. “We’ve done all right, Draco, and Karkaroff would have to spell the Imperius Curse,” said Dumbledore. “But Harry, never found out about the happy against the school.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Albus Dumbledore, I should, do you? But he doesn’t want to adding the thing that you are at Hogwarts, so we can run and get more than one else, you see you, Harry.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;“I know I don’t think I’ll be here in my bed!” said Ron, looking up at the owners of the Dursleys.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Well, you can’t be the baby way?” said Harry. “He was a great Beater, he didn’t want to ask for more time.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;“What about this thing, you shouldn’t,” Harry said to Ron and Hermione.  “I have no furious test,” said Hermione in a small voice.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To be honest, I was impressed by what the Model can generate. After leaving it a while for learning, as you could see, not only it can generate nearly perfect English words, but it also learned the structures, which means it capitalizes the first letter after period, it knows how to use the quotation marks, etc. And if I don’t tell you anything about RNNs, you may think (even I do too!) that the paragraphs above were written by somebody. So, it’s now your turn to train your own Network using the dataset of your own choice, and see what you achieve. And if you find the result interesting, please let me know by dropping me a line below!&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;So we have come a long way to finish today’s post, and I hope you all now obtain some interesting results for your own. We have walked through a brief introduction about the need of Recurrent Neural Networks o solve the limitation of common Neural Networks and figured out how LSTMs even improved the state-of-the-art vanilla RNNs.&lt;/p&gt;

&lt;p&gt;And we also implemented our own Networks to create a simple text generator, which we can use to generate some sample texts in the style of what they learned from! Note that this is just a fast and dirty implementation, and obviously there are a lot of rooms for improvement, which I will leave them for you to improvise by yourself.&lt;/p&gt;

&lt;p&gt;That’s it for today. I will be back with you guys in the coming post, with even more interesting stuff. So just stay updated!&lt;/p&gt;</content><author><name>Mahaveer Suthar</name><email>mahaveer0suthar@gmail.com</email></author><category term="machine-learning" /><category term="deep-learning" /><category term="keras" /><category term="recurrent neural network" /><category term="gpu" /><category term="training" /><category term="RNN" /><category term="LSTM" /><category term="GRU" /><category term="text generator" /><summary type="html">Hello guys, it’s been another while since my last post, and I hope you’re all doing well with your own projects. I’ve been kept busy with my own stuff, too. And till this point, I got some interesting results which urged me to share to all you guys. Yeah, what I did is creating a Text Generator by training a Recurrent Neural Network Model. Below is a sample which was generated by the trained Model:</summary></entry><entry><title type="html">Training With Your Own Dataset on Caffe</title><link href="http://localhost:4000/mahaveer0suthar.github.io/project/Training-Your-Own-Data-On-Caffe/" rel="alternate" type="text/html" title="Training With Your Own Dataset on Caffe" /><published>2017-11-09T00:00:00+05:30</published><updated>2017-11-09T00:00:00+05:30</updated><id>http://localhost:4000/mahaveer0suthar.github.io/project/Training-Your-Own-Data-On-Caffe</id><content type="html" xml:base="http://localhost:4000/mahaveer0suthar.github.io/project/Training-Your-Own-Data-On-Caffe/">&lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;p&gt;Hi, everyone! Welcome back to my Machine Learning page today. I have been playing around with Caffe for a while, and as you already knew, I made a couple of posts on my experience in installing Caffe and making use of its state-of-the-art pre-trained Models for your own Machine Learning projects. Yeah, it’s really great that Caffe came bundled with many cool stuff inside which leaves developers like us nothing to mess with the Networks. But of course, there comes sometime that you want to set up your own Network, using your own dataset for training and evaluating. And it turns out that using all the things which Caffe provides us doesn’t help Caffe look less like a &lt;em&gt;blackbox&lt;/em&gt;, and it’s pretty hard to figure things out from the beginning. And that’s why I decided to make this post, to give you a helping hand to literally make use of Caffe.&lt;/p&gt;

&lt;p&gt;Before getting into the details, for ones that missed my old posts on Caffe, you can check it out anytime, through the links below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://mahaveer0suthar.github.io/project/Installing-Caffe-CPU-Only/&quot; target=&quot;_blank&quot;&gt;Installing Caffe on Ubuntu (CPU_ONLY)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://mahaveer0suthar.github.io/project/Installing-Caffe-Ubuntu/&quot; target=&quot;_blank&quot;&gt;Installing Caffe on Ubuntu (GPU)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, let’s get down to business. In today’s post, I will mainly tell you about two points below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Downloading your own dataset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Preparing your data before training&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training with your prepared data&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, I will go straight to each part right below.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Downloading your data&lt;/strong&gt;&lt;br /&gt;
I think there’s a lot of ways which everyone of you managed to get your own dataset. If your dataset has been already placed on your hard disk, then you can skip the &lt;strong&gt;Downloading&lt;/strong&gt; section and jump right into the &lt;strong&gt;Preparing&lt;/strong&gt; section. Here I’m assuming that you do not have any dataset of your own, and you’re intending to use some dataset from free sources like ImageNet or Flickr or Kaggle. Then it’s likely that: you can directly download the dataset (from sources like Kaggle), or you will be provided a text file which contains URLs of all the images (from sources like Flickr or ImageNet). The latter seems to be harder, but don’t worry, it won’t be that hard.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Directly downloading from source:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This kind of download is quite easy. Here I will use the &lt;strong&gt;Dogs vs. Cats&lt;/strong&gt; dataset from Kaggle for example. You can access the dataset from the Download page: &lt;a href=&quot;https://www.kaggle.com/c/dogs-vs-cats&quot;&gt;Dogs vs. Cats&lt;/a&gt;. All you have to do is just register an account, then you can download the whole dataset. There are two of them, one for training purpose, which was named &lt;em&gt;train&lt;/em&gt;, and one for evaluating, which was named &lt;em&gt;test1&lt;/em&gt; respectively. I suggest that you should download the training set only. I will explain why when we come to the &lt;strong&gt;Preparing&lt;/strong&gt; section. The file size is quite large so it should take a while to finish. And that’s it. You have the dataset stored on your hard disk!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Downloading from URLs&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As you could see above, it’s great if every dataset was zipped and provided directly to developers. But in fact, due to the copyright of the images (as well as other data types), providing data that way isn’t simple, especially when we talk about an extremely large dataset like ImageNet. So data providers have another way, which is providing you the URLs only, and you will have to access to the image hosts yourself to download the data. I will use a very famous site for example, which is ImageNet, the site which holds the annual ILSVRC. You can read more about ILSVRC &lt;a href=&quot;http://www.image-net.org/challenges/LSVRC/&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;First, let’s go to the ImageNet’s URLs download page: &lt;a href=&quot;http://image-net.org/download-imageurls&quot; target=&quot;_blank&quot;&gt;Download Image URLs&lt;/a&gt;. All you need to know to get the URLs is something called &lt;strong&gt;WordNet ID&lt;/strong&gt; (or &lt;strong&gt;wnid&lt;/strong&gt;). You can read more about ImageNet’s dataset and WordNet to grab some more details because this post will be too long if I explain it here. To make it simple right now, ImageNet uses WordNet’s synset, such as &lt;em&gt;n02084071&lt;/em&gt;, &lt;em&gt;n02121620&lt;/em&gt; which represents &lt;em&gt;dogs&lt;/em&gt; and &lt;em&gt;cats&lt;/em&gt; respectively, to name its classes. To find out what the synset of a particular noun, just access &lt;a href=&quot;http://www.image-net.org/synset?wnid&quot; target=&quot;_blank&quot;&gt;Noun to Synset&lt;/a&gt;, then search for any noun you want, then you will see the corresponding synset.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Once you knew the synset, you can download the URLs by going to this page:&lt;br /&gt;
http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=[wnid], which &lt;em&gt;[wnid]&lt;/em&gt; is the synset of the object you want to download data for. For example, let’s use two synsets above, to download the URLs of the Dogs and Cats images of ImageNet:&lt;/p&gt;

&lt;p&gt;Dogs: http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n02084071&lt;/p&gt;

&lt;p&gt;Cats: http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n02121620&lt;/p&gt;

&lt;p&gt;If you access the links above, you will see something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/training-your-own-data-on-caffe/urls.png&quot; alt=&quot;urls&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, the next thing to do is just simple, you have to copy those URLs and paste somewhere, let’s say a text file or something. With ones who are familiar with Linux commands, you can see that we can use &lt;em&gt;wget&lt;/em&gt; to grab all the images with ease. But there’s some problem here: using &lt;em&gt;wget&lt;/em&gt; is hard to rename the images as &lt;em&gt;wget&lt;/em&gt; will use the name right in each URL to name each image. Training your own data with CNN on Caffe may not require any naming rules, but if you have intention to use your own data in other places, for example, the state-of-the-art Faster R-CNN, then the naming convention does matter! And as far as I know, we can manually rename all the images while downloading using &lt;em&gt;wget&lt;/em&gt;, but it requires some experience in Linux commands, and to be honest, I tried and failed. But don’t worry, I found the solution for that!&lt;/p&gt;

&lt;p&gt;You know that Caffe provides us so many useful tools, to help us do all the heavy things so that we can use all the pre-trained Models without worrying about the data preparation, which means that, if you want to play with MNIST, Caffe provides you the script to download MNIST, if you want to play with CIFAR-10, Caffe got a script to download CIFAR-10 too. So, we can make use of the tools Caffe provides, and modify a little to make it work with our data. Not so bad, right?&lt;/p&gt;

&lt;p&gt;All you have to do, is to make use of the tool which Caffe uses to download Flickr’s images for fine-tuning (I will tell you about fine-tuning in the second part, so don’t care about that term). Open your terminal, and type the commands below (make sure that you are in the root folder of Caffe):&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;data
&lt;span class=&quot;nb&quot;&gt;mkdir &lt;/span&gt;DogsCats

&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ../examples
&lt;span class=&quot;nb&quot;&gt;mkdir &lt;/span&gt;DogsCats
&lt;span class=&quot;nb&quot;&gt;sudo cp &lt;/span&gt;finetune_flickr_style/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; DogsCats/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;What we just did, is to create the neccessary folders for storing the script (&lt;em&gt;./examples/DogsCats&lt;/em&gt;) and the images (*./data/DogsCats), then we copied the script to download Flickr’s images to our new folder. Obviously, we have to make some changes in order to make it work properly, just some minor changes.&lt;/p&gt;

&lt;p&gt;First, let’s go to &lt;em&gt;./examples/DogsCats&lt;/em&gt; folder, unzip the &lt;em&gt;flickr_style.csv.gz&lt;/em&gt; to get a CSV file named &lt;em&gt;flickr_style.csv&lt;/em&gt;. Open it up, take a look at the file. There are five columns but just three of them are actually used: &lt;em&gt;image_url&lt;/em&gt;, &lt;em&gt;label&lt;/em&gt; and &lt;em&gt;_split&lt;/em&gt;. The &lt;em&gt;image_url&lt;/em&gt; column stores all the URLs to all the images, the &lt;em&gt;label&lt;/em&gt; column stores the label values, and the &lt;em&gt;_split&lt;/em&gt; column tells whether each image is used for training or evaluating purpose.&lt;/p&gt;

&lt;p&gt;As I mentioned earlier, we are not only downloading the images, but also renaming it, so we will use an additional column to store the name associating with each image URL, which I chose column &lt;em&gt;A&lt;/em&gt; for that task. Before making any changes, let’s deleting all the records, except the first row. Then, let’s name cell A1 &lt;em&gt;image_name&lt;/em&gt;. Next, in the &lt;em&gt;image_url&lt;/em&gt; column, paste all the URLs of each class. Note that we won’t paste all the URLs of all classes at once, since we have to labeling them. After pasting all the URLs of one class, let’s say the Dogs class with &lt;em&gt;n02084071&lt;/em&gt; synset, we will fill the &lt;em&gt;image_name&lt;/em&gt; column. Start from cell A2, let’s fill that it will &lt;em&gt;n02084071_0&lt;/em&gt; then drag until you see the last URL in &lt;em&gt;image_url&lt;/em&gt; column. Don’t forget to add the &lt;em&gt;.jpg&lt;/em&gt; extension when you finish (just use the CONCATENATE function).&lt;/p&gt;

&lt;p&gt;Next, we will label the images we have just added URLs for. In the &lt;em&gt;label&lt;/em&gt; column, let’s fill with &lt;em&gt;0&lt;/em&gt; until the last row containing URL. Since all URLs we pasted belong to Dogs, so they will have the same label. And lastly, let’s fill in the &lt;em&gt;_split&lt;/em&gt; column. In case of the Dogs’ images, we have 1603 images in total, so let’s fill &lt;em&gt;train&lt;/em&gt; for the first 1200 images and &lt;em&gt;test&lt;/em&gt; for the rest (here the train:test ratio I chose is 0.75:0.25). After all, your CSV should look similar to this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/training-your-own-data-on-caffe/csv.png&quot; alt=&quot;csv&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And we can continue with other classes’ images, don’t forget to increase the value of &lt;em&gt;label&lt;/em&gt; column each time you add another class’s URLs.&lt;/p&gt;

&lt;p&gt;So, we have done with the CSV file, let’s go ahead and modify the Python script (make sure that you are in the root folder of Caffe):&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;examples/DogsCats
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;vim assemble_data.py&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;First, let’s replace all the phrase &lt;em&gt;data/finetune_flickr_style&lt;/em&gt; with *data/DogsCats. That value tells where to store the downloaded images, so we have to point to our new created folder. Next, make some changes like below:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-vim&quot; data-lang=&quot;vim&quot;&gt;# Line &lt;span class=&quot;m&quot;&gt;63&lt;/span&gt;
df &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; pd&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;read_csv&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;csv_filename&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; index_col&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;None&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; compression&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'gzip'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

# Line &lt;span class=&quot;m&quot;&gt;77&lt;/span&gt;
os&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;path&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;images_dirname&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; value&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; value &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; df&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'image_name'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;That’s it. And now we are ready to download the images, and have them renamed the way we wanted:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;python assemble_data.py&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;It will take a while for the script to run. Note that many of the URLs are inaccessible at the time of writing, since many of them were added quite so long ago. So if you notice that the number of downloaded images is not equal to the number of URLs, don’t be confused.&lt;/p&gt;

&lt;p&gt;As soon as the script finished running, then your images are all stored on your drive. So now your dataset is ready for the next stage!&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Preparing the data before training&lt;/strong&gt;&lt;br /&gt;
So we just managed to have the desired dataset stored on your hard disk. And believe me or not, we have just completed the most time-consuming task! Before we can train our Network using the data we have just downloaded, there’s some things we need to do. First, we need to convert the downloaded images into the format that the Networks can read. In fact, the Networks in Caffe accepts not just one kind of input data. As far as I know, there are three different ways to prepare our images so that the Networks can read them, and I’m gonna tell you about two of them: normal format and LMDB format. And second, we need to provide one special image called &lt;em&gt;the mean image&lt;/em&gt;. Okay, let’s get into each of them.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Creating the train.txt and test.txt files&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s first talk about the data conversion. As I said above, we have two choices. You can choose whether to use the normal format (leave the images untouched after downloaded), or to convert them to LMDB format. In both cases, you have to create two files called &lt;em&gt;train.txt&lt;/em&gt; and &lt;em&gt;text.txt&lt;/em&gt;. What the two files do is to tell our Network where to look for each image and its corresponding class. To understand better, let’s go and create them.&lt;/p&gt;

&lt;p&gt;I’m gonna use the &lt;em&gt;Dogs vs Cats&lt;/em&gt; dataset which we downloaded from Kaggle (because we haven’t touched it yet, have we?). Let’s create two similar folders just like we did above with ImageNet’s images, one for storing the images, and one for storing the necessary scripts:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;examples
&lt;span class=&quot;nb&quot;&gt;mkdir &lt;/span&gt;DogsCatsKaggle
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ../data
&lt;span class=&quot;nb&quot;&gt;mkdir &lt;/span&gt;DogsCatsKaggle&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Then, let’s place the zip file which we downloaded from Kaggle into &lt;em&gt;./data/DogsCatsKaggle&lt;/em&gt; folder and upzip it. After unzipped, all of the images will be stored into the subfolder called &lt;em&gt;train&lt;/em&gt;. Next, we’re gonna create the &lt;em&gt;train.txt&lt;/em&gt; and &lt;em&gt;test.txt&lt;/em&gt; files. Let’s go into the &lt;em&gt;./examples/DogsCatsKaggle&lt;/em&gt; folder and create a Python file, name it &lt;em&gt;create_kaggle_txt.py&lt;/em&gt; and fill the codes below:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-vim&quot; data-lang=&quot;vim&quot;&gt;import numpy &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; np
import os
 
CURRENT_DIR &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; os&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;path&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;abspath&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;os&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;path&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;dirname&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;__file__&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
DATA_DIR &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; os&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;path&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;abspath&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;os&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;path&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;CURRENT_DIR&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'../../data/DogsCatsKaggle/train'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
TXT_DIR &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; os&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;path&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;abspath&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;os&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;path&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;CURRENT_DIR&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'../../data/DogsCatsKaggle'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
 
dog_images &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;image &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; image &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; os&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;listdir&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;DATA_DIR&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'dog'&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; image&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
cat_images &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;image &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; image &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; os&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;listdir&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;DATA_DIR&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'cat'&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; image&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
 
dog_train &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; dog_images&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;int&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;len&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;dog_images&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;*&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
dog_test &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; dog_images&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;int&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;len&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;dog_images&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;*&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):]&lt;/span&gt;
 
cat_train &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; cat_images&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;int&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;len&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;cat_images&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;*&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
cat_test &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; cat_images&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;int&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;len&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;cat_images&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;*&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):]&lt;/span&gt;
 
with open&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'{}/train.txt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;format&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;TXT_DIR&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'w'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; image &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; dog_train&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;write&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'{} 0\n'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;format&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;image&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; image &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; cat_train&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;write&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'{} 1\n'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;format&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;image&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
 
with open&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'{}/text.txt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;format&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;TXT_DIR&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'w'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; image &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; dog_test&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;write&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'{} 0\n'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;format&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;image&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; image &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; cat_test&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;write&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'{} 1\n'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;format&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;image&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Then, all you have to do is to execute the Python script you have just created above:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;python examples/DogsCatsKaggle/create_kaggle_txt.py&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Now let’s jump into &lt;em&gt;./data/DogsCatsKaggle&lt;/em&gt;, you will see &lt;em&gt;train.txt&lt;/em&gt; and &lt;em&gt;test.txt&lt;/em&gt; has been created. And that’s just it. We have finished creating the two mapping files for &lt;em&gt;Dogs vs Cats&lt;/em&gt; dataset from Kaggle!&lt;/p&gt;

&lt;p&gt;So, what about the Dogs and Cats images from ImageNet? Well, you may want to take a look at &lt;em&gt;./data/DogsCats&lt;/em&gt;. Voila! When were the two files created? - You may ask. They were created when you ran the script to download the images! So with ImageNet’s dataset, you don’t have to create the mapping files yourself. That was great, right? Now we got the images, the mapping text files ready, there’s only one step left to deal with the data: create the &lt;em&gt;mean image&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The need of computing the mean image&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Why do we need the mean image anyway? First, that’s just one type of &lt;em&gt;Data Normalization&lt;/em&gt;, a technique to process our data before training. As I told you in previous post, the final goal of the learning process is finding the global minimum of the cost function. There’s many factors that affect the learning process, one of which is how well our data was pre-processed. The better it is pre-processed, the more likely our Model will learn faster and better.&lt;/p&gt;

&lt;p&gt;The goal of computing the mean image is to make our data have zero mean. What does that mean? For example, we have a set of training data like this: \(x^{(1)}, x^{(2)}, \dots, x^{(m)}\). Let’s call \(x_\mu\) the mean value, which means:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_\mu=\frac{x^{(1)}+x^{(2)}+\dots+x^{(m)}}{m}=\frac{1}{m}\sum_{i=1}^mx^{(i)}&lt;/script&gt;

&lt;p&gt;Next, a new set of data will be created, where each \(x_{new}^{(i)}=x^{(i)}-x_\mu\). It’s easy to see that the mean value of the new dataset is zero:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^mx_{new}^{(i)}=\sum_{i=1}^mx^{(i)}-mx_\mu=\sum_{i=1}^mx^{(i)}-m\frac{1}{m}\sum_{i=1}^mx^{(i)}=0&lt;/script&gt;

&lt;p&gt;So, above I just showed you a short explanation about one type for Data Normalization, which subtracting by the mean value to get a new dataset with zero mean. I will talk more about Data Normalization in future post. Now, how do we compute the mean image? As you may guess, of course Caffe provides some script to deal with some particular dataset. And we’re gonna make use of it with some modifications! But before we can compute the mean image, we must convert our images into &lt;em&gt;LMDB&lt;/em&gt; format first.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Converting data into LMDB format&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But first, why LMDB? Why is LMDB converting considered recommended, especially when we are working with large image database? To make it short, because it helps improving the performance of our Network. At present, performance is not all about accuracy anymore, but required to be both fast and accurate. With a same Network and a same dataset, how the data was prepared will decide how fast our Network learns. And LMDB conversion is one way (among many) which helps accomplish that. And the trade-off? The converted LMDB file will double the size of your downloaded images, since your images were decompressed before being converted (that’s one reason why our Network performs faster with LMDB file, right?)&lt;/p&gt;

&lt;p&gt;Next, let’s copy the necessary script that we will make use of. I will use &lt;em&gt;Dogs vs Cats&lt;/em&gt; dataset from Kaggle for example.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;sudo cp &lt;/span&gt;examples/imagenet/create_imagenet.sh examples/DogsCatsKaggle/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;To convert the downloaded &lt;em&gt;Dogs vs Cats&lt;/em&gt; dataset to LMDB format using the script above, we will have to make some changes. But it’s not a big deal at all because all we have to change is just the correct path to our images and the mapping text files. Below is the lines which I have applied changes for your reference:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-vim&quot; data-lang=&quot;vim&quot;&gt;EXAMPLE&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;examples/DogsCatsKaggle
DATA&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;data/DogsCatsKaggle
TOOLS&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;build/tools
 
TRAIN_DATA_ROOT&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;data&lt;span class=&quot;sr&quot;&gt;/DogsCatsKaggle/&lt;/span&gt;train/
VAL_DATA_ROOT&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;data&lt;span class=&quot;sr&quot;&gt;/DogsCatsKaggle/&lt;/span&gt;train/
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
RESIZE&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;true
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
GLOG_logtostderr&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; $TOOLS/convert_imageset \
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
    $EXAMPLE/dogscatskaggle_train_lmdb

echo &lt;span class=&quot;s2&quot;&gt;&quot;Creating val lmdb...&quot;&lt;/span&gt;
 
GLOG_logtostderr&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; $TOOLS/convert_imageset \

&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
    $DATA/text&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;txt \
    $EXAMPLE/dogscatskaggle_val_lmdb
 
echo &lt;span class=&quot;s2&quot;&gt;&quot;Done.&quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Next, let’s go ahead and run the script above:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;./examples/DogsCatsKaggle/create_imagenet.sh &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;It will take a while for the conversion to complete. After the process completes, take a look at &lt;em&gt;./examples/DogsCatsKaggle&lt;/em&gt; folder, you will see two new folders which are named &lt;em&gt;dogscatskaggle_train_lmdb&lt;/em&gt; and &lt;em&gt;dogscatskaggle_val_lmdb&lt;/em&gt;, and new LMDB files were placed inside each folder, created from the training data and test data respectively.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Making the mean image&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After creating LMDB files, making the mean image is no other than one last simple task to complete. All we have to do is to copy and apply some tiny changes into the script which computes the mean image.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;sudo cp &lt;/span&gt;examples/imagenet/make_imagenet_mean.sh examples/DogsCatsKaggle/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And here’s what it looks after modified:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-vim&quot; data-lang=&quot;vim&quot;&gt;EXAMPLE&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;examples/DogsCatsKaggle
DATA&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;data/DogsCatsKaggle
TOOLS&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;build/tools
 
$TOOLS&lt;span class=&quot;sr&quot;&gt;/compute_image_mean $EXAMPLE/&lt;/span&gt;dogscatskaggle_train_lmdb \
  $DATA/dogscatskaggle_mean&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;binaryproto&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And, only one last command to execute:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;./examples/DogsCatsKaggle/make_imagenet_mean.sh &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And that’s it. Let’s go into &lt;em&gt;./data/DogsCatsKaggle&lt;/em&gt; folder, you will see one new file called &lt;em&gt;dogscatskaggle_mean.binaryproto&lt;/em&gt;, which means that the mean image was created successfully!&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Training with your prepared data&lt;/strong&gt;&lt;br /&gt;
So now you nearly got everything ready to train the Network with the data prepared by yourself. The last thing is, of course, the Network! At this time, you may want to create a Network of your own, and train it using the data above (of your own, too!). But I recommend you try some available Networks which is provided by Caffe, some of which are very famous such as VGG16 or AlexNet. Let’s pick AlexNet for now since it’s quite simpler than VGG16, which will make it train faster. We need to create one new folder and copy the necessary files for Network definition. And for your information, Caffe uses the &lt;em&gt;protobuf&lt;/em&gt; format to define the Networks, which you can read for details here: &lt;a href=&quot;https://developers.google.com/protocol-buffers/&quot; target=&quot;_blank&quot;&gt;Protocol Buffers&lt;/a&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;models
mkdif dogscatskaggle_alexnet
&lt;span class=&quot;nb&quot;&gt;sudo cp &lt;/span&gt;bvlc_alexnet/solver.prototxt dogscatskaggle_alexnet/
&lt;span class=&quot;nb&quot;&gt;sudo cp &lt;/span&gt;bvlc_alexnet/train_val.prototxt dogscatskaggle_alexnet/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Let’s first modify the &lt;em&gt;solver.prototxt&lt;/em&gt; first. This file stores the necessary information which the Network needs to know before training, such as the path to the Network definition file, the learning rate, momentum, weight decay iterations, etc. But all you need to do is just to change the file paths:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-vim&quot; data-lang=&quot;vim&quot;&gt;net&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;models/dogscatskaggle_alexnet/train_val.prototxt&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
snapshot_prefix&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;models/dogscatskaggle_alexnet/caffe_alexnet_train&quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Next, we will make change to the Network definition file, which is the &lt;em&gt;train_val.prototxt&lt;/em&gt; file. In fact, it was nearly set up and we only need to modify a little bit. First, we have to tell it where to look for your prepared data. And second, we must change the output layer, since our dataset only contains two classes (change this accordingly if you have a different dataset with me). Now open up the file, you will see the first two layers are the data layers, which provide the input to the Network. Stanford University has an excelent tutorial on defining the Network in Caffe at here: &lt;a href=&quot;http://vision.stanford.edu/teaching/cs231n/slides/caffe_tutorial.pdf&quot; target=&quot;_blank&quot;&gt;Caffe Tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Let’s change the path to the mean image and two LMDB folders which we created above:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-vim&quot; data-lang=&quot;vim&quot;&gt;name&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;AlexNet&quot;&lt;/span&gt;
layer &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  name&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;data&quot;&lt;/span&gt;
  type&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Data&quot;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;top&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;data&quot;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;top&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;label&quot;&lt;/span&gt;
  include &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    phase&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; TRAIN
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  transform_param &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    mirror&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; true
    crop_size&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;227&lt;/span&gt;
    mean_file&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;data/DogsCatsKaggle/dogscatskaggle_mean.binaryproto&quot;&lt;/span&gt; # MODIFIED
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  data_param &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    source&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;examples/DogsCatsKaggle/dogscatskaggle_train_lmdb&quot;&lt;/span&gt; # MODIFIED
    batch_size&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;256&lt;/span&gt;
    backend&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; LMDB
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
layer &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  name&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;data&quot;&lt;/span&gt;
  type&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Data&quot;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;top&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;data&quot;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;top&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;label&quot;&lt;/span&gt;
  include &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    phase&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; TEST
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  transform_param &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    mirror&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; false
    crop_size&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;227&lt;/span&gt; 
    mean_file&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;data/DogsCatsKaggle/dogscatskaggle_mean.binaryproto&quot;&lt;/span&gt; # MODIFIED
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  data_param &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    source&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;examples/DogsCatsKaggle/dogscatskaggle_val_lmdb&quot;&lt;/span&gt; # MODIFIED
    batch_size&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;50&lt;/span&gt;
    backend&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; LMDB
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;And there’s only one place left to change: the output layer. Let’s look through the file to find the layer named &lt;em&gt;fc8&lt;/em&gt;, that’s the last layer of our Network. It now has 1000 outputs because it was created to train on full ImageNet’s images. Let’s change the number of output according to our dataset:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-vim&quot; data-lang=&quot;vim&quot;&gt;layer &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  name&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;fc8&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
  inner_product_param &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    num_output&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt; # MODIFIED
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Then save the file and that’s it, you can now train the Network with your own dataset! We can’t wait to do it, can we?&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;./build/tools/caffe train &lt;span class=&quot;nt&quot;&gt;--solver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;models/dogscatskaggle_alexnet/solver.prototxt&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Our Network should be running flawlessly now. And all we have to do is wait until it’s done! We have come a long way until this point. So I think we deserve a cup of coffee or something. That was so fantastic! You all did a great job today.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;So in today’s post, I have shown you how to train the Network in Caffe, using your own dataset. We went through from how to download the data from URLs file (or directly from host), how to prepare the data to be read by the Network and how to make change to the Network to make it work using our dataset. As you could see, it was not so hard, but it did require some time to dig into. I hope this post can save you quite some of your previous times, and instead, you can spend them on improving your Network’s performance. And that’s all for today. Thank you for reading such a long post. And I’m gonna see you again in the coming post!&lt;/p&gt;</content><author><name>Mahaveer Suthar</name><email>mahaveer0suthar@gmail.com</email></author><category term="machine-learning" /><category term="deep-learning" /><category term="caffe" /><category term="installation" /><category term="gpu" /><category term="training" /><category term="fine-tuning" /><category term="own data" /><category term="essential" /><summary type="html">Hi, everyone! Welcome back to my Machine Learning page today. I have been playing around with Caffe for a while, and as you already knew, I made a couple of posts on my experience in installing Caffe and making use of its state-of-the-art pre-trained Models for your own Machine Learning projects. Yeah, it’s really great that Caffe came bundled with many cool stuff inside which leaves developers like us nothing to mess with the Networks. But of course, there comes sometime that you want to set up your own Network, using your own dataset for training and evaluating. And it turns out that using all the things which Caffe provides us doesn’t help Caffe look less like a blackbox, and it’s pretty hard to figure things out from the beginning. And that’s why I decided to make this post, to give you a helping hand to literally make use of Caffe.</summary></entry><entry><title type="html">Machine Learning Part 10: Linear Support Vector Machine</title><link href="http://localhost:4000/mahaveer0suthar.github.io/tutorial/Support-Vector-Machine/" rel="alternate" type="text/html" title="Machine Learning Part 10: Linear Support Vector Machine" /><published>2017-11-05T00:00:00+05:30</published><updated>2017-11-05T00:00:00+05:30</updated><id>http://localhost:4000/mahaveer0suthar.github.io/tutorial/Support-Vector-Machine</id><content type="html" xml:base="http://localhost:4000/mahaveer0suthar.github.io/tutorial/Support-Vector-Machine/">&lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;p&gt;Hi guys! It’s been while since my last tutorial post about Regularization. And today, as I promised, I’m gonna talk about one supervised learning algorithm which took the throne of Neural Network a decade ago. It was fast, agile and outperformed almost the algorithms back in the days. Guys, today I want to tell you about Support Machine Learning, or SVM for short.&lt;/p&gt;

&lt;p&gt;Many of you may have heard about the term SVM. For example, if you have experience in Computer Vision, especially using OpenCV to accomplish the task, you may have seen something like this on OpenCV’s page in HOG Descriptor section:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;OpenCV provides an Linear SVM Model for People detection&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;And you will likely come across in other places with the same content, which gives us some proof of the irresistible power of SVM. Despite the fact that there are a great deal of supervised learning algorithms out there nowadays, SVM is still among the mostly applied algorithms. And everytime I face a new Machine Learning problem, the first algorithm I apply is SVM, not only for its performance, but also for its speed and easy-to-implement mechanism, which can give me an overview of the problem as fast as I expect.&lt;/p&gt;

&lt;p&gt;Above is a brief introduction about SVM. Now let’s go finding the anwser for the question we are longing for: What is SVM?&lt;/p&gt;

&lt;p&gt;SVM is a supervised learning algorithm which is mostly used for classification problems. It can perform well no matter our dataset is linear or non-linear distributed. But first, to make it easy to understand, in today’s post I’m gonna talk only about how SVM work when dealing with linear data, which can also be called Linear SVM algorithm.&lt;/p&gt;

&lt;p&gt;And you may remember that I had made a post about one learning algorithm which can give awesome result when dealing with linear dataset. Yeah, I’m talking about Logistic Regression. So, to have a better understanding about Linear SVM, it’s a great idea to recall a little bit about Logistic Regression, and see what they differ from each other. For ones who haven’t skimmed through my post about Logistic Regression, you can find it right below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://mahaveer0suthar.github.io/tutorial/Logistic-Regression/&quot; target=&quot;_blank&quot;&gt;Machine Learning Part 6: Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;When we talk about Logistic Regression, we may all think of the sigmoid function, which we use as the activation function. Below is what a sigmoid function looks like:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_\theta(X)=\frac{1}{1+e^{-\theta^TX}}&lt;/script&gt;

&lt;p&gt;As I told you before, using sigmoid function will ensure that the output will be restricted in the range between \(0\) and \(1\), which then assigned to either \(0\) or \(1\) depends on its value and the threshold value. And of course, after getting the predictions with the help of the sigmoid function, we cannot evaluate the Model’s performance without a cost function (or loss function). And the cost function we use in Logistic Regression is the cross-entropy function, which can also be called the log-likelihood cost function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta)=-\frac{1}{m}\sum_{i=1}^m\left[y^{(i)}\log(h_\theta(X^{(i)}))+(1-y^{(i)})\log(1-h_\theta(X^{(i)})\right]&lt;/script&gt;

&lt;p&gt;So now if I set \(cost_1(\theta^TX)=-log\left(\frac{1}{1+e^{-\theta^TX}}\right)\), and \(cost_0(\theta^TX)=-log\left(1-\frac{1}{1+e^{-\theta^TX}}\right)\) in case of \(\theta^TX&amp;gt;=0\) and \(\theta^TX&amp;lt;0\) respectively, we can re-write the cost function in a simple form like below:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta)=\frac{1}{m}\sum_{i=1}^m\left[y^{(i)}cost_1(\theta^TX)+(1-y^{(i)})cost_0(\theta^TX)\right]&lt;/script&gt;

&lt;p&gt;Not let’s consider the graph of each seperate part which I divided above:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(cost_1(\theta^TX)\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/tutorials/support-vector-machine/cost_1.png&quot; alt=&quot;cost_1&quot; /&gt;&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;As you can see, the \(cost_1(\theta^TX)\) term will be very large when \(\theta^TX\) is close to zero, and decrease toward zero as the value of \(\theta^TX\) increases. What does this mean? Before answering that question, let’s consider the other one:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(cost_0(\theta^TX)\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/tutorials/support-vector-machine/cost_0.png&quot; alt=&quot;cost_0&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Similar to the \(cost_1(\theta^TX)\) above, the \(cost_0(\theta^TX)\) term will be extremely large when \(\theta^TX\) is close to zero, but this time decrease toward zero as \(\theta^TX\) goes toward to the left.&lt;/p&gt;

&lt;p&gt;The two terms above were divided from our cost function, which means that their values will be accumulated to the cost function. And our target is to minimize the cost function, you remember that? So, the smaller the two terms are, the smaller the cost function becomes. The smaller the cost function is, the closer our Predictions are comparing to the Label \(y\).&lt;/p&gt;

&lt;p&gt;Now, let’s consider the \(cost_1(\theta^TX)\) term. We compute this term only when the corresponding label \(y=1\). As we saw in the graph above, when \(\theta^TX\approx0\), \(cost_1(\theta^TX)\) becomes very large. That is because we now have the probability that our Model predict the label \(y=1\) is very small, and may be even worse if it predict the label to be \(0\). As the result, the cost function will become large as a penalty. In contrast, if \(\theta^TX\) is much greater than \(0\), then the probability that \(y=1\) will be higher. And as the probability becomes nearly \(1\), we will have a nearly \(0\) cost value.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;You can explain the \(cost_0(\theta^TX)\) term in the same way. As a conclusion, we will have a result like this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
y_{predict} = \cases{ 1 &amp; \text{if } \theta^TX \ge 0 \cr 0 &amp; \text{if } \theta^TX \lt 0} %]]&gt;&lt;/script&gt;

&lt;p&gt;The conclusion above is something which I had shown you in the end of the Logistic Regression tutorial, right? This time I just want to make it more clear if we explain it with considering the effect toward the cost function. Now let’s move to the case of Linear SVM.&lt;/p&gt;

&lt;h3 id=&quot;linear-svms-cost-function&quot;&gt;Linear SVM’s Cost Function&lt;/h3&gt;

&lt;p&gt;After doing some revision on Logistic Regression. Let’s see what the cost function of Linear SVM looks like. First, let me re-write the cost function above, with the use of the two \(cost_1(\theta^TX)\) and \(cost_0(\theta^TX)\) terms, but this time without omitting the regularization term:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta)=\frac{1}{m}\sum_{i=1}^m\left[y^{(i)}cost_1(\theta^TX)+(1-y^{(i)})cost_0(\theta^TX)\right]+\frac{\lambda}{2m}\sum^m_{j=1}\theta_j^2&lt;/script&gt;

&lt;p&gt;To make it even simpler, I will omit the \(\frac{1}{m}\) factor. It may affects the value of our cost function, but it doesn’t affect the way our algorithm works, since we are just eliminating a constant from a function.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta)=\sum_{i=1}^m\left[y^{(i)}cost_1(\theta^TX)+(1-y^{(i)})cost_0(\theta^TX)\right]+\frac{\lambda}{2}\sum^m_{j=1}\theta_j^2&lt;/script&gt;

&lt;p&gt;Now, the new cost function looks like above. We can see that it has the form of: \(\mathbf{A}+\lambda\mathbf{B}\). Let’s talk a little bit about the \(\lambda\) term, as we call it the weight of regularization, which control how much we want to regularize our parameters. If it’s large, then our parameters will become much smaller and vice versa. Skipped my previous post? You can find it right below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://mahaveer0suthar.github.io/tutorial/Regularization/&quot;&gt;Machine Learning Part 9: Regularization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now how about saying we want to put more weight on the actual cost value? The answer may be: just decrease \(\lambda\). It’s a little bit confusing to someone, so instead of using the form of \(\mathbf{A}+\lambda\mathbf{B}\), many people prefer the \(\mathbf{C}\mathrm{A}+\mathrm{B}\) form. So now we can say, if we want to emphasize on the actual cost value, we can do it by increasing \(\mathbf{C}\). And that way of expression is also the standard which the scikit-learn library are using. For example, here’s the full description when initializing Logistic Regression I grabbed on scikit-learn’s homepage:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/tutorials/support-vector-machine/sklearn_reg_c.png&quot; alt=&quot;sklearn_reg_c&quot; /&gt;&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;With that approach, let’s re-write our cost function again, using the inverse of regularization weight \(\mathbf{C}\) instead of \(\lambda\), here’s what we will have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta)=\mathbf{C}\sum_{i=1}^m\left[y^{(i)}(-\log(h_\theta(X^{(i)})))+(1-y^{(i)})(-\log(1-h_\theta(X^{(i)}))\right]+\frac{1}{2}\sum^m_{j=1}\theta_j^2&lt;/script&gt;

&lt;p&gt;And as I told you above about deciding the value of the Predictions by considering its effect on the value of our cost function, which I can also say that the way we compute our cost function, or the two \(cost_1(\theta^TX)\) and \(cost_0(\theta^TX)\) terms will affect the way our Model predicts the output. And here’s what Linear SVM differs from Logistic Regression. We will modify the two cost terms a little bit, to have the new graphs like below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Linear SVM’s \(cost_1(\theta^TX)\) graph&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/tutorials/support-vector-machine/cost_1_svm.png&quot; alt=&quot;cost_1_svm&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Linear SVM’s \(cost_0(\theta^TX)\) graph&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/tutorials/support-vector-machine/cost_0_svm.png&quot; alt=&quot;cost_0_svm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Obviously, as you can see, the two cost terms of Linear SVM looks different from what we saw in Logistic Regression. It the difference in how we define the cost terms in Linear SVM makes it predict in a different way. Telling you about this now may makes you feel confusing a little. But in fact, many cannot tell the difference between Linear SVM and Logistic Regression, since they seem to work the same way. So before I talk further, I think it’s good to notice the difference right this time, so you won’t make any unexpected assumption.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;predictions-of-linear-svm&quot;&gt;Predictions of Linear SVM&lt;/h3&gt;

&lt;p&gt;So, as you see from the graphs of the Linear SVM’s cost terms above. They look pretty much like what we saw in Logistic Regression except two things. First: instead of the non-linear graph which we obtained by the logarithmic function, now we have a new graph with two parts, one part which the cost values are \(0\), and the other part which values are not \(0\) is now linear. The second thing that Linear SVM differs from Logistic Regression is, the constraint to decide the value of the Prediction is now a little bit harder. Linear SVM requires a &lt;strong&gt;safety margin&lt;/strong&gt; when deciding the Prediction, which we can express like below:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
y_{predict} = \cases{ 1 &amp; \text{if } \theta^TX \ge 1 \cr 0 &amp; \text{if } \theta^TX \lt -1} %]]&gt;&lt;/script&gt;

&lt;p&gt;Intuitively, this safety margin is the reason why SVM is called the Maximum Margin Classifier, which I told you earlier in this post. What the algorithms does is to find a decision boundary which can obtain the maximum margins from the nearest point of each class. We will have a better visualization of &lt;strong&gt;maximum margin&lt;/strong&gt; right below, in the Implementation section.&lt;/p&gt;

&lt;p&gt;So, that’s all about Linear SVM. As you can see, if you have already known about Logistic Regression, it’s pretty easy to understand Linear SVM since they have some similar behavior in between. And the main point which drives Linear SVM apart from Logistic Regression is how we define the cost terms in Linear SVM, which then affects the way it decides the value of our Predictions. And now, after reading through a great deal of “lecture”, let’s jump into the Implementation section!&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;So, we finally come to the Implementation of Linear SVM. And just like Logistic Regression and Decision Tree, scikit-learn library provides us a well pre-implemented Linear SVM. All we have to do is… just use it!&lt;/p&gt;

&lt;p&gt;You may now became very familiar with scikit-learn library as well as some Python codes we used for data initialization or graph drawing, so I won’t talk about those so much this time. And now, let’s get our hands dirty, by import all the necessary stuff we’re gonna use:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; 
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.colors&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ListedColormap&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisticRegression&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.tree&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DecisionTreeClassifier&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.svm&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SVC&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_classification&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Next, we will create our data using &lt;em&gt;make_classification&lt;/em&gt; method:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; 
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_classification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;60&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_informative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_redundant&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;94&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;To make it simple, this time we just created a dataset with two features and two classes only. Next, let’s create our Linear SVM model, and train it using the data created above:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; 
&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SVC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'linear'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You may wonder what the &lt;em&gt;kernel&lt;/em&gt; parameter means. But I will talk about it more in the later post, so now just implement as I did. We set its value “linear” so that scikit-learn knows we want to create a Linear SVM Model.&lt;/p&gt;

&lt;p&gt;Next, we will draw the decision boundary which seperates the points of two classes for better visualization of the Model’s performance. To help you recall a little bit, the decision boundary in Logistic Regression seperates the coordinate plane into two parts like below (in case we have a dataset of two features and two classes):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
y_{predict} = \cases{ 1 &amp; \text{if } \theta_0+\theta_1X_1+\theta_2X_2 \ge0 \cr 0 &amp; \text{if } \theta_0+\theta_1X_1+\theta_2X_2 \lt0} %]]&gt;&lt;/script&gt;

&lt;p&gt;And through the &lt;em&gt;coef_&lt;/em&gt; and &lt;em&gt;intercept_&lt;/em&gt; attributes of the trained Model, we can use them to draw the decision boundary, just like what we did with Logistic Regression.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; 
&lt;span class=&quot;n&quot;&gt;xx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1_min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x1_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coef_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;yy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intercept_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;And remember that Linear SVM is different from Logistic Regression by the way it defines the cost terms, which then affects the way it decides the value of our Predictions. Concretely, SVM will tend to keep a safety margin when making Predictions, so we’re gonna compute the upper boundary and the lower boundary to help visualize the term &lt;strong&gt;maximum margin&lt;/strong&gt; better:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; 
&lt;span class=&quot;n&quot;&gt;margin&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coef_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;yy_down&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;margin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;yy_up&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;margin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And finally, we now have everything ready, let’s go ahead and plot everything on:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; 
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2_min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rainbow'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'k-'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yy_down&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'k--'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yy_up&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'k--'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And here’s the result I received:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/tutorials/support-vector-machine/maximum-margin.png&quot; alt=&quot;maximum_margin&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you could see in the graph above, what Linear SVM did is to find a decision boundary which can keep the maximum margins between the nearest point of each class. And that’s the reason why SVM is usually called the &lt;strong&gt;maximum margin classifier&lt;/strong&gt;. And through implementing Linear SVM as well as drawing both the upper and lower boundaries, I hope you now have a better visualization of what Linear SVM does.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;So, thank you for staying with me until the end, in my longest post ever. We have talked about Linear SVM, and of course, a little bit deeper about Logistic Regression, just to help you understand better how the two algorithms differ from each other. I hope after this post, you can both have a deep understand about Logistic Regression, and add Linear SVM, one of the most powerful algorithm to your Machine Learning toolbox. In the next post, I will continue with Support Vector Machine, but there won’t be any linear data any more. Next time we will see how SVM can deal with non-linear distributed data, by using something called: the kernel trick. Until then, stay tuned and I will be right back!&lt;/p&gt;</content><author><name>Mahaveer Suthar</name><email>mahaveer0suthar@gmail.com</email></author><category term="machine-learning" /><category term="support vector machine" /><category term="svm" /><category term="classification" /><category term="regularization" /><category term="SVC" /><category term="essential" /><summary type="html">Hi guys! It’s been while since my last tutorial post about Regularization. And today, as I promised, I’m gonna talk about one supervised learning algorithm which took the throne of Neural Network a decade ago. It was fast, agile and outperformed almost the algorithms back in the days. Guys, today I want to tell you about Support Machine Learning, or SVM for short.</summary></entry><entry><title type="html">Solving problem when running Faster R-CNN on GTX 1070</title><link href="http://localhost:4000/mahaveer0suthar.github.io/project/Problem-Faster-RCNN-GPU/" rel="alternate" type="text/html" title="Solving problem when running Faster R-CNN on GTX 1070" /><published>2017-11-03T00:00:00+05:30</published><updated>2017-11-03T00:00:00+05:30</updated><id>http://localhost:4000/mahaveer0suthar.github.io/project/Problem-Faster-RCNN-GPU</id><content type="html" xml:base="http://localhost:4000/mahaveer0suthar.github.io/project/Problem-Faster-RCNN-GPU/">&lt;p&gt;Hello guys, it’s great to be here with you today (why do I keep saying that boring greeting, you may ask). To be honest, there are a lot of things I want to share to you, especially since I built my own machine for Deep Learning. Of course, having my own machine is great, it allows me to try every crazy idea which has ever crossed through my mind, without giving a damn thought about the payment. However, good thing is followed by troubles, as it always be.&lt;/p&gt;

&lt;p&gt;You might read my last post about my experience in installing Caffe on Ubuntu 16.04, with CUDA and cuDNN to make use of the great power of the GPU. Yeah, we went through so many steps to install so many necessary things. And fortunately, things worked flawlessly in the end. For ones who haven’t read it yet, you can find it right below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://mahaveer0suthar.github.io/project/Installing-Caffe-Ubuntu/&quot; target=&quot;_blank&quot;&gt;Installing Caffe on Ubuntu 16.04 &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And as you can guess, the next thing I did right after having Caffe installed on my machine, is grabbing the latest Python implementation of Faster R-CNN. I once talked about how to compile and run Faster R-CNN on Ubuntu in CPU Mode, you can refer to it here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://mahaveer0suthar.github.io/project/Running-Faster-RCNN-Ubuntu/&quot; target=&quot;_blank&quot;&gt;Compiling and Running Faster R-CNN on Ubuntu (CPU Mode)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MyPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2103500933&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Obviously, once you are able to run Faster R-CNN in CPU Mode, making it work with GPU may not sound like a big deal. Why? Because you had successfully installed Caffe, which means you had gone through all the most confusing steps to get CUDA and cuDNN libraries ready. But to tell the truth, I failed to, on the first try!&lt;/p&gt;

&lt;p&gt;It was a shock to me, which took me a while to overcome. Then I soon realized that, it’s all on me now because it was me who built my own machine. So I had no choice, but to figure it out myself. And after just more than one hour, the problem was solved. The problem always looks harder than it’s supposed to be. I’ve been taught that simple thing so many times in my life, and I just kept forgetting about it. And that’s also the reason why I’m writing this post, to share with you some experience to deal with this kind of troubles.&lt;/p&gt;

&lt;p&gt;Let’s start from the beginning. I was so excited to get everything ready right after installing Ubuntu, so I immediately jumped into installing Caffe without any consideration. And as you may guess, I grabbed all in the latest version, which means that at first, I installed cuDNN v5.1 to CUDA installation folder.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Things worked just right with Caffe, until it came to Faster R-CNN. To make it more clear, I downloaded the latest Python implementation of Faster R-CNN from their GitHub as before:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;git&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clone&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recursive&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;https&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;github&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;com&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rbgirshick&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;faster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcnn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;git&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;After that, I compiled the libraries by jumping into &lt;em&gt;lib&lt;/em&gt; folder and run:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;cd&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lib&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;make&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Luckily, there’s no need to change anything this time. Phew! Then I went backward, then jumped into the &lt;em&gt;caffe-fast-rcnn&lt;/em&gt; folder, create &lt;em&gt;Makefile.config&lt;/em&gt; from &lt;em&gt;Makefile.config.example&lt;/em&gt; file, then applied some necessary modifications to it. That was exactly the same as what we did with Caffe’s installation. You can refer to the link I showed above. And it’s time to compile Caffe for Faster R-CNN:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;cd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;../&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;caffe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fast&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcnn&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;make&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j8&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pycaffe&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And I received some unexpected result, like below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/problem-faster-rcnn-gpu/cudnn5_error.png&quot; alt=&quot;cudnn5_error&quot; /&gt;&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MyPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2103500933&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Obviously, as the errors were self-explained, there was something wrong with the cuDNN v5.1 library. Did I just say &lt;em&gt;v5.1&lt;/em&gt;? That was all my bad, since I forgot that Faster R-CNN is still &lt;strong&gt;incompatible&lt;/strong&gt; with cuDNN v5.1. This wouldn’t happen if I considered carefully before installing cuDNN. But that was an easy fix, since it seemed like replacing with &lt;em&gt;cuDNN v4&lt;/em&gt; helps fix the problem. So I gave that thought a try. The installation of cuDNN v4 is exactly same as cuDNN v5.1 so I omit it from here.&lt;/p&gt;

&lt;p&gt;After re-installing cuDNN, I ran it again to see if it works:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;make&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clean&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# to delete the previous progress
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j8&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pycaffe&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;No errors shown this time. Thank God, seems like it works now, I thought.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/problem-faster-rcnn-gpu/cudnn4_ok.png&quot; alt=&quot;cudnn4_ok&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So I moved on to prepare the pre-trained model, just like the instructions on their GitHub repository:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;cd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;..&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;./&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scripts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fetch_faster_rcnn_models&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sh&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;It took some minutes to complete, since the model is quite large in size. Now, I got everything ready and couldn’t wait any longer to run the &lt;em&gt;demo&lt;/em&gt;:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;./&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tools&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;demo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And here’s the result I had, nothing seems to go wrong, I guessed:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/problem-faster-rcnn-gpu/cudnn4_no_image.png&quot; alt=&quot;cudnn4_no_image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To tell the truth, I think I’m a very patient guy. So I just left it there for a while. “So, where are all the images?”, I nearly talked to the screen. The reason why I couldn’t stay calm is that, this time there were no errors shown, and no images came out to screen, either. It took me another while to admit that something was going wrong somewhere, and I had to figure it out. Since we ran the &lt;em&gt;demo.py&lt;/em&gt; file, then looking at that file first may help find something.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Since I had read the paper of Faster R-CNN before, so it was somehow easy to understand what each part of the code is doing. To recap a little bit, as I shown you in the previous post, here’s the result we want to see:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/running-faster-rcnn-ubuntu/1.png&quot; alt=&quot;result&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So what we want is an image (at least), in which each detected object was bounded by a rectangle, with some text to indicate which class it belongs to.
Therefore, it’s not the complicated code used for detecting, but the two parts below that I want you to focus into:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/problem-faster-rcnn-gpu/demo_py.png&quot; alt=&quot;demo_py&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can refer to the paper of Faster R-CNN to find some more details. To make it easy to understand, Faster R-CNN searched for some regions which likely contains an object, then each object was detected with probabilities to indicate how likely that object belongs to a particular class. Let’s open the &lt;em&gt;demo.py&lt;/em&gt; file, and make the following modification for a better understand:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;demo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
     &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Detect object classes in an image using pre-computed object proposals.&quot;&quot;&quot;&lt;/span&gt;
 
     &lt;span class=&quot;c1&quot;&gt;# Load the demo image
&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;im_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cfg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DATA_DIR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'demo'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;im&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 
     &lt;span class=&quot;c1&quot;&gt;# Detect all object classes and regress object bounds
&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;timer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Timer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;timer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_detect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
     &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# ADD THIS LINE!!!
&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;timer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Add one line like above inside the &lt;em&gt;demo&lt;/em&gt; method in &lt;em&gt;demo.py&lt;/em&gt; file so that we can know the shape of the output scores, then run it again, here’s what I received:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;Loaded&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;home&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mahaveer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;faster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcnn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;faster_rcnn_models&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VGG16_faster_rcnn_final&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;caffemodel&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Demo&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;demo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;000456.j&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pg&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Detection&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;took&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.069&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proposals&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Demo&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;demo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;000542.j&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pg&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;259&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Detection&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;took&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.064&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;259&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proposals&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Demo&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;demo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;001150.j&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pg&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;223&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Detection&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;took&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.054&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;223&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proposals&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Demo&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;demo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;001763.j&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pg&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;201&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Detection&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;took&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.052&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;201&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proposals&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Demo&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;demo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;004545.j&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pg&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;172&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Detection&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;took&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.051&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;172&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proposals&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MyPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2103500933&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The model used for demo file was fine-tuned so that it can classify 20 classes, plus one &lt;em&gt;background&lt;/em&gt; class, so we have 21 classes in total. Let’s look at the result of the first image. The algorithm proposed 300 object regions (or we can say 300 rectangles can be drawn at max), and the corresponding scores had the shape of (300, 21). It means that each proposal, the algorithm computed all 21 probabilities for all classes. And we will base on those probabilities to decide which class it belongs to.&lt;/p&gt;

&lt;p&gt;Now let’s look at the first part which I highlighted above. What it’s doing is getting the first probability which is greater than the threshold value (it was set to 0.5 this time). So, let’s print out the probabilities to see why it couldn’t get through the &lt;em&gt;if&lt;/em&gt; condition which followed:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;vis_detections&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thresh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
     &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Draw detected bounding boxes.&quot;&quot;&quot;&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;inds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thresh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
     &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# ADD THIS LINE!!!
&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Let’s add the line above, right before the &lt;em&gt;if&lt;/em&gt; condition, then run it again to see what happens:&lt;/p&gt;

&lt;p&gt;And here’s the result:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/problem-faster-rcnn-gpu/low_proba.png&quot; alt=&quot;low_proba&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And at this time, I thought I knew where the problem came from. None of the probabilities was greater than 0.5, so obviously I ended up with no images being shown. Then, an idea came through my mind. “Why don’t I try to run on CPU?”, I thought.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;./&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tools&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;demo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The result was like below. I was totally speechless.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/problem-faster-rcnn-gpu/cpu_ok.png&quot; alt=&quot;cpu_ok&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So I got to go through such a long way, to find out the place which may cause the problem. And thanks to the CPU, or I have to say: I had to make use of the CPU to be sure whether I got it right. Anyway, at least I knew that the compilation was successfully got through. And, the problem is likely from the GPU. Wait, what? The most expensive part of the machine caused the problem? That was ridiculous, I thought. But I couldn’t admit that, so I ran into the net right after. The bad news is: yeah, it was caused by the GPU. And the good news? The good news is: the problem happened because the new GPUs don’t support cuDNN older than v5. Well, you gotta be kidding me. Caffe doesn’t work with cuDNN newer than cuDNN v4, and my GTX 1070 doesn’t support cuDNN older than v5. So does it mean that I won’t be able to run Faster R-CNN on my machine?&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Thinking it that way did depress me a lot. But fortunately, it turns out that re-compiling Faster R-CNN without cuDNN help solve it. So I moved on and gave it my last shot. I opened the &lt;em&gt;Makefile.config&lt;/em&gt;, comment out the &lt;em&gt;USE_CUDNN&lt;/em&gt; option, then compile it again. After the compilation completes, I ran it again, hoped that it works for me this time:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/problem-faster-rcnn-gpu/gpu_ok.png&quot; alt=&quot;gpu_ok&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It finally worked! That was like you woke up after a nightmare. But it was fantastic! I mean, the feeling when you could finally make things done is hard to express, right?&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MyPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2103500933&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;In today’s post, I have shown you a problem when running Faster R-CNN with the GTX-1070, which caused the images not being shown when it’s done. Going through every step above, it seemed like the problem was not that hard, and that I didn’t have to make such a long post. As I mentioned earlier, sometimes you will have to deal with the problems yourself, and sometimes, searching Google for the answer in the very beginning is not a good practice at all. As an developer, especially a Machine Learning developer, it’s likely that you are working with not only codes, but also many hard-to-understand algorithms, at least trying to figure things out yourself in the beginning will help you understand the problem more deeper so that the community can get involved efficiently, it also help you gain some very precious experience. And through times, you will be less likely to give up when struggling problems.&lt;/p&gt;

&lt;p&gt;That’s all I want to tell you today. I’ll be more than happy if you find this post helpful. Now, goodbye and see you in the next post!&lt;/p&gt;</content><author><name>Mahaveer Suthar</name><email>mahaveer0suthar@gmail.com</email></author><category term="machine-learning" /><category term="faster r-cnn" /><category term="gpu" /><category term="gtx 1070" /><category term="image not showing" /><category term="caffe" /><category term="compile" /><category term="cpu mode" /><category term="essential" /><summary type="html">Hello guys, it’s great to be here with you today (why do I keep saying that boring greeting, you may ask). To be honest, there are a lot of things I want to share to you, especially since I built my own machine for Deep Learning. Of course, having my own machine is great, it allows me to try every crazy idea which has ever crossed through my mind, without giving a damn thought about the payment. However, good thing is followed by troubles, as it always be.</summary></entry><entry><title type="html">Installing Caffe on Ubuntu 16.04 (GPU Mode with CUDA)</title><link href="http://localhost:4000/mahaveer0suthar.github.io/project/Installing-Caffe-Ubuntu/" rel="alternate" type="text/html" title="Installing Caffe on Ubuntu 16.04 (GPU Mode with CUDA)" /><published>2017-11-01T00:00:00+05:30</published><updated>2017-11-01T00:00:00+05:30</updated><id>http://localhost:4000/mahaveer0suthar.github.io/project/Installing-Caffe-Ubuntu</id><content type="html" xml:base="http://localhost:4000/mahaveer0suthar.github.io/project/Installing-Caffe-Ubuntu/">&lt;p&gt;It’s great to be with all you guys again in today’s post. As you already knew, it’s been a while since I built my own desktop for Deep Learning. Or for ones who missed that post, you can have a look at my build here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://mahaveer0suthar.github.io/project/Building-Desktop-For-Deep-Learning/&quot; target=&quot;_blank&quot;&gt;Building up my own machine for Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And in the last few days, I was like a kid who had just received some new toys from his parents (I bought my desktop by my own money, though). I was so excited that I couldn’t wait any longer to get started. So right after I put all the parts right into their places, the first thing I got to do was installing the OS, of course. I’m running Ubuntu 16.04 on my laptop, so I couldn’t find any reason for not installing the latest Long-Term-Support version of Ubuntu on my desktop. The OS installation was quite easy, especially Ubuntu or any Linux based OS. The next thing to do was to install the necessary drivers. Actually nearly all the drivers were installed during the installation of Ubuntu, so I only had to manually install the GTX 1070 driver, but it was a piece of cake and you would laugh at me if I write it down here. In this post, I want to talk about the three main points below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Installing Caffe on Ubuntu 16.04 in GPU Mode&lt;/li&gt;
  &lt;li&gt;Comparing the performance between CPU and GPU using MNIST and CIFAR-10 datasets&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As you may notice that I once talked about the first one in my previous posts. Actually I didn’t have myself a desktop with GPU in it, so that post was mainly about how to make things work only by using CPU. And obviously I can’t just do the same thing this time if I want the GTX 1070 to be on the field. In short, there’s a great deal of extra work to do if you want to make use the power of your GPU. And in this post I’m gonna show you how.&lt;/p&gt;

&lt;h3 id=&quot;installing-caffe-on-ubuntu-1604-in-gpu-mode&quot;&gt;Installing Caffe on Ubuntu 16.04 in GPU Mode&lt;/h3&gt;

&lt;p&gt;The first thing to do before installing Caffe was to install OpenCV, because I wanted to compile Caffe with OpenCV. Installing OpenCV wasn’t a big deal at all. You can refer at my previous post here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://mahaveer0suthar.github.io/tutorial/Setting-Up-Python-Environment-For-Computer-Vision-And-Machine-Learning/&quot; target=&quot;_blank&quot;&gt;Installing OpenCV &amp;amp; Keras&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To make it more convenient for you without having to switch between your browser tabs, I think it’s better if I write out the steps for installing OpenCV in this post, too.&lt;/p&gt;

&lt;p&gt;First, because I got in my desktop a fresh and new OS, I had to perform the commands below to make sure everything is updated to the latest version:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get update
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get upgrade&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Type the password if prompted. When the process completes, let’s install all the necessary packages:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;build-essential cmake git pkg-config libjpeg8-dev &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
libjasper-dev libpng12-dev libgtk2.0-dev &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
libavcodec-dev libavformat-dev libswscale-dev libv4l-dev gfortran
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;libtiff5-dev &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And we need a library for computing optimization purpose. We will use BLAS just like before:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;libatlas-base-dev&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Next, we will install &lt;strong&gt;pip&lt;/strong&gt;, a useful tool for managing all Python packages:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;wget https://bootstrap.pypa.io/get-pip.py
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;python get-pip.py&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In order to use OpenCV and Caffe, we need to install Python Development Tools package:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;python2.7-dev&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And of course, a powerful module for dealing with arrays, Numpy:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;numpy&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;In the next step, let’s download OpenCV 3.0 and its contribution modules:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~
git clone https://github.com/Itseez/opencv.git
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;opencv
git checkout 3.0.0
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~
git clone https://github.com/Itseez/opencv_contrib.git
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;opencv_contrib
git checkout 3.0.0&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Note that you have to tell &lt;strong&gt;git&lt;/strong&gt; to checkout to &lt;em&gt;3.0.0&lt;/em&gt; branch. Now we have everything ready, let’s go and make it:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~/opencv
&lt;span class=&quot;nb&quot;&gt;mkdir &lt;/span&gt;build
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;build
cmake &lt;span class=&quot;nt&quot;&gt;-D&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;CMAKE_BUILD_TYPE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;RELEASE &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
	&lt;span class=&quot;nt&quot;&gt;-D&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;CMAKE_INSTALL_PREFIX&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/local &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
	&lt;span class=&quot;nt&quot;&gt;-D&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;INSTALL_C_EXAMPLES&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ON &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
	&lt;span class=&quot;nt&quot;&gt;-D&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;INSTALL_PYTHON_EXAMPLES&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ON &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
	&lt;span class=&quot;nt&quot;&gt;-D&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;OPENCV_EXTRA_MODULES_PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;~/opencv_contrib/modules &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
	&lt;span class=&quot;nt&quot;&gt;-D&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;BUILD_EXAMPLES&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ON ..
  
make
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;make &lt;span class=&quot;nb&quot;&gt;install
sudo &lt;/span&gt;ldconfig&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;It should take a while to complete the installation. Try to have yourself a cup of coffee or something, because we are just half way there, lol.&lt;/p&gt;

&lt;p&gt;After the installation finishes, let’s check if everything works:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;python
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; import cv2
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; cv2.__version__
&lt;span class=&quot;s1&quot;&gt;'3.0.0'&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If you got a result like above, then OpenCV 3.0 was successfully installed on your machine. If something goes wrong, try to do it all over again, this time make sure that you don’t miss any line above. If you still can’t make it work, please let me know by dropping me a line below. I’ll be glad to help.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Unlike the previous post, I will skip the installation of Keras this time, and focus on installing Caffe instead. If you want to install Keras, please give the link above a look.&lt;/p&gt;

&lt;p&gt;So now we have OpenCV 3.0 successfully installed. Next we will continue with Caffe. I’m assuming that you have at least one GPU installed. If you don’t, please refer to the post below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://mahaveer0suthar.github.io/project/Installing-Caffe-CPU-Only/&quot; target=&quot;_blank&quot;&gt;Installing Caffe on Ubuntu (CPU-ONLY)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This time we want to make use the power of GPU, we can tell Caffe that we want to use GPU, by commenting out the &lt;em&gt;CPU_ONLY&lt;/em&gt; option, do you remember that? Unfortunately, it’s not that simple. Caffe is just a framework which helps us handle the Network, which means that with Caffe, we can define the Network structure, we can define rules, then Caffe will train and evaluate our Model. In fact, Caffe makes use of CUDA, a superb library provided by NVIDIA, to handle the communication with our GPU.&lt;/p&gt;

&lt;p&gt;So, in the next step, we will install the CUDA Toolkit. Let’s go to the &lt;a href=&quot;https://developer.nvidia.com/cuda-downloads&quot; target=&quot;_blank&quot;&gt;CUDA Toolkit download page&lt;/a&gt;, choose your OS, the OS Distribution and version carefully. The rest is simple, just follow the guide on the download page, and it’s done. The installation file’s size is pretty large, so it’s likely to take a while, so don’t lose your patience, lol.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/installing-caffe-ubuntu/cuda.png&quot; alt=&quot;cuda&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next, we will download cuDNN, which is a GPU-accelerated library of primitives for deep neural networks provided by NVIDIA. With cuDNN, the computation speed will be significantly accelerated. All we have to do is going to &lt;a href=&quot;https://developer.nvidia.com/cudnn&quot; target=&quot;_blank&quot;&gt;cuDNN home page&lt;/a&gt;, register to the &lt;em&gt;Accelerated Computing Developer Program&lt;/em&gt; (it’s free, but it’s a must to download cuDNN). After registering and completing their short survey, you will be redirected to the download page like below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/installing-caffe-ubuntu/cudnn.png&quot; alt=&quot;cudnn&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You may want to download the latest version, as NVIDIA claimed that it’s much faster than its predecessor. If you don’t have any intention to play around with Faster R-CNN, then you can grab the latest version. But if you want to play around with the most outstanding Object Detection algorithm out there, then I highly recommend you to choose the v4 Library. I will tell you why in later post. Before downloading, make sure that you choose the right version for Linux, the upper most one below the install guide:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/installing-caffe-ubuntu/cudnn_down.png&quot; alt=&quot;cudnn_down&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After the download process completes, let’s extract the downloaded file (assuming that you’re placing it under Downloads folder):&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~/Downloads
&lt;span class=&quot;nb&quot;&gt;tar&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-xvf&lt;/span&gt; cudnn-7.0-linux-x64-v4.0-prod.tgz&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;In the next step, you just have to copy the two extracted folders to where CUDA was installed, which is most likely at &lt;em&gt;/usr/local/cuda&lt;/em&gt;:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;sudo cp &lt;/span&gt;lib64/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; /usr/local/cuda/lib64
&lt;span class=&quot;nb&quot;&gt;sudo cp &lt;/span&gt;include/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; /usr/local/cuda/include&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;That’s it. You have just successfully installed CUDA and cuDNN. Let’s move on and install Caffe. There won’t be much difference with the installation in CPU_ONLY mode.&lt;/p&gt;

&lt;p&gt;First, we have to install all the necessary packages and libraries:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libboost-all-dev libhdf5-serial-dev &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
libgflags-dev libgoogle-glog-dev liblmdb-dev protobuf-compiler
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--no-install-recommends&lt;/span&gt; libboost-all-dev&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Next, let’s go to BVLC GitHub repository and grab the latest version of Caffe:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;git clone https://github.com/BVLC/caffe
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;caffe&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Then we will create the &lt;em&gt;Makefile.config&lt;/em&gt; file from the example file, just like before:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cp &lt;/span&gt;Makefile.config.example Makefile.config&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Let’s apply those modifications below:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-vim&quot; data-lang=&quot;vim&quot;&gt;# cuDNN acceleration switch &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;uncomment &lt;span class=&quot;k&quot;&gt;to&lt;/span&gt; build with cuDNN&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;
USE_CUDNN &lt;span class=&quot;p&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;

# Uncomment &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; you'&lt;span class=&quot;k&quot;&gt;re&lt;/span&gt; using OpenCV &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;
OPENCV_VERSION &lt;span class=&quot;p&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;

# We need &lt;span class=&quot;k&quot;&gt;to&lt;/span&gt; be able &lt;span class=&quot;k&quot;&gt;to&lt;/span&gt; find Python&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;h&lt;/span&gt; and numpy/arrayobject&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
PYTHON_INCLUDE &lt;span class=&quot;p&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;sr&quot;&gt;/usr/&lt;/span&gt;include/python2&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt; \
        &lt;span class=&quot;sr&quot;&gt;/usr/&lt;/span&gt;local&lt;span class=&quot;sr&quot;&gt;/lib/&lt;/span&gt;python2&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/dist-packages/&lt;/span&gt;numpy&lt;span class=&quot;sr&quot;&gt;/core/&lt;/span&gt;include

# Uncomment &lt;span class=&quot;k&quot;&gt;to&lt;/span&gt; support layers written &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; Python &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;will link against    Python libs&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
WITH_PYTHON_LAYER &lt;span class=&quot;p&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;

# Whatever &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; you find you need goes here&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
INCLUDE_DIRS &lt;span class=&quot;p&quot;&gt;:=&lt;/span&gt; $&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;PYTHON_INCLUDE&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;sr&quot;&gt;/usr/&lt;/span&gt;local&lt;span class=&quot;sr&quot;&gt;/include /&lt;/span&gt;usr&lt;span class=&quot;sr&quot;&gt;/include/&lt;/span&gt;hdf5/serial
LIBRARY_DIRS &lt;span class=&quot;p&quot;&gt;:=&lt;/span&gt; $&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;PYTHON_LIB&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;sr&quot;&gt;/usr/&lt;/span&gt;local&lt;span class=&quot;sr&quot;&gt;/lib /&lt;/span&gt;usr&lt;span class=&quot;sr&quot;&gt;/lib /&lt;/span&gt;usr&lt;span class=&quot;sr&quot;&gt;/lib/&lt;/span&gt;x86_64&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt;linux&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt;gnu&lt;span class=&quot;sr&quot;&gt;/hdf5/&lt;/span&gt;serial/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If you went through my post about installing Caffe in CPU_ONLY mode before, then all the modifications above should sound familiar with you. If you didn’t, you might want to take a look at that post to know why we have to make those changes. The only difference with what we did in the previous post is, instead of uncommenting the &lt;em&gt;CPU_ONLY := 1&lt;/em&gt; line, we uncomment the &lt;em&gt;USE_CUDNN := 1&lt;/em&gt; to take advantage of cuDNN.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;At this point, we can go through the compilation of Caffe without any error:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;make all &amp;amp; make &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; make runtest &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; make pycaffe&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Next, in order to use Caffe inside our Python code, we have to add pycaffe to the PYTHONPATH:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;vim ~/.bashrc

&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PYTHONPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/Downloads/caffe/python:&lt;span class=&quot;nv&quot;&gt;$PYTHONPATH&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Then execute the command below to make the change take effect:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ~/.bashrc&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now let’s check if we have things work properly:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;python
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; import caffe
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If it don’t show any import error, then Congratulations, you have just successfully installed Caffe. The installation itself was confusing a little, but it didn’t require any complicated modifications, so somehow we still made it till the end. We can finally exhale now, lol.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;comparing-the-performance-between-cpu-and-gpu&quot;&gt;Comparing the performance between CPU and GPU&lt;/h3&gt;

&lt;p&gt;So we have Caffe compiled, and with the support from CUDA &amp;amp; cuDNN, we can take avantage of our GPU to speed up the learning process significantly. But, that’s just what we have been told so far. When we speak about the performance term, the words “good”, “faster”, “much faster” or even “significantly faster” are way too subtle and not much informative. In order to answer the question “How faster?”, it’s better to consider the difference in computing time between CPU Mode and GPU Mode. I will use two datasets which Caffe provided the trained models: MNIST and CIFAR-10 for comparing purpose. Note that in this post, I just consider the size of the dataset for simplicity, without considering the complexity of the Networks. I will dig more further about it on future posts on Neural Network.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MNIST Dataset&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;First, make sure you are in the root folder of Caffe, and run the commands below to download the MNIST dataset:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$CAFFE_ROOT&lt;/span&gt;
./data/mnist/get_mnist.sh
./examples/mnist/create_mnist.sh&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;That’s all we have to do to prepare the data. Let’s see how much time the CPU need to run each iteration:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;./build/tools/caffe &lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;examples/mnist/lenet_train_test.prototxt&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And here’s my result on my Intel Core i7-6700K CPU:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/installing-caffe-ubuntu/mnist_cpu.png&quot; alt=&quot;mnist_cpu&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, my CPU took approximately 55ms to run each iteration, in which 23ms for forward pass and 32ms for backward pass. Let’s go ahead and see if the GPU can do better:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;./build/tools/caffe &lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;examples/mnist/lenet_train_test.prototxt&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And here’s the result on my GTX 1070.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/installing-caffe-ubuntu/mnist_gpu.png&quot; alt=&quot;mnist_gpu&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The result came out nearly right after I hit Enter. I was really impressed, I admit. Each iteration took only 1.7ms to complete, in which 0.5ms for forward pass and 1.2ms for backpropagation. Let’s do some calculation here: the computing time when using GPU is roughly 32 times faster than when using CPU. Hmm, not so bad, you may think.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Because MNIST dataset is pretty small in size, which each example is just a 28x28 grayscale image, and it contains only 70000 images in total, the CPU still can give us an acceptable performance. Also note that in order to make use of the power of GPU, our computer has to take some times to transfer data to the GPU, so with small dataset and simple Network, the difference between the two may not be easily seen.&lt;/p&gt;

&lt;p&gt;Let’s go ahead and give them a more challenging one.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CIFAR-10 Dataset&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CIFAR-10 is way larger comparing to MNIST. It contains 60000 32x32 color images, which means CIFAR-10 is roughly three times larger than MNIST. That’s a real challenge for both to overcome, right?&lt;/p&gt;

&lt;p&gt;Just like what we did with MNIST dataset, let’s first see how much time it takes using CPU:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;./build/tools/caffe &lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;examples/cifar10/cifar10_full_train_test.prototxt&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And here’s the result I got:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/installing-caffe-ubuntu/cifar10_cpu.png&quot; alt=&quot;cifar_cpu&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, with a larger dataset (and a more complicated Network, of course), the computing speed was much slower comparing with MNIST dataset. It took approximately 526ms to complete one iteration: 238ms for forward pass and 288ms for backward pass. Let’s go ahead and see how well the big guy can do:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;./build/tools/caffe &lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;examples/cifar10/cifar10_full_train_test.prototxt &lt;span class=&quot;nt&quot;&gt;--gpu&lt;/span&gt; 0&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And the result I had with my GTX 1070:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/installing-caffe-ubuntu/cifar10_gpu.png&quot; alt=&quot;cifar_gpu&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Look at the result above. Unlike the significant decrease in performance as we saw when running on CPU, my GTX 1070 still brought me an impressive computing speed. It took only 11ms on one iteration, in which 3ms for forward pass and 8ms for backpropagation. So when running on CIFAR-10 dataset, the GPU really did outperform the CPU, which computed 48 times faster. Imagine you are working with some real large dataset in real life such as ImageNet, using GPU would save you a great deal of time (let’s say days or even weeks) on training. The faster you obtain the result, the more you can spend on improving the Model. That’s also the reason why Neural Network, especially Deep Neural Network, has become the biggest trend in Machine Learning after long time being ignored by the lack of computing power. Obviously not only nowadays, but Deep Neural Network will continue to grow in the future.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;So in this post, I have just shown you how to install OpenCV and Caffe in GPU Mode with CUDA Toolkit and cuDNN. I really appreciate that you made it to the end with patience. I hope that this post can help you prepare the necessary environment for your Deep Learning projects.&lt;/p&gt;

&lt;p&gt;And I also did some comparison on performance between GPU and CPU using two most common datasets: MNIST and CIFAR-10. Through the results above, I think you can now see how using GPU on Deep Neural Network can bring up a big difference.&lt;/p&gt;

&lt;p&gt;Finally, if you come across any compilation error, please kindly let me know. I’ll try my best to help. Can’t wait to see you soon, in the upcoming post.&lt;/p&gt;</content><author><name>Mahaveer Suthar</name><email>mahaveer0suthar@gmail.com</email></author><category term="machine-learning" /><category term="deep-learning" /><category term="caffe" /><category term="installation" /><category term="gpu" /><category term="cuda" /><category term="cudnn" /><category term="environment" /><category term="essential" /><summary type="html">It’s great to be with all you guys again in today’s post. As you already knew, it’s been a while since I built my own desktop for Deep Learning. Or for ones who missed that post, you can have a look at my build here:</summary></entry><entry><title type="html">Machine Learning Part 9: Regularization</title><link href="http://localhost:4000/mahaveer0suthar.github.io/tutorial/Regularization/" rel="alternate" type="text/html" title="Machine Learning Part 9: Regularization" /><published>2017-10-31T00:00:00+05:30</published><updated>2017-10-31T00:00:00+05:30</updated><id>http://localhost:4000/mahaveer0suthar.github.io/tutorial/Regularization</id><content type="html" xml:base="http://localhost:4000/mahaveer0suthar.github.io/tutorial/Regularization/">&lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;p&gt;Welcome back to my 9th tutorial on Machine Learning! I have been kept busy in last weekends, struggling in getting my desktop ready for Deep Learning. You may consider giving that post a look if you are planning to build your own “Monster” too: &lt;a href=&quot;https://mahaveer0suthar.github.io/project/Building-Desktop-For-Deep-Learning/&quot; target=&quot;_blank&quot;&gt;Building up my own machine for Deep Learning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Of course I got a lot to tell you about things I’ve been doing with my new desktop. But it will be a little bit selfish of mine and unfair to all of you if I let dust cover my Machine Learning tutorial series. So let’s say, I’m back on track. And today, I will talk about &lt;strong&gt;Regularization&lt;/strong&gt;, a technique to deal with Overfitting problem. You may still remember that I mentioned eariler in previous posts, Overfitting is a big headache in all Machine Learning problems. Beside Cross Validation that I told you before, &lt;strong&gt;Regularization&lt;/strong&gt; is a must-know technique that you will nearly apply in all of your Machine Learning problems. Furthermore, applying &lt;strong&gt;Regularization&lt;/strong&gt; is a default setting in all algorithms provided by &lt;em&gt;scikit-learn&lt;/em&gt; library.&lt;/p&gt;

&lt;p&gt;And I’m not going to waste any minute of yours. Let’s go straight into the most likely asked question: What is Regularization?&lt;/p&gt;

&lt;p&gt;When we hear the word &lt;strong&gt;Regularization&lt;/strong&gt; without anything else related to Machine Learning, we all understand that Regularization is the process of regularizing something, or the process in which something is regularized. The problem is: what is exactly &lt;em&gt;something&lt;/em&gt;. In terms of Machine Learning, we talk about learning algorithms or models, and what is actually inside the algorithms or models? That’s the set of parameters. In short, &lt;strong&gt;Regularization&lt;/strong&gt; in Machine Learning is the process of regularizing the parameters.&lt;/p&gt;

&lt;p&gt;After knowing that Regularization is actually to regularize our parameters, then you may wonder: Why regularizing the parameters help prevent Overfitting? Let’s consider the graph that I had prepared for this tutorial. A picture is worth a thousand words, right?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/tutorials/regularization/reg_graph.png&quot; alt=&quot;reg_graph&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see in the graph I have just shown you, we got two functions represented by a green curve and a blue curve respectively. Both curve fit those red points so well that we can consider they both incur zero loss. And if you followed all my previous tutorials, you would be able to point out that the green curve is likely to overfit the data. Yeah, you are totally right. But have you ever wondered why the green curve (or any curve which is similar to it) is overfitting the data?&lt;/p&gt;

&lt;p&gt;To understand that in a more mathematical way, let’s consider the two functions that I used to draw the graph above:&lt;/p&gt;

&lt;p&gt;The green curve:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_1(x)=-x^4+7x^3-5x^2-31x+30&lt;/script&gt;

&lt;p&gt;The blue curve:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_2(x)=\frac{x^4}{5}-\frac{7x^3}{5}+x^2+\frac{31x}{5}-6&lt;/script&gt;

&lt;p&gt;Does it sound similar to you? I once told you about one way to improve the performance of Linear Regression model, that is adding polynomial features. You can refer it here: &lt;a href=&quot;https://mahaveer0suthar.github.io/tutorial/Underfit-Overfit/&quot; target=&quot;_blank&quot;&gt;Underfitting and Overfitting Problems&lt;/a&gt;. You knew that by adding more features, we will have a more well learned model which can fit our data far better. But everything has its drawback, if we add so many features, we will be purnished with Overfitting. That was what I told you in the earlier tutorial. Not so hard to recall, right?&lt;/p&gt;

&lt;p&gt;If you look at each function’s equation, you will find that the green curve has larger coefficients, and that’s the main caution of Overfitting. As I mentioned before, Overfitting can be interpreted that our model fits the dataset so well, which it seems to memorize the data we showed rather than actually learn from it. Intuitively, having large coefficients can be seen as an evidence of memorizing the data. For example, we got some noises in our training dataset, where the data’s magnitude is far difference than the others, those noises will cause our model to put more weight into the coefficient of higher degree, and what we received is a model that overfits our training data!&lt;/p&gt;

&lt;p&gt;Some of you may think, if adding so many features causes Overfitting, than why don’t we just stop adding features when we got an acceptable model? But think about that this way. If your customer or your boss wants a learned model with \(95%\) accuracy, but you can’t achieve that result without adding some more features, which results in overfitting the data. What will you do in the next step?&lt;/p&gt;

&lt;p&gt;Or think about it in one more other way. You are facing a problem where you are provided with a large dataset, which each of them contains a great deal of features. You don’t know which features to drop, and even worse if it turns out that every feature is fairly informative, which means that dropping some features will likely ruin the algorithm’s performance. What do you plan to do next?&lt;/p&gt;

&lt;h3 id=&quot;the-regularization-term&quot;&gt;The regularization term&lt;/h3&gt;

&lt;p&gt;Therefore, it’s not always a good idea to drop some features just to prevent Overfitting. And as you saw in the example above, it requires further analysis to know whether you can remove some less informative features. So, it’s a good practice that you use all features to build your first model in the beginning. And &lt;strong&gt;regularization&lt;/strong&gt; comes out as a solution. To make it more clear for you, let’s consider our MSE cost function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J = \frac{1}{2m}\sum_{i=1}^m(h_\theta(X^{(i)})-y^{(i)})^2&lt;/script&gt;

&lt;p&gt;I once introduced the MSE cost function before in &lt;a href=&quot;https://mahaveer0suthar.github.io/tutorial/Linear-Regression/&quot; target=&quot;_blank&quot;&gt;Logistic Regression&lt;/a&gt; tutorial. And as you know, the objective of learning is to minimize that MSE function. It means that our parameters can be updated in anyway, just to lower the MSE value. And as I told you above, the larger our parameters become, the higher chance our Model overfits the data. So the question is: can we not only minimize the cost function, but also restrict the parameters not to become too large? The answer is: we &lt;strong&gt;CAN&lt;/strong&gt;, by adding the regularization term like below:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J = \frac{1}{2m}\sum_{i=1}^m(h_\theta(X^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2&lt;/script&gt;

&lt;p&gt;, where \(\lambda\) is a constant to control the value of regularization term, \(n\) is the number of the features.&lt;/p&gt;

&lt;p&gt;Take a look at our new cost function after adding the regularization term. What the regularization term does is to penalize large parameters. Obviously, minimize the cost function consists of minimizing both terms in the right: the MSE term and the regularization term. So each time some parameter is updated to become significantly large, it will increase the value of the cost function by the regularization term, and as a result, it will be penalized and updated to a small value.&lt;/p&gt;

&lt;p&gt;Also note that we only compute the regularization term with the weights only, DON’T include the bias in the regularization term! You may ask why? Well, we can re-write our activation function like below (in case of polynomial function):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_\theta(X)=\theta_0X^0+\theta_1X+\theta_2X^2+\dots+\theta_nX^n&lt;/script&gt;

&lt;p&gt;As you can see, we can think that the bias term goes with the \(X^0\) term, which means that it doesn’t affect to the form of our function, so include the bias term into the regularization term doesn’t make any sense.&lt;/p&gt;

&lt;p&gt;With the new added regularization term, obviously we have to make some change to the way we update the parameters too. But it’s not a big deal at all, just take all the partial derivatives and we will achieve the result below very easily:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For weights (\( \theta_1, \ldots, \theta_n\))&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial}{\partial \theta_j}J(\theta) = \frac{1}{m}\sum_{i=1}^m(h_\theta^{(i)}(x^{(i)})-y^{(i)}).x_j^{(i)}+\frac{\lambda}{m}\theta_j&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;For bias (\( \theta_0 \)) it remains unchanged:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial}{\partial \theta_0}J(\theta) = \frac{1}{m}\sum_{i=1}^m(h_\theta^{(i)}(x^{(i)})-y^{(i)})&lt;/script&gt;

&lt;p&gt;Next, let’s put things together to see how the parameters are updated after adding regularization term:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{bmatrix}\theta_0\\\theta_1\\\vdots\\\theta_n\end{bmatrix}=\begin{bmatrix}\theta_0\\\theta_1(1-\alpha\frac{\lambda}{m})\\\vdots\\\theta_n(1-\alpha\frac{\lambda}{m})\end{bmatrix}-\frac{\alpha}{m}\begin{bmatrix}\sum_{i=1}^m(h_\theta^{(i)}(x^{(i)})-y^{(i)})\\\sum_{i=1}^m(h_\theta^{(i)}(x^{(i)})-y^{(i)}).x_1^{(i)}\\\vdots\\\sum_{i=1}^m(h_\theta^{(i)}(x^{(i)})-y^{(i)}).x_n^{(i)}\end{bmatrix}&lt;/script&gt;

&lt;p&gt;That’s it. All we have to change is just adding the factor \((1-\frac{\lambda}{m})\) to the parameter when updating. You can prove the result above yourselves as an assignment. It’s very easy, but I recommend you do it yourselves to become more familiar with those mathematical terms like the Chain Rule or partial derivatives, which you will use a lot in the next tutorials.&lt;/p&gt;

&lt;h3 id=&quot;more-about-regularization&quot;&gt;More about regularization&lt;/h3&gt;

&lt;p&gt;Above I have shown you about adding the regularization term in our MSE function, and how to apply regularization in updating parameters, too. But it doesn’t mean that when applying regularization, you always stick to the term I have shown you. In fact, it’s just one among many forms of regularization. It’s just like the way we have many options for the cost function (or you can call it the loss function), we have MSE function, we have log-likelihood cost function, etc. To a particular problem, there will always be more than one approach, and each one will likely work best in a particular set of conditions.&lt;/p&gt;

&lt;p&gt;Below I will introduce to you the two which are mostly used in real world projects:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;L2 Regularization:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J = \frac{1}{2m}\sum_{i=1}^m(h_\theta(X^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\sum_{j=1}^n(\theta_j^2)&lt;/script&gt;

&lt;p&gt;This is actually the one I have shown above. It’s also easy to remember: L2 means degree \(2\) regularization term. Before talking any further, let’s consider the other one first:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;L1 Regularization:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J = \frac{1}{2m}\sum_{i=1}^m(h_\theta(X^{(i)})-y^{(i)})^2+\frac{\lambda}{m}\sum_{j=1}^n|\theta_j|&lt;/script&gt;

&lt;p&gt;Another form of regularization, called the L1 Regularization, looks like above. As you can see, instead of computing mean value of squares of the parameters as L2 Regularization does, what L1 Regularization does is to compute the mean magnitude of the parameters. Also note that in L1 Regularization term, we multiply the sum with a fraction of \(\frac{\lambda}{m}\), not \(\frac{\lambda}{2m}\). Remember that the term \(\frac{\lambda}{2m}\) helps vanish the \(2\) factor in the derivative of polynomial of degree 2. So I hope you won’t be confused which one to use.&lt;/p&gt;

&lt;p&gt;Looking at two cost functions above, adding L1 Regularization term or L2 Regularization term have nearly the same effect, that is to penalize large parameters. As a result, we end up with a learned model with all parameters being kept small, so that our model won’t depend on some particular parameters, thus less likely to overfit.&lt;/p&gt;

&lt;p&gt;To understand how the two terms differ from each other, let’s see how they affect the parameter update process:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;L2 Regularization:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_j = \theta_j-\frac{\alpha}{m}\sum_{i=1}^m(h_\theta(X^{(i)})-y^{(i)}).X_j^{(i)}-\alpha\frac{\lambda}{m}\theta_j=(1-\alpha\frac{\lambda}{m})\theta_j-\frac{\alpha}{m}\sum_{i=1}^m(h_\theta(X^{(i)})-y^{(i)}).X_j^{(i)}&lt;/script&gt;

&lt;p&gt;To make it simple, I just computed for one parameter \(\theta_j\). As you can see, when applying regularization, we have the new \((1-\alpha\frac{\lambda}{m})\) factor. What does it affect the update for \(\theta_j\) anyway? After each learning iteration, \(\theta_j\) decreases by an amount of \(\alpha\frac{\lambda}{m}\), which means that L2 Regularization tends to shink it by an amount proportional to \(\theta_j\). The larger \(\theta_j\) becomes, the more it will shrink. Now, what it’s like in the case of L1 Regularization:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;L1 Regularization:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_j = \theta_j-\alpha\frac{\lambda}{m}sign(\theta_j)-\frac{\alpha}{m}\sum_{i=1}^m(h_\theta(X^{(i)})-y^{(i)}).X_j^{(i)}&lt;/script&gt;

&lt;p&gt;When we use L1 Regularization, our parameters shrink in a different way. Because \(sign(\theta_j)\) can only be either \(-1\) or \(1\), \(\theta_j\) now shrinks by a constant amount, and it tends to move toward zero. This makes the update process different from what we saw in L2 Regularization. Therefore, we can easily see that L1 Regularization tends to penalize small parameters more than L2 Regularization does. In short:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If \(\theta_j\) is large, L2 Regularization shinks \(\theta_j\) more than L1 Regularization does.&lt;/li&gt;
  &lt;li&gt;If \(\theta_j\) is small, L1 Regularization shinks \(\theta_j\) more than L2 Regularization does.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;last-but-not-least&quot;&gt;Last, but not least…&lt;/h3&gt;

&lt;p&gt;Above I have just shown you two mostly applied forms of regularization, and how each of them affect the learning process. The last thing I want to tell you in this post is about the constant \(\lambda\). As I mentioned earlier, \(\lambda\) controls how much we want to regularize our parameters. But what is really behind this? Let’s consider the two scenarios below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(\lambda\) is very small (nearly zero):&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When \(\lambda\) is nearly zero, then the regularization term will become nearly zero. As a result, the cost function mostly depends on the MSE term just like before applying regularization. Or we can say that, when \(\lambda\) is nearly zero, the regularization term won’t have any significant effect on shrinking the parameters. Therefore the model is more likely to overfit.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(\lambda\) is very large:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Conversely, let’s consider the case where \(\lambda\) becomes extremely large. This time we put much weight in the regularization term. And as a result, the parameters will shrink to a very small values. That approach, however, brings a real problem, that is, rather than preventing Overfitting, our parameters now become so small so that it can’t even fit the training data well, or we can say that: applying so much regularization cause our model to underfit the dataset.&lt;/p&gt;

&lt;p&gt;Through the two scenarios above, you can see that choosing the right value for \(\lambda\) is not an easy task. But with the right choice of \(\lambda\), the regularization term can have a significant effect on the learning process, which results in a model which both fits the dataset very well, but not likely to overfit.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;In today’s post, we have talked about &lt;strong&gt;Regularization&lt;/strong&gt;, an important technique applied in every Machine Learning model in the real world to deal with the Overfitting problem. I hope after this tutorial, you can have a deeper understanding about what actually causes Overfitting, and the right way to deal with that headache. In the next post, I will continue with one last supervised learning algorithm, the one that I should have showed you in this post instead. But I soon realized that with the lack of knowledge about Regularization, it will be pretty hard to fully understand that algorithm. I hope you not blame me for this. So stay updated and I will be right back!&lt;/p&gt;</content><author><name>Mahaveer Suthar</name><email>mahaveer0suthar@gmail.com</email></author><category term="machine-learning" /><category term="overfitting" /><category term="regression" /><category term="classification" /><category term="regularization" /><category term="essential" /><summary type="html">Welcome back to my 9th tutorial on Machine Learning! I have been kept busy in last weekends, struggling in getting my desktop ready for Deep Learning. You may consider giving that post a look if you are planning to build your own “Monster” too: Building up my own machine for Deep Learning.</summary></entry><entry><title type="html">Building up my own machine for Deep Learning</title><link href="http://localhost:4000/mahaveer0suthar.github.io/project/Building-Desktop-For-Deep-Learning/" rel="alternate" type="text/html" title="Building up my own machine for Deep Learning" /><published>2017-10-29T00:00:00+05:30</published><updated>2017-10-29T00:00:00+05:30</updated><id>http://localhost:4000/mahaveer0suthar.github.io/project/Building-Desktop-For-Deep-Learning</id><content type="html" xml:base="http://localhost:4000/mahaveer0suthar.github.io/project/Building-Desktop-For-Deep-Learning/">&lt;p&gt;Hello guys, it’s me again being here with all you guys today. There will be no tutorial in this post, so at first I’m sorry for that. But Halloween is coming to town, right? The weather is good out there, too. So there’s no reason to stay at home spending time on some boring Machine Learning stuff, right? But I did! I spent a whole day on building my own desktop for Deep Learning!&lt;/p&gt;

&lt;p&gt;Of course I still remember telling you about using &lt;em&gt;g2.2xlarge&lt;/em&gt; instance of Amazon Web Service. For ones who are considering using some GPU instance with a reasonable price, please have a look at my previous post:&lt;br /&gt;
&lt;a href=&quot;https://mahaveer0suthar.github.io/project/Prepare-AWS-Instance/&quot; target=&quot;_blank&quot;&gt;Using AWS EC2 Instance&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The main reason why I decided to build my own machine is just simple: I needed a more powerful GPU. Of course I can change to &lt;em&gt;g2.8xlarge&lt;/em&gt; instance but I will soon get broke. Having a desktop at home also makes sense for one who mainly works with images and videos like me, since I can visualize the output without writing more codes to use the raw results from EC2 instance.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The third reason is: because it’s kind of cool, lol. To be honest, I have never done it before, so I was really eager to try.&lt;/p&gt;

&lt;p&gt;I was thinking about that for a while. I was surfing on Googles, seeing some cool guys doing some reviews on performance of different GPUs. And below is the build I chose for my own, eventually:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/building-desktop-for-deep-learning/all.JPG&quot; alt=&quot;all&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;CPU: Intel Core i7-6700K 4.0GHz Quad-Core Processor&lt;/li&gt;
  &lt;li&gt;CPU Cooler: Scythe Kotetsu 79.0 CFM&lt;/li&gt;
  &lt;li&gt;Motherboard: ASRock Z170 Extreme4 ATX LGA1151&lt;/li&gt;
  &lt;li&gt;GPU: Gigabyte GeForce GTX 1070 8GB G1&lt;/li&gt;
  &lt;li&gt;RAM: Corsair Vengeance LPX 16GB (2 x 8GB) DDR4-2666&lt;/li&gt;
  &lt;li&gt;HDD: Seagate Barracuda 2TB 3.5” 7200RPM&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The most important part is the GTX 1070 GPU. At first I intended to build with GTX 970, but I soon realized that the performance of GTX 970 is just not much different from GTX 960, so it may not a good choice especially if you mostly work with Convolutional Neural Network. Guys on Deep Learning community recommended to use at least GTX 980 for CNN. And because GTX 1070 is just slightly more expensive than GTX 980, whereas GTX 1080 is still a big deal, I thought GTX 1070 was the best choice for me.&lt;/p&gt;

&lt;p&gt;Working with Deep Neural Network requires mostly the power of GPU, so we don’t necessarily buy a giant-killer CPU. But a little guy won’t be a good fit (comparing to the giant GTX 1070), so I chose Intel Core i7-6700K. I also needed a good CPU cooler (not a very big one), too.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Since GTX 1070 will be with my team, not only the CPU, but I also needed a motherboard which can hold them well, and has an efficient power consumption, too. My choice was Z170 Extreme4, a motherboard mainly used for gaming PC.&lt;/p&gt;

&lt;p&gt;The next to consider is storage and RAM. CNN requires a lot of memory to hold the network temporary values (all the parameters and gradients from backpropagation), and a great deal of memory to save the Model, so now I started with 16GB of RAM, and 2TB internal hard disk.&lt;/p&gt;

&lt;p&gt;The last one is the PSU (power supply unit). I chose a 600W PSU from a Japanese brand, and they don’t sell them outside Japan (as I searched on the net and found no result), so I don’t list out the PSU I bought here. You can buy from your local brand, but I recommend that you choose a PSU which can provide from at least 600W, because a big GPU will likely consume a lot of power.&lt;/p&gt;

&lt;p&gt;So that’s was all about the parts I need. It’s time to put things together (actually it was finished at the time of writing).&lt;/p&gt;

&lt;p&gt;Here is the Z170 Extreme4, fantastic design, isn’t it?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/building-desktop-for-deep-learning/z170.JPG&quot; alt=&quot;z170&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And if there’s something I forgot to mention, that is the big one below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/building-desktop-for-deep-learning/case.jpg&quot; alt=&quot;case&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Since the GTX 1070 is 28cm long, obviously I needed a big case in order to put all these things in. I chose the NZXT S340 Mid Tower case. The price wasn’t good at all, but in the end it turned out that it’s worth every &lt;em&gt;yen&lt;/em&gt;.&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Next, let’s first put the motherboard into the case. I heard some guys recommend putting everything on the motherboard first, then put all into the case. I think both ways work well. But I think it’s easier if in the beginning our motherboard is mounted stably somewhere, and there’s no better place than its final shelter, right?&lt;/p&gt;

&lt;p&gt;Here’s the picture of my Z170 put into the S340 case:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/building-desktop-for-deep-learning/z170_case.JPG&quot; alt=&quot;z170_case&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next, let’s take a look at the tiny CPU. Despite of the power it has, it’s literally tiny comparing to other parts.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/building-desktop-for-deep-learning/cpu.JPG&quot; alt=&quot;cpu&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But it was not until I pull it out that I realized it was heavier than it looks. Maybe that was because the metal which helps it transmit temperature adds more extra weight. Next, let’s mount it onto the motherboard.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/building-desktop-for-deep-learning/cpu_z170.JPG&quot; alt=&quot;cpu_z170&quot; /&gt;&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;When I tried to close the cover afterward, it was a lot heavier than it was supposed to be. And I kept wondering for a while, whether I should add more strength or something. After watching some guys on Youtube mounting the core i7-6700k, and mentioning about how hard it was to close the cover, I decided to add more force, too. It turns out that the shape of the core i7-6700K, LGA 1151 caused that problem.&lt;/p&gt;

&lt;p&gt;Next, I put the cooler on the top of the CPU. It was a little bit tall, that I was afraid if it fit my case. Fortunately, it does.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/building-desktop-for-deep-learning/cooler.JPG&quot; alt=&quot;cooler&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And here comes the boss. I had to say, it was so amazing. Fantastic, tremendous design that I couldn’t have been able to imagine a GPU could be!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/building-desktop-for-deep-learning/gpu.JPG&quot; alt=&quot;gpu&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And here it is after put onto the motherboard. It takes a great deal of space and makes everything look small.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/building-desktop-for-deep-learning/gpu_case.JPG&quot; alt=&quot;gpu_case&quot; /&gt;&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;4068904466&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The rest was so simple: putting the RAMs, the PSU, and carefully plug them into the right places. And my “Monster” is ready to be unleashed:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/building-desktop-for-deep-learning/done.jpg&quot; alt=&quot;done&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/building-desktop-for-deep-learning/done_2.jpg&quot; alt=&quot;done_2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/projects/building-desktop-for-deep-learning/done_3.JPG&quot; alt=&quot;done_3&quot; /&gt;&lt;/p&gt;

&lt;script async=&quot;&quot; src=&quot;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;

&lt;!-- MidPageAds2 --&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-client=&quot;ca-pub-3852793730107162&quot; data-ad-slot=&quot;2275566366&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The moment I stood up to plug the PSU’s cord, I realized that I was sitting for nearly 7 hours! My Saturday was just an extended Friday! But at least, it worked in the end. And I can draw a smile in my face now. I also realized that I just ate some noodles for breakfast, and I started to feel hungry now. Few more hours left for Saturday, and I’ve been thinking about curry for a while. So, I’ll go get some beef right now. Thank you all for watching. If you need some experience on building your own machine for Deep Learning, feel free to contact me. I’ll be glad to help! Goodbye and see you in the next tutorial!&lt;/p&gt;</content><author><name>Mahaveer Suthar</name><email>mahaveer0suthar@gmail.com</email></author><category term="machine learning" /><category term="deep learning" /><category term="PC" /><category term="desktop" /><category term="GPU" /><category term="GTX 1070" /><summary type="html">Hello guys, it’s me again being here with all you guys today. There will be no tutorial in this post, so at first I’m sorry for that. But Halloween is coming to town, right? The weather is good out there, too. So there’s no reason to stay at home spending time on some boring Machine Learning stuff, right? But I did! I spent a whole day on building my own desktop for Deep Learning!</summary></entry></feed>