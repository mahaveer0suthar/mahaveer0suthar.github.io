var tipuesearch = {"pages": [{
    "title": "What I talk when I talk about Tensorflow",
    "text": "Some of my collegues, as well as many of my readers told me that they had problems using Tensorflow for their projects. Something like this: Hey Mahaveer, what is the difference between tf.contrib.layers and tf.layers? Oh, and what is the TF-Slim thing? And now we have the godd*** tf.estimator. What are all these for? What are we supposed to use? To be honest, when I started using Tensorflow, I was in that situation too. Tensorflow was already pretty bulky back then, and to make things even worse, it just kept getting bigger and bigger. If you don’t believe me, just look at the size of the current installation package and compare it to previous versions. But wait! I just got an idea. Why not create a series of blog posts about Tensorflow ;) Objectives Let’s talk about what we’re gonna focus on in this post. I learned this thing the hardest way, guys. I think I will make a post about what I learned from writing technical blog posts, and one of them is: do talk about the objectives first! So, here’s what we will do in this post: Address some confusing problems of Tensorflow Understand the mostly used Tensorflow modules (Optional) Get out hands dirty with some easy code! Okay, let’s tackle them one by one! Common Problems Before diving in the details, I think I should list out the most common problems that we might face when using Tensorflow: Don’t know how to start Whether it comes to importing data, creating the model or visualizing the results, we usually get confused. Technically, there are so many ways to do the exact same thing in Tensorflow. And it’s the urge of doing things in the most proper way that drives us crazy. Don’t know what to do when things go wrong I think this is the problem that a lot of you guys can relate to. In Tensorflow, we must first define the computation graph. Not only doing this way prevents us from modifying the graph when it’s running (sometimes we just want it to be dynamic), but it also does a good job at hiding things from us, which we can’t know what the hell under the hood is causing the trouble. We are the Python guys, we want things to be Pythonic! Tensorflow’s Vocabulary As I said above, one problem with Tensorflow is that there are a lot of ways to do the exact same thing. Even experienced users find it confusing sometimes. Below, I’m gonna list out some “concepts” that are mostly confusing to beginners: Low-level API It used to be how we did everything in Tensorflow when it first came out. Want to create a fully connected layer? Create some weights, some biases and roll them in! with tf.variable_scopes('fc_layer'): weights = tf.get_variables('weights', [5, 5, 3, 64], initializer=tf.initializers.truncated_normal(stddev=5e-2)) biases = tf.get_variables('biases', [64], initializer=tf.zeros_initializer) output = tf.nn.conv2d(inputs, weights, [1, 1, 1, 1], padding='SAME') output = tf.nn.bias_add(conv, biases) output = tf.nn.relu(output) tf.contrib You are likely to come across tf.contrib.layers a lot. Basically, it’s backed by the large community of Tensorflow (contrib stands for contribution) and it contains experimental implementation, such as a newly introduced layer, a new optimization method, or just wrappers for low-level API, etc. Although they are technically just experimental codes, they actually work very well and will be merged to Tensorflow’s core code in the future. tf.layers As its name is self explained, this is the package for defining layers. We can think of it as an official version of tf.contrib.layers. They basically do the same job: to make defining layers less tiresome. Using tf.contrib.layers or tf.layers to create a conv2d layer like we did above, we now need only one line: output = tf.contrib.layers.conv2d(inputs, 64, [5, 5], weights_initializer=tf.initializers.truncated_normal(stddev=5e-2)) Or with tf.layers: output = tf.layers.conv2d(inputs, 64, [5, 5], padding='same', kernel_initializer=tf.initializers.truncated_normal(stddev=5e-2)) I bet you wouldn’t create any layers by hand from now on! tf.contrib.slim (or TF-Slim) Okay, this may be the most confusing one. At first, I thought that was the light-weight version of Tensorflow but soon enough, I realized I was wrong. slim only stands for fewer lines to do the same thing (comparing with low-level API). For example, to create not only one, but three conv2d layers, we only need to write one line: slim = tf.contrib.slim output = slim.repeat(inputs, 3, slim.conv2d, 64, [5, 5]) Other than that though, tf.slim can help you build an entire pipeline for training the network, which means they have some functions to help you get the losses, train or evaluate the model, etc. Overall, TF-Slim may be a good option for fast experimenting new idea (Tensorflow’s research uses TF-Slim for building the networks). What you need to take into account is, TF-Slim’s codes actually came from tf.contrib (e.g. slim.conv2d is just an alias for tf.contrib.layers.conv2d), so there’s no magic here. tf.keras This is legend! Keras came out when we had to write everything using low-level API. So technically it was used as the high-level API for Tensorflow. In my opinion, it did help make the community (especially researchers) adopt Tensorflow. And since Keras is officially a module of Tensorflow (from version 1.0 I think), you don’t have to worry about version compatibility any more. The great thing about Keras is, it does all the hard tasks for you. So going from idea to result is just a piece of cake. Want to create a network? Just stack up the layers! Want to train it? Just compile and call fit! model = Sequential() model.add(Dense(out_size, activation='relu', input_shape=(in_size,))) # Add more here model.add(...) # Compile model.compile(loss='categorical_crossentropy', optimizer=SGD()) # Train model.fit(x=inputs, y=labels) Although Keras is super convenient, especially for those who don’t like to write code, it abstracts so many things from us. François Chollet, the author of Keras, claimed that Keras will act like an interface only, but it does have some constraints which may confuse you sometimes (Want model.fit to compute validation loss after a specific number of batches? It can’t!). You may also have hard time implementing newly introduced deep-learning papers entirely by Keras since they require some minor tweaks within some layer. Eager Execution As I mentioned earlier, when implementing in Tensorflow, you must first define all the operations to form a graph. It’s not until the graph is finalized (it’s locked, no more in, no more out, no more update) that you can run it to see the results. Because of this, Tensorflow is hard to debug and incapable of creating dynamic graph. So Eager Execution came out to help deal with these problems. The name is kind of weird though. I interprete it as “can’t wait to execute”. With the additional 2 new lines, you can now do something like: evaluate the new created variable (which seems trivial but used to be impossible in Tensorflow): tf.enable_eager_execution() tf.executing_eagerly() import tensorflow.contrib.eager as tfe weights = tfe.Variable(tf.truncated_normal(shape=[2, 3], stddev=5e-2), name='weights') weights &lt;tf.Variable 'weights:0' shape=(2, 3) dtype=float32, numpy= array([[ 0.06691323, -0.01890625, -0.00283119], [-0.0536754 , 0.00109388, -0.04310168]], dtype=float32)&gt; Rumor has it Eager Execution is gonna be set to default from Tensorflow 2.0. I think this move will please a lot of Tensorflow fans out there. But please bear in mind that at the moment, not everything is gonna work in Eager Execution mode (yet). So while we’re waiting for Tensorflow 2.0 to be released, it’s a good idea to stay updated to the latest news from Tensorflow’s team and Google. (Optional) Let’s play with Tensors! Okay guys, this is an optional section. We’re gonna see if different approaches produce exactly the same results. We’re gonna create a “real” convolution2d layer, including activation functions and regularization terms, by using tf.contrib.layers and tf.layers. We will check the similarity among their results by checking the variables and operations that they created. Oh hold on! There’s one more thing I want you to pay attention to. I will write out all the arguments whether some of them have default values. The reason is, the two modules’ conv2d functions set the default values differently for the same terms! For example, padding is set to ‘SAME’ by default in tf.contrib.layers.conv2d, but ‘valid’ in case of tf.layers.conv2d. Now we’re ready to move on. tf.contrib.layers Let’s start with tf.contrib.layers (tf.contrib is technically big and capable of a lot of things, so by saying tf.contrib.layers here seems more appropriate). I don’t want to think of the amount of work to achieve the same result by using low-level API. That’s why having any kinds of high-level API will save us a ton of time and effort. Not only researchers, developers do love high-level APIs! # The inputs we use is one image of shape (224, 224, 3) inputs = tf.placeholder(tf.float32, [1, 224, 224, 3]) conv2d = tf.contrib.layers.conv2d(inputs=inputs, num_outputs=64, kernel_size=3, stride=1, padding='SAME', activation_fn=tf.nn.relu, weights_initializer=tf.initializers.truncated_normal(stddev=0.01), weights_regularizer=tf.contrib.layers.l2_regularizer(0.005), biases_initializer=tf.zeros_initializer()) tf.layers Next, let’s see how we can create a convolution2d layer with tf.layers, an official modules by the core team of Tensorflow ;) Obviously we expect that it can produce the same result, with less or (at least) similar effort. conv2d = tf.layers.conv2d(inputs=inputs, filters=64, kernel_size=3, strides=1, padding='same', activation=tf.nn.relu, kernel_initializer=tf.initializers.truncated_normal(stddev=0.01), kernel_regularizer=tf.contrib.layers.l2_regularizer(0.005), bias_initializer=tf.zeros_initializer()) It’s time to compare the results. Did both of tf.contrib and tf.layers produce the layers with similar functionality? Did one of them do more than the other? First, let’s consider the variables created by above commands. You can use the method tf.global_variables() to get all variables in the current graph. # Variables created by tf.contrib.layers.conv2d [&lt;tf.Variable 'Conv/weights:0' shape=(3, 3, 3, 64) dtype=float32_ref&gt;, &lt;tf.Variable 'Conv/biases:0' shape=(64,) dtype=float32_ref&gt;] # Variables created by tf.contrib.layers.conv2d [&lt;tf.Variable 'conv2d/kernel:0' shape=(3, 3, 3, 64) dtype=float32_ref&gt;, &lt;tf.Variable 'conv2d/bias:0' shape=(64,) dtype=float32_ref&gt;] Phew, the variable sets are similar. They both created a weights Tensor, and a biases Tensor with the same shape. Notice that their names are slightly different, though. Next, let’s check if the two functions generated different sets of operations. The command we can use is tf.get_default_graph().get_operations(). # Operations created by tf.contrib.layers.conv2d &lt;tf.Operation 'Placeholder' type=Placeholder&gt;, &lt;tf.Operation 'Conv/weights/Initializer/truncated_normal/shape' type=Const&gt;, &lt;tf.Operation 'Conv/weights/Initializer/truncated_normal/mean' type=Const&gt;, &lt;tf.Operation 'Conv/weights/Initializer/truncated_normal/stddev' type=Const&gt;, &lt;tf.Operation 'Conv/weights/Initializer/truncated_normal/TruncatedNormal' type=TruncatedNormal&gt;, &lt;tf.Operation 'Conv/weights/Initializer/truncated_normal/mul' type=Mul&gt;, &lt;tf.Operation 'Conv/weights/Initializer/truncated_normal' type=Add&gt;, &lt;tf.Operation 'Conv/weights' type=VariableV2&gt;, &lt;tf.Operation 'Conv/weights/Assign' type=Assign&gt;, &lt;tf.Operation 'Conv/weights/read' type=Identity&gt;, &lt;tf.Operation 'Conv/kernel/Regularizer/l2_regularizer/scale' type=Const&gt;, &lt;tf.Operation 'Conv/kernel/Regularizer/l2_regularizer/L2Loss' type=L2Loss&gt;, &lt;tf.Operation 'Conv/kernel/Regularizer/l2_regularizer' type=Mul&gt;, &lt;tf.Operation 'Conv/biases/Initializer/zeros' type=Const&gt;, &lt;tf.Operation 'Conv/biases' type=VariableV2&gt;, &lt;tf.Operation 'Conv/biases/Assign' type=Assign&gt;, &lt;tf.Operation 'Conv/biases/read' type=Identity&gt;, &lt;tf.Operation 'Conv/dilation_rate' type=Const&gt;, &lt;tf.Operation 'Conv/Conv2D' type=Conv2D&gt;, &lt;tf.Operation 'Conv/BiasAdd' type=BiasAdd&gt;, &lt;tf.Operation 'Conv/Relu' type=Relu&gt; # Operations created by tf.layers.conv2d &lt;tf.Operation 'Placeholder' type=Placeholder&gt;, &lt;tf.Operation 'conv2d/kernel/Initializer/truncated_normal/shape' type=Const&gt;, &lt;tf.Operation 'conv2d/kernel/Initializer/truncated_normal/mean' type=Const&gt;, &lt;tf.Operation 'conv2d/kernel/Initializer/truncated_normal/stddev' type=Const&gt;, &lt;tf.Operation 'conv2d/kernel/Initializer/truncated_normal/TruncatedNormal' type=TruncatedNormal&gt;, &lt;tf.Operation 'conv2d/kernel/Initializer/truncated_normal/mul' type=Mul&gt;, &lt;tf.Operation 'conv2d/kernel/Initializer/truncated_normal' type=Add&gt;, &lt;tf.Operation 'conv2d/kernel' type=VariableV2&gt;, &lt;tf.Operation 'conv2d/kernel/Assign' type=Assign&gt;, &lt;tf.Operation 'conv2d/kernel/read' type=Identity&gt;, &lt;tf.Operation 'conv2d/kernel/Regularizer/l2_regularizer/scale' type=Const&gt;, &lt;tf.Operation 'conv2d/kernel/Regularizer/l2_regularizer/L2Loss' type=L2Loss&gt;, &lt;tf.Operation 'conv2d/kernel/Regularizer/l2_regularizer' type=Mul&gt;, &lt;tf.Operation 'conv2d/bias/Initializer/zeros' type=Const&gt;, &lt;tf.Operation 'conv2d/bias' type=VariableV2&gt;, &lt;tf.Operation 'conv2d/bias/Assign' type=Assign&gt;, &lt;tf.Operation 'conv2d/bias/read' type=Identity&gt;, &lt;tf.Operation 'conv2d/dilation_rate' type=Const&gt;, &lt;tf.Operation 'conv2d/Conv2D' type=Conv2D&gt;, &lt;tf.Operation 'conv2d/BiasAdd' type=BiasAdd&gt;, &lt;tf.Operation 'conv2d/Relu' type=Relu&gt; Now, what is the verdict? As we can observe above. Using tf.contrib or tf.layers will save us a lot of time and prevent us from headache later on. Moreover, they create absolutely similar things. What does that mean to us? It means that it doesn’t matter what your preferred module is, you can create/re-create any networks or you can even use the weights trained by the code written on the other module. But hey, you can’t see that the names are obviously different, can you? You might ask. As long as the shapes and types of variables are the same, mapping the names between the two variable sets is not that painful task. In fact, it’s just no more than 5 lines of code and yeah, you only need to know how to do it. As we addressed the problems earlier in this post, Tensorflow is not hard, it is just kind of confusing. Conclusion Oh, that was so long. Thank you guys for reading. Before we say goodbye, let’s take a look at what we did in this post: We discussed why Tensorflow may seem confusing We talked about heavily in-use Tensorflow module We checked if different modules produce different results on the same task This post is no more than an entry point, some kind of what-I-would-talk-about-when-I-talk-about-Tensorflow (I borrowed that title from Haruki Murakami, check it here). Personally, I am a big fan of learning-by-doing style. Even in the deep-learning field which seems way too deeply academic, it can work out well. In the future posts, I will guide you through how we can accomplish the most common tasks with Tensorflow. Those won’t make you a deep learning guru, but with a solid understanding about how to use the proper tool, and with some practice from your own, what can stop you from building amazing things? Okay, I might have exaggerated a little bit, but honestly I hope that I can make something that you guys can benefit from. So, I’m gonna see you soon, in the next blog post of this Tensorflow series.",
    "tags": "machine-learning deep-learning keras tensorflow tutorials gpu note Tutorial",
    "url": "/mahaveer0suthar.github.io/tutorial/What-I-talk-about-Tensorflow/"
  },{
    "title": "Installing NVIDIA Docker On Ubuntu 16.04",
    "text": "Hey guys, it has been quite a long while since my last blog post (for almost a year, I guess). Today, I am going to tell you about something that I wish I had known before: NVIDIA Docker. What is Docker? And what is NVIDIA Docker? When you come across the term Docker somewhere, you may feel (just like I did) a little bit confused about what it is, and why we should consider using it. That’s totally understandable if you are just machine learning enthusiasts like I was before, which you may not need to care about managing resources, setting up development/testing/production environment or something like that. But what if you are serious about getting a career in this field, working on some large scale projects with a team? It is likely that you and your guys will end up sharing one “bucky” supermachine (such as DGX-1), rather than using your own machine. Because of that, it will be best if you know about Docker or at least know how to use it! So, what is Docker? I’m not gonna rewrite any kind of long explanation since you can get it from Docker’s page. In short (which is why it may not be absolutely precise), Docker is a software which help you create a virtual Linux/Windows environment called container, which you can then develop and run applications on. (Image captured from Docker’s page) “Hey, isn’t it what VM (Virtual Machine) does?” - You may ask. Docker containers and VMs generally can do the same thing. For instance, you want to run Windows application on Ubuntu? Well, you can use Docker to create a Windows container, and VM can do the trick as well. To answer to the question above, please take a look at the picture below which I took from Docker’s page. They even show us the difference, don’t they? (Image captured from Docker’s page) What we can see from the picture is that, using VM requires creating a full copy of the OS for each application, whereas within Docker, many containers can share the same OS while running their own applications, which is the reason why: Docker takes less hard disk space Docker takes less RAM to operate Docker containers can boot within seconds Containerized software will always run the same, regardless of the environment For a more detailed comparison, you can take a look at this thread on StackOverflow: How is Docker different from VM? Last, but not least, what about NVIDIA Docker??? Well, it’s a pretty tedious task to get your graphic cards working on virtual machines, and the same thing happens when using Docker too. And then NVIDIA Docker, which is simply a plugin to Docker, came out and turn that task into just a piece of cake! And that’s it, even the name explains itself, right? Installing NVIDIA docker So now you may got some idea about what Docker is, let’s get into the most important part: installing NVIDIA Docker. As I said in the previous part, NVIDIA docker is just a plugin to docker, which makes GPU accessible from inside docker’s containers. Therefore, installing NVIDIA docker consists of three steps like below: Installing NVIDIA driver Installing docker Installing NVIDIA docker Installing NVIDIA driver NVIDIA driver is simply the necessary driver to use your GPU. You only need to install NVIDIA driver to your PC in order to use NVIDIA docker, which is a big advantage of docker. How to install NVIDIA driver depends on what Linux distribution you are using. And NVIDIA has a very detailed step-by-step instructions for all Linux distributions, so I think I should leave this part for you guys :) It’s very easy, don’t panic. My one piece of advice is: the newer the driver is, the better! The reason is that, newer version of CUDA toolkit may not work with old version of NVIDIA driver. Here is the link to the instructions: CUDA Installation Guide. Installing docker Next, we will install docker. Docker has two available editions: Community Edition (CE) and Enterprise Edition (EE). And just like NVIDIA driver, you need to know what Linux distribution you are using to choose the proper installation file. Below is the installing instructions for Docker Community Edition on Ubuntu (the OS I am using, of course). Firstly, you need to remove (if any) old version of docker. If you can assure that this is the first time you install docker on your machine, then you can skip to the next step. Otherwise, you’d better run the following command: $ sudo apt-get remove docker docker-engine docker.io If docker is not installed on your machine, then apt-get will tell you that. It is totally fine. Next, we will install docker. I recommend installing docker using the repository so that when the new version is available, you can get the update from repository easily. To installing from repository, we need to set up the Docker repository first. As usual, you may want to update the apt package index: $ sudo apt-get update Next, install the packages to allow apt to use a repository through HTTPS: $ sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common Next, add the official GPG key of Docker: $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Verify that the command below print out 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88: $ sudo apt-key fingerprint 0EBFCD88 Next, tell apt to use the stable repository by run the command below: $ sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" At this point, we have finished set up the repository. Next, we will update the apt package index and install Docker CE: $ sudo apt-get update &amp;&amp; apt-get install docker-ce Next, we will check if Docker is installed correctly by running the well-known hello-world Image: $ sudo docker run hello-world Your screen should print out something like below: Installing NVIDIA docker In the next step, we will finish our job by installing NVIDIA docker, which is just a plug in of Docker to help container use the GPUs of the host machine. First, you need to remove NVIDIA docker 1.0 (if installed): docker volume ls -q -f driver=nvidia-docker | xargs -r -I{} -n1 docker ps -q -a -f volume={} | xargs -r docker rm -f sudo apt-get purge -y nvidia-docker Next, we will add the necessary repository, then update the apt package index: curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \\ sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/ubuntu16.04/amd64/nvidia-docker.list | \\ sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update We are nearly there, next we will install NVIDIA docker: sudo apt-get install -y nvidia-docker2 sudo pkill -SIGHUP dockerd That’s it. We have installed NVIDIA docker. Let’s verify the installation by running the latest CUDA image, which is officially provided by NVIDIA: docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi If this is the first time you run the command above, you might notice that Docker is trying to download something like below: Remember that Docker somehow works the same way as VM? It means that in order to create Containers, Docker first needs an Image too. And where is it getting the Image from? Well, I will make it clear in the next blog post. After the Image is downloaded and the command is executed, you will see something like that: The command above should print out the information of your host machine’s GPU(s) (as the nvidia-smi command usually does). Summary Today I have introduced to you NVIDIA Docker, which I personally think that every deep learning developer should know about and get used to. And I hope you all successfully installed NVIDIA Docker. Some of you may not be convinced yet, which is why I am creating another post about some practical use cases as well as some tips on using Docker. So how do we utilize Docker to be more efficient at work? Wait for the next blog post! See you!",
    "tags": "machine-learning deep-learning docker nvidia nvidia-docker gpu note Project",
    "url": "/mahaveer0suthar.github.io/project/Installing-NVIDIA-Docker-On-Ubuntu-16.04/"
  },{
    "title": "Creating A Language Translation Model Using Sequence To Sequence Learning Approach",
    "text": "Hello guys. It’s been quite a long while since my last blog post. It may sound like an excuse, but I’ve been struggling with finding a new place to move into. And I had to say, it’s a real problem for a foreigner to find a reasonable apartment in Japan. Luckily, I somehow managed to find one, and I have just moved in for nearly two weeks. Anyway, the toughest time has gone, and now I can get myself back to work, to bring to you guys new interesting (and maybe boring as usual) blog posts on Deep Learning. So, in my previous blog post, I told you about how to create a simple text generator by training a Recurrent Neural Network model. What RNNs differ from normal Neural Networks is, instead of computing the output prediction on each input independently, RNNs compute the output of timestep \\(t\\) using not only the input of timestep \\(t\\), but also involving the input of previous timesteps (say, timestep \\(t-1\\), \\(t-2\\), \\(\\dots\\)). As you already saw in my previous post, inputs are actually sequences of characters, and each output was simply the corresponding input shifted by one character to the right. Obviously, you can see that each pair of input sequence and output sequence has the same length. Then, the network was trained using the famous Harry Potter as training dataset and as a result, the trained model could generate some great J.K. Rowling-style paragraphs. If you haven’t read my previous post yet, please take a look at it by the link below (make sure you do before moving on): Creating A Text Generator Using Recurrent Neural Network But here comes a big question: What if input sequence and output sequence have different lengths? You know, there are many types of Machine Learning problems out there where input and output sequences don’t necessarily have the exact same length. And in terms of Natural Language Processing (or NLP for short), you are more likely to face problems where their lengths are totally different, not only between each pair of input and output sequence, but also between input sequences themselves! For example, in building a language translation model, each pair of input and output sequence are in different languages, so there’s a big chance that they don’t have the same length. Moreover, I can bet my life that there is no known language where we can create all sentences with the exact same length! Obviously, that is a really big big problem, because the model I showed you in the last post required all the input and output sequences have the same length. Sounds impossible, huh? The answer is: NO. Big problems only got smart people attracted, and as a result, solved by not only one, but many solutions. Let’s go back to our problem. A lot of attempts were made, each of them has its own advantages and disadvantages when compared to others. And in today’s post, I will introduce to you one approach which received great attention from NLP community: The Sequence To Sequence Networks (or seq2seq for short), great work by Ilya Sutskever, Oriol Vinyals, Quoc V. Le from Google. I will talk briefly about the idea behind seq2seq right below. For ones who want to understand deeply about the state-of-the-art model, please refer to the link to the paper at the end of this post. At this point, we have already known the problem we must deal with, that we have input and output sequences of different lengths. To make the problem become more concrete, let’s take a look at the graph below: (Image cut from the original paper of Sequence to Sequence Learning with Neural Networks) As illustrated in the graph above, we have “ABC” as the input sequence, and “WXYZ” as the output sequence. Obviously, the lengths of the two sequences are different. So, how does seq2seq approach to solve that problem? The answer is: they create a model which consists of two seperate recurrent neural networks called Encoder and Decoder respectively. To make it easy for you, I drew a simple graph below: As the names of the two networks are somehow self-explained, first, it’s clear that we can’t directly compute the output sequence by using just one network, so we need to use the first network to encode the input sequence into some kind of “middle sequence”, then the other network will decode that sequence into our desire output sequence. So, what does the “middle sequence” look like? Let’s take a look at the next graph below: The mystery was revealed! Concretely, what the Encoder actually did is creating a temporary output vector from the input sequence (you can think about that temporary output vector as a sequence with only one timestep). Then, that vector is repeated \\(n\\) times, with \\(n\\) is the length of our desire output sequence. Up to this point, you may get all the rest. Yep, the Decoder network acts exact the same way with the network I talked about in the last post. After repeating the output vector from the Encoder \\(n\\) times, we obtain a sequence with exact the same length with the associated output sequence, we can leave the computation for the Decoder network! And that’s the idea behind seq2seq. It’s not as hard as it might seem, right? So we now know about how to output a sequence from an input of different length. But what about the lengths of input sequences? As I mentioned above, the input sequences themselves don’t necessarily have the exact same length, either! Sounds like an other headache, doesn’t it? Fortunately, it’s far more relaxing than the problem above. In fact, all we need to do is just something called: Zero Padding. To make it easier for you to understand, let’s see the image below: Here I prepared five sentences (they were actually from a great song of Twenty One Pilots, link provided at Reference) and let’s imagine that they will be the input sequences to our network. As you could see, three sentences are not equal in length. To make them all equal in length, let’s take the length of the longest sentence as the common length, and we only need to add one same word some times to the end of the other two, until they have the same length as the longest one. The added word must not resemble any words in the sentences, since it will cause their meaning to change. I will use the word ZERO, and here’s the result I received: You might get this now. And that’s why it is called zero padding. In fact, what I did above is not exactly zero padding, and we will likely implement it differently. I’ll tell you more in the Implementation section. For now, all I wanted to do is just to help you understand zero padding without any hurt. We are half way there! We now know all we need to know about the-state-of-the-art Sequence to Sequence Learning. I can’t help jumping right into Implementation section. Neither can you, right? Implementation (You can find the whole source files on my GitHub repository here: seq2seq) So, now we are here, finally, right in the Implementation section. Working with NLP problems is literally abstract (than what we did in Computer Vision problems, which we could at least have some visualization). Even worse, deep neural network in common is kind of abstract itself, so it seems that thing’s gonna get more complicated here. That’s the reason why I decided not to dig into details in the previous section, but to explain it along with the corresponding part in the code instead so that you won’t find it difficult to understand the abstract terms (at least I think so). And now, let’s get your hands dirty! As usual, we will start with the most tedious (and boring) but important task, which is Data Preparation. As you already saw in my previous post, it is a little bit more complicated to prepare language data rather than image data. You will understand why soon. Before doing any complicated processing, first we need to read the training data from file. I defined X_data and y_data variables to store the input text and the output text, respectively. They are all raw string objects, which means that we must split them into sentences: X = [text_to_word_sequence(x)[::-1] for x, y in zip(X_data.split('\\n'), y_data.split('\\n')) if len(x) &gt; 0 and len(y) &gt; 0 and len(x) &lt;= max_len and len(y) &lt;= max_len] y = [text_to_word_sequence(y) for x, y in zip(X_data.split('\\n'), y_data.split('\\n')) if len(x) &gt; 0 and len(y) &gt; 0 and len(x) &lt;= max_len and len(y) &lt;= max_len] Let’s break it down for a better understanding. I will use the three sentences above as our X_data, here’s what happened after X_data.split(‘\\n’) The easiest way to split a raw text into sentences is looking for the line break. Of course, there are many other better ways, but let’s make it simple this time. So, from a raw text we now obtained an array of sentences. Next, for each sentence in the array, we must then split it into an array of words, or say it in a more proper way, a sequence of words. We will do this by using Keras’ predefined method called text_to_word_sentence, as illustrated below: Splitting a sentence into a sequence of words is harder than splitting text into sentences, since there are many ways to seperate words in a sentence, e.g. spaces, commas and so on. So we should not self-implement it but make use of predefined method instead. text_to_word_sentence also helps us remove all the sentence ending marks such as periods or exclamation marks. Quite helpful, isn’t it? So, here’s what we received, an array of sequences of words: But wait! There’s one minor change which needs to be made to the input sequences, as mentioned from the paper as follow: We found it extremely valuable to reverse the order of the words of the input sentence. So for example, instead of mapping the sentence a, b, c to the sentence α, β, γ, the LSTM is asked to map c, b, a to α, β, γ, where α, β, γ is the translation of a, b, c. This way, a is in close proximity to α, b is fairly close to β, and so on, a fact that makes it easy for SGD to “establish communication” between the input and the output. We found this simple data transformation to greatly boost the performance of the LSTM. If you noticed the graph I drew above, you would have some doubt about the order of the input sequence. Yeah, as you might guess, the order of the input sequence is reversed before going into the network. And that’s the reason why I added [::-1] to reverse the sequence split from the raw text. So, the final input sequences look like below: Seems like we’re done, right? But sadly, we are only half way there before we can actually have the network train our data. As computers can only understand the gray scale values of pixels in an image, inputting sequences of raw human-alike words will make no sense to computers. For that reason, we need to take a further step, which is converting the raw words into some kind of numeric values. To do that, we need a dictionary to map from a word to its corresponding index value, and another dictionary for the same purpose, but in reverse direction. But first, what we need is a vocaburaly set. You can think of vocabulary set as an array which stores all the words in the raw text, but each word only appears once. dist = FreqDist(np.hstack(X)) X_vocab = dist.most_common(vocab_size-1) dist = FreqDist(np.hstack(y)) y_vocab = dist.most_common(vocab_size-1) In real deep learning projects, especially when we’re dealing with NLP problems, our training data is pretty large in size, which the number of vocabularies may be up to millions. Obviously, that’s too much for our computers to handle. Furthermore, words which appear only a few times (typically once or twice) in the whole text may not have a significant impact on the learning of our network. So, what we do first is to count the frequency which a word appears in the text, then we create the vocabulary set using only 10000 words with highest frequencies (you can change to 20000 or more, but make sure that your machine can handle it). The result may look like below: So we just have created the vocabulary set from the input text. In the next step, we will create two dictionaries to map between each word and its index in the vocabulary set, and vice versa. # Creating an array of words from the vocabulary set, we will use this array as index-to-word dictionary X_ix_to_word = [word[0] for word in X_vocab] # Adding the word \"ZERO\" to the beginning of the array X_ix_to_word.insert(0, 'ZERO') # Adding the word 'UNK' to the end of the array (stands for UNKNOWN words) X_ix_to_word.append('UNK') With the vocabulary set we created above, it’s pretty easy to create an array to store only the words, and eliminate their frequencies of occurrence (we don’t need that information after all). But you may wonder, we were supposed to create some kind of dictionary here in order to convert each index to its associated word, and now what I told you to create is an array. Well, since we want to create the index-to-word dictionary, and we can access any element of an array through its index, it’s better just to create a simple array instead of a dictionary where keys are all indexes! I’m sure you get that now. Next, we will need to add two special words. As I mentioned earlier, we need a word called ZERO in order to make all sequences have the exact same length, and another word called UNK, which stands for unknown words or out of vocabulary in order to represent words which are not in the vocabulary set. There’s nothing special with the word “UNK”, which we can just append it to the end of the index-to-word array. But I want you to pay attention to the word “ZERO”, it must be the element of index 0! You will understand why as we move on to the next steps. So here’s what the index-to-word array looks like: As I told you above, don’t forget to confirm that the word ZERO always be the first element before moving to the next step! Our next step is pretty simple which is creating the word-to-index dictionary from the array above, so all we need is just a single line of code! # Create the word-to-index dictionary from the array created above X_word_to_ix = {word:ix for ix, word in enumerate(X_ix_to_word)} Let’s confirm the dictionary we have just created. Once again, make sure the word ZERO is associated with the index 0! After that, we can move on to the next step. So now we got the two dictionaries ready. The next step is pretty simple: we will loop through the sequences and replace every word in each sequence by its corresponding index number. And also remember that we’re only putting 10000 words with highest frequencies into the vocabulary set, which also means that our network will actually learn words from that vocabulary set only. So here comes the question: What happens to the other words and how can we converse them to numeric values? That’s where the word UNK makes sense. It stands for “Unknown words”, or it’s sometimes called OOV, which means “Out Of Vocabulary”. So, for words which are not in the vocabulary set, we will simply assign them as UNK. And as you may guess, they will all have the same index value. # Converting each word to its index value for i, sentence in enumerate(X): for j, word in enumerate(sentence): if word in X_word_to_ix: X[i][j] = X_word_to_ix[word] else: X[i][j] = X_word_to_ix['UNK'] And here’s what we obtained. Obviously, our sequences don’t contain any ZERO, so the converted sequences only contain numbers from \\(1\\) (which are the associated indexes of words in the sequences). And now we got an array of sequences which all elements are numeric values instead of raw words. In the next step, we will use Keras’ pad_sequences method to pad zeros into our sequences, so as all the sequences will have a same length. I told you about zero padding above, so there’s not much left to talk here, I think. X = pad_sequences(X, maxlen=X_max_len, dtype='int32') y = pad_sequences(y, maxlen=y_max_len, dtype='int32') And here’s what our sequences looks like, after zero padded. As you could see from the image above, what pad_sequences method did is just add additional \\(0\\) to each sequence, to make all the sequences have a same length with the longest one. So it’s very important that the original sequences don’t contain any \\(0\\). That’s the reason why we must add the word ZERO to the beginning of the index-to-word array, so that the index of every word in the vocabulary set is not \\(0\\). If we don’t, and have some word with index \\(0\\) instead, then our network won’t be able to decide whether that \\(0\\) is padded zero, or index of a particular word. And it will definitely lead to a really bad learning. So we now got a new array of sequences which all the lengths are the same. But it still can’t be understand by the network. Concretely, we have to do a final processing step called vectorization: sequences = np.zeros((len(word_sentences), max_len, len(word_to_ix))) for i, sentence in enumerate(word_sentences): for j, word in enumerate(sentence): sequences[i, j, word] = 1. Explaining the process of vectorization (especially in terms of NLP) is kind of tedious, so I think it’s better help you guys have a visualization of it. I’m quite sure you will get it just by having a look at the image below. A picture is worth a thousand words! So, we have finished the toughest part and got our training data ready. Phew! You’d better take a break, we all deserve it! In the next step, we will create the encoder network. Since we need to compute only a single vector from the input sequences, the encoder network is pretty simple, just a network with a single hidden layer is far from enough. But wait! What the heck is Embedding, you may probably ask. In fact, we are supposed to input directly the vectorized array from above step into some kind of recurrent neural network like LSTM or vanilla RNN. But what we’re gonna do is slightly different. We will vectorize only the output sequences, and leaving the zero padded input sequences unchanged. Then, we will put that input sequences into a special layer called Embedding first. Remember that you don’t necessarily use that Embedding layer, instead you can just vectorize the input sequences and put it directly to the LSTM layer. Talking further into Word Embedding is beyond the scope of this post. The reason I use that layer is just to obtain a better result, from the fact that the size of vocabulary set is pretty small. I will definitely talk about Word Embedding in the coming post, I promise. For now, you have two choices, and it’s all on you. model = Sequential() model.add(Embedding(X_vocab_len, 1000, input_length=X_max_len, mask_zero=True)) model.add(LSTM(hidden_size)) Next, we will create the decoder network, which does the main job. First, we need to repeat the single vector outputted from the encoder network to obtain a sequence which has the same length with the output sequences. The rest is similar to the encoder network, except that the decoder will be more complicated, which we will have two or more hidden layers stacked up. For ones who are not familiar with Recurrent Neural Networks and how to create them using Keras, please refer to my previous post from the link in the beginning of this post. model.add(RepeatVector(y_max_len)) for _ in range(num_layers): model.add(LSTM(hidden_size, return_sequences=True)) model.add(TimeDistributed(Dense(y_vocab_len))) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) So we finally got everything ready. Let’s go ahead and train our models. Due to some limitations of memory, I was able to train 1000 sequences, which means 1 batch at a time (with batch size 1000). I still can’t find another better solution to this probem. If you guys have some ideas about it, please kindly let me know :) for k in range(k_start, NB_EPOCH+1): indices = np.arange(len(X)) np.random.shuffle(indices) X = X[indices] y = y[indices] for i in range(0, len(X), 1000): if i + 1000 &gt;= len(X): i_end = len(X) else: i_end = i + 1000 y_sequences = process_data(y[i:i_end], y_max_len, y_word_to_ix) print('[INFO] Training model: epoch {}th {}/{} samples'.format(k, i, len(X))) model.fit(X[i:i_end], y_sequences, batch_size=BATCH_SIZE, nb_epoch=1, verbose=2) model.save_weights('checkpoint_epoch_{}.hdf5'.format(k)) At the time of writing, the model is on its third day of learning and everything seems promising. I will continue to update the result, maybe after letting it learn for four or five more days! Summary So, in today’s blog post, I have talked about the incapability of normal RNN networks to deal with complicated NLP problems, where sequences differ in length. And through a project of creating an English-Finnish Language Translating Model, I also introduced to you a solution to this big problem by using Sequence To Sequence Learning Approach. It’s just a simple experiment, so obviously, there are many places that you can improve. Feel free to play with the model and modify it for your own purposes. After all, language modeling is a quite complicated problem, I think, and so is Sequence To Sequence Approach. For that reason, I don’t expect you to fully understand the idea behind it just by reading this blog post (I myself can’t say that I fully understand it, either!). So I recommend you to take not just one look at the paper, but to read it many times to grab a better understanding. And just don’t forget that we can always discuss here to help each other learn better. Hope you all enjoy my post, and I’m gonna see you guys soon, on my next post. Reference Ilya Sutskever, Oriol Vinyals and Quoc V. Le. Sequence to Sequence Learning with Neural Networks Keras Addition RNN (Sequence to Sequence Learning based implementation) addition_rnn The sentences above were from Heathens, an addicting song of Twenty One Pilots which I kept repeating recently. Watch it here: Heathens",
    "tags": "machine-learning deep-learning keras recurrent neural network gpu training RNN LSTM GRU seq2seq translator model Project",
    "url": "/mahaveer0suthar.github.io/project/Sequence-To-Sequence/"
  },{
    "title": "Creating A Text Generator Using Recurrent Neural Network",
    "text": "Hello guys, it’s been another while since my last post, and I hope you’re all doing well with your own projects. I’ve been kept busy with my own stuff, too. And till this point, I got some interesting results which urged me to share to all you guys. Yeah, what I did is creating a Text Generator by training a Recurrent Neural Network Model. Below is a sample which was generated by the trained Model: They had no choice but the most recent univerbeen fairly uncomfortable and dangerous as ever. As long as he dived experience that it was not uncertain that even Harry had taken in black tail as the train roared and was thin, but Harry, Ron, and Hermione, at the fact that he was in complete disarraying the rest of the class holding him, he should have been able to prove them. Does it sound similar? Yeah, you may recognize J. K. Rowling’s style in the paragraph above. That’s because I trained the Model using the famous Harry Potter series! Do you feel excited and want to create something of your own? Just keep reading, a lot of fun is waiting ahead, I promise! Many of you may know about Recurrent Neural Networks, and many may not, but I’m quite sure that you all heard about Neural Networks. We have already seen how Neural Networks can solve nearly all Machine Learning problems no matter how complicated they are. And because to fully understand how Neural Networks work does require a lot of time for reading and implementing by yourself, and yet I haven’t made any tutorials on them, it’s nearly impossible to write it all in this post. So it’d be better to leave them for some future tutorials and make it easy this time by looking at the picture below instead. As you could see in the picture above, the main reason why Neural Network can out-perform other learning algorithms is because of the hidden layers. What the hidden layers do is to create a more complicated set of features, which results in a better predicting accuracy. I also mentioned about this in my previous posts: the more complicated and informative the features become, the more likely your Model can learn better and give more precise predictions. Despite the outstanding performance that Neural Networks have shown us over the last decade, they still have a big big limitation: they can’t understand the sequence, in which the current state is affected by its previous states. And Recurrent Neural Networks came out as a promising solution for that. The explanation of Recurrent Neural Networks such as what they are, how they work, or something like that is quite long and not the main purpose of this post, which I mainly want to guide you to create your own text generator. In fact, there are many guys out there who made some excellent posts on how Recurrent Neural Networks work. You can refer to their post through the links below. Some of them provides their codes too, but they used Theano or Torch for their work, which may hurt a lot if you don’t have experience with those frameworks. To make it easy for you, I tried to re-implement the code using a more relaxing framework called Keras. You can check it out in the Implementation section below. Recurrent Neural Networks tutorial by Denny Britz Understanding LSTMs by Colah The Unreasonable Effectiveness of Recurrent Neural Networks by Andrej Karpathy And because the fact that there are already many great posts on Recurrent Neural Networks, I will only talk briefly about some points which confused me, and may confuse you too, I think. Vanilla RNN The very first basic idea of RNN is to stack one or more hidden layers of previous timesteps, each hidden layer depends on the corresponding input at that timestep and the previous timestep, like below: The output, on the other hand, is computed using only the associating hidden layer: So, with hidden layers of different timesteps, obviously the new tyep of Network can now have ability to “remember”. But it can’t not remember over a long timestep due to a problem called vanishing gradient (I will talk about it in future post), and it can’t decide which information of some timestep is valuable (which it should keep) and which information is not valuable (which it should forget). So an improvement was required. And Long Short-term Memory, or LSTM came out as a potential successor. Long Short-term Memory Networks Having seen the limitation of vanilla RNN, now let’s take a look at its successor, the LSTM Networks. The explanations of LSTM in the links above are pretty awesome, but honestly, they confused me a little. Personally I think it would be easier to understand if we begin from what RNNs could accomplish: Comparing to RNN, the equation above is exactly the same with RNN to compute the hidden state at timestep \\(t\\). But it’s not the actual hidden state in terms of LSTM, so we name it differently, let’s say \\(o_t\\). So from here, we will see how LSTM was improved from RNN. First, LSTM is given the ability to “forget”, which mean it can decide whether to forget the previous hidden state. All is done by adding Forget Gate Layer: In contrast to forget gate layer, to tell the Model whether to update the current state using the previous state, we need to add Input Gate Layer accordingly. Next, we will compute the temporal cell state for the current timestep. It looks just like the output of RNN above, except that tanh activation function is used: And now, we will compute the actual cell state for current timestep, using the forget gate and input gate above. Intuitively, doing so makes LSTM be able to keep only the necessary information and forget the unnecessary one. After we computed the current cell state, we will use it to compute the current hidden state like below: So after all, we now have the hidden state for the current timestep. The rest is similar to vanilla RNN, which is computing the actual output \\(y_t\\): That’s all I want to tell you about RNNs and LSTMs. I suggest that you read the three articles above for better understanding about how they work. And now let’s jump into the most interesting part (I think so): the Implementation section! Implementation As I mentioned earlier in this post, there are quite a lot of excellent posts on how Recurrent Neural Networks work, and those guys also included the implementations for demonstration. Actually, because they wrote code for teaching purpose, reading the codes does help understanding the tutorials a lot. But I must say that it may hurt, especially if you don’t have any experience in Theano or Torch (Denny wrote his code in Theano and Andrej used Torch). I want to make it easy for you, so I will show you how to implement RNN using Keras, an excellent work from François Chollet, which I had a chance to introduced to you in my previous posts. If you don’t have Keras installed on your machine, just give the link below a click. The installation only takes 20 minutes (max): Installing OpenCV &amp; Keras Now, let’s get down to business. For sake of simplicity, I will divide the code into four parts and dig into each part one at a time. Of course I will omit some lines used for importing or argument parsing, etc. You can find the full source file in my GitHub here: Text Generator. Now let’s go into the first part: preparing the data. 1. Prepare the training data I always try to deal with the most tedious part in the beginning, which is data preparation. Not only because a good data preparation can result in a well learned Model, but this step is also some kind of tricky, which we likely spend a lot of time until it works (especially if you are working with different frameworks). We are gonna work with text in this post, so obviously we have to prepare a text file to train our Model. You can go on the internet to grab anything you want such as free text novels here, and I recommend the file size is at least 2MB for an acceptable result. In my case, I used the famous Harry Potter series for training (of course I can’t share it here for copyright privacy). data = open(DATA_DIR, 'r').read() chars = list(set(data)) VOCAB_SIZE = len(chars) First, we will read the text file, then split the content into an array which each element is a character, and store it into data variable. Next, we will create a new array called chars to store the unique values in data. For example, your text file contains only the following sentence: I have a dream. Then the data array will look like this: data ['I',' ', 'h', 'a', 'v', 'e', ' ', 'a', ' ', 'd', 'r', 'e', 'a', 'm', '.'] And the chars array will look like this: chars ['I',' ', 'h', 'a', 'v', 'e', 'd', 'r', 'm', '.'] As you could see, every element in char array only appears once. So the data array contains all the examples, and the chars array acts like a features holder, which we then create two dictionaries to map between indexes and characters: ix_to_char = {ix:char for ix, char in enumerate(chars)} char_to_ix = {char:ix for ix, char in enumerate(chars)} Why do we have to do the mapping anyway? Because it’s better to input numeric training data into the Networks (as well as other learning algorithms). And we also need a different dictionary to convert the numbers back to the original characters. That’s why we created the two dictionaries above. After we’ve done the file reading, we will create the actual input for the Network. We’re gonna use Keras to create and train our Network, so we must convert the data into this form: (number_of_sequences, length_of_sequence, number_of_features). The last dimension is the number of the features, in this case the length of the chars array above. Next, the length of sequence means how long you want your Model to learn at a time. It’s also the total timesteps of our Networks which I showed you above. The first dimension is the number of sequences, which is easy to achieve by dividing the length of our data by the length of each sequence. Of course we also need to convert each character into the corresponding index number. And what about the target sequences? In this post, we only make a simple text generator, so we just need to set the target by shifting the corresponding input sequence by one character. Obviously our target sequence will have the same length with the input sequence. About model that can output target sequences with different length, I will leave for the next post. X = np.zeros((len(data)/SEQ_LENGTH, SEQ_LENGTH, VOCAB_SIZE)) y = np.zeros((len(data)/SEQ_LENGTH, SEQ_LENGTH, VOCAB_SIZE)) for i in range(0, len(data)/SEQ_LENGTH): X_sequence = data[i*SEQ_LENGTH:(i+1)*SEQ_LENGTH] X_sequence_ix = [char_to_ix[value] for value in X_sequence] input_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE)) for j in range(SEQ_LENGTH): input_sequence[j][X_sequence_ix[j]] = 1. X[i] = input_sequence y_sequence = data[i*SEQ_LENGTH+1:(i+1)*SEQ_LENGTH+1] y_sequence_ix = [char_to_ix[value] for value in y_sequence] target_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE)) for j in range(SEQ_LENGTH): target_sequence[j][y_sequence_ix[j]] = 1. y[i] = target_sequence The code is not difficult to understand at all, but make sure you take a look before moving on. 2. Create the Network So we have done with the data preparation. The rest is some kind of relaxing since we can make use of Keras to help us handle the hardest part: create the Network. We’re gonna use LSTM for its ability to deal with long sequences, you can experiment other Model by changing LSTM to SimpleRNN or GRU. The choice is yours! model = Sequential() model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True)) for i in range(LAYER_NUM - 1): model.add(LSTM(HIDDEN_DIM, return_sequences=True)) model.add(TimeDistributed(Dense(VOCAB_SIZE))) model.add(Activation('softmax')) model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\") You should have no problem in understand the code above, right? There are only few points that I want to make clear: return_sequences=True parameter: We want to have a sequence for the output, not just a single vector as we did with normal Neural Networks, so it’s necessary that we set the return_sequences to True. Concretely, let’s say we have an input with shape (num_seq, seq_len, num_feature). If we don’t set return_sequences=True, our output will have the shape (num_seq, num_feature), but if we do, we will obtain the output with shape (num_seq, seq_len, num_feature). TimeDistributed wrapper layer: Since we set return_sequences=True in the LSTM layers, the output is now a three-dimension vector. If we input that into the Dense layer, it will raise an error because the Dense layer only accepts two-dimension input. In order to input a three-dimension vector, we need to use a wrapper layer called TimeDistributed. This layer will help us maintain output’s shape, so that we can achieve a sequence as output in the end. 3. Train the Network In the next step, we will train our Network using the data we prepared above. Here we want the Model to generate some texts after each epoch, so we set nb_epoch=1 and put the training into a while loop. We also save the weights after each 10 epochs in order to load it back later, without training the Network again! nb_epoch = 0 while True: print('\\n\\n') model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, nb_epoch=1) nb_epoch += 1 generate_text(model, GENERATE_LENGTH) if nb_epoch % 10 == 0: model.save_weights('checkpoint_{}_epoch_{}.hdf5'.format(HIDDEN_DIM, nb_epoch)) 4. Generate text Last but not least, I want to talk a little about the method to generate text. We begin with some random character and use the trained Model to predict the next one. Then we append the predicted character into the input, and have the Model predict the next one, which is the third character. We continue the process until we obtain a sequence with the length we want (500 characters by default). It’s just that simple! def generate_text(model, length): ix = [np.random.randint(VOCAB_SIZE)] y_char = [ix_to_char[ix[-1]]] X = np.zeros((1, length, VOCAB_SIZE)) for i in range(length): X[0, i, :][ix[-1]] = 1 print(ix_to_char[ix[-1]], end=\"\") ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1) y_char.append(ix_to_char[ix[-1]]) return ('').join(y_char) 5. Result I created the Network with three LSTM layers, each layer has 700 hidden states, with Dropout ratio 0.3 at the first LSTM layer. I was training the Network on GPU for roughly a day (\\(\\approx200\\) epochs), and here are some paragraphs which were generated by the trained Model: “Yeah, I know, I saw him run off the balls of the Three Broomsticks around the Daily Prophet that we met Potter’s name!” said Hermione. “We’ve done all right, Draco, and Karkaroff would have to spell the Imperius Curse,” said Dumbledore. “But Harry, never found out about the happy against the school.” “Albus Dumbledore, I should, do you? But he doesn’t want to adding the thing that you are at Hogwarts, so we can run and get more than one else, you see you, Harry.” “I know I don’t think I’ll be here in my bed!” said Ron, looking up at the owners of the Dursleys. “Well, you can’t be the baby way?” said Harry. “He was a great Beater, he didn’t want to ask for more time.” “What about this thing, you shouldn’t,” Harry said to Ron and Hermione. “I have no furious test,” said Hermione in a small voice. To be honest, I was impressed by what the Model can generate. After leaving it a while for learning, as you could see, not only it can generate nearly perfect English words, but it also learned the structures, which means it capitalizes the first letter after period, it knows how to use the quotation marks, etc. And if I don’t tell you anything about RNNs, you may think (even I do too!) that the paragraphs above were written by somebody. So, it’s now your turn to train your own Network using the dataset of your own choice, and see what you achieve. And if you find the result interesting, please let me know by dropping me a line below! Summary So we have come a long way to finish today’s post, and I hope you all now obtain some interesting results for your own. We have walked through a brief introduction about the need of Recurrent Neural Networks o solve the limitation of common Neural Networks and figured out how LSTMs even improved the state-of-the-art vanilla RNNs. And we also implemented our own Networks to create a simple text generator, which we can use to generate some sample texts in the style of what they learned from! Note that this is just a fast and dirty implementation, and obviously there are a lot of rooms for improvement, which I will leave them for you to improvise by yourself. That’s it for today. I will be back with you guys in the coming post, with even more interesting stuff. So just stay updated!",
    "tags": "machine-learning deep-learning keras recurrent neural network gpu training RNN LSTM GRU text generator Project",
    "url": "/mahaveer0suthar.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/"
  },{
    "title": "Training With Your Own Dataset on Caffe",
    "text": "Hi, everyone! Welcome back to my Machine Learning page today. I have been playing around with Caffe for a while, and as you already knew, I made a couple of posts on my experience in installing Caffe and making use of its state-of-the-art pre-trained Models for your own Machine Learning projects. Yeah, it’s really great that Caffe came bundled with many cool stuff inside which leaves developers like us nothing to mess with the Networks. But of course, there comes sometime that you want to set up your own Network, using your own dataset for training and evaluating. And it turns out that using all the things which Caffe provides us doesn’t help Caffe look less like a blackbox, and it’s pretty hard to figure things out from the beginning. And that’s why I decided to make this post, to give you a helping hand to literally make use of Caffe. Before getting into the details, for ones that missed my old posts on Caffe, you can check it out anytime, through the links below: Installing Caffe on Ubuntu (CPU_ONLY) Installing Caffe on Ubuntu (GPU) Now, let’s get down to business. In today’s post, I will mainly tell you about two points below: Downloading your own dataset Preparing your data before training Training with your prepared data So, I will go straight to each part right below. 1. Downloading your data I think there’s a lot of ways which everyone of you managed to get your own dataset. If your dataset has been already placed on your hard disk, then you can skip the Downloading section and jump right into the Preparing section. Here I’m assuming that you do not have any dataset of your own, and you’re intending to use some dataset from free sources like ImageNet or Flickr or Kaggle. Then it’s likely that: you can directly download the dataset (from sources like Kaggle), or you will be provided a text file which contains URLs of all the images (from sources like Flickr or ImageNet). The latter seems to be harder, but don’t worry, it won’t be that hard. Directly downloading from source: This kind of download is quite easy. Here I will use the Dogs vs. Cats dataset from Kaggle for example. You can access the dataset from the Download page: Dogs vs. Cats. All you have to do is just register an account, then you can download the whole dataset. There are two of them, one for training purpose, which was named train, and one for evaluating, which was named test1 respectively. I suggest that you should download the training set only. I will explain why when we come to the Preparing section. The file size is quite large so it should take a while to finish. And that’s it. You have the dataset stored on your hard disk! Downloading from URLs As you could see above, it’s great if every dataset was zipped and provided directly to developers. But in fact, due to the copyright of the images (as well as other data types), providing data that way isn’t simple, especially when we talk about an extremely large dataset like ImageNet. So data providers have another way, which is providing you the URLs only, and you will have to access to the image hosts yourself to download the data. I will use a very famous site for example, which is ImageNet, the site which holds the annual ILSVRC. You can read more about ILSVRC here. First, let’s go to the ImageNet’s URLs download page: Download Image URLs. All you need to know to get the URLs is something called WordNet ID (or wnid). You can read more about ImageNet’s dataset and WordNet to grab some more details because this post will be too long if I explain it here. To make it simple right now, ImageNet uses WordNet’s synset, such as n02084071, n02121620 which represents dogs and cats respectively, to name its classes. To find out what the synset of a particular noun, just access Noun to Synset, then search for any noun you want, then you will see the corresponding synset. Once you knew the synset, you can download the URLs by going to this page: http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=[wnid], which [wnid] is the synset of the object you want to download data for. For example, let’s use two synsets above, to download the URLs of the Dogs and Cats images of ImageNet: Dogs: http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n02084071 Cats: http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n02121620 If you access the links above, you will see something like this: So, the next thing to do is just simple, you have to copy those URLs and paste somewhere, let’s say a text file or something. With ones who are familiar with Linux commands, you can see that we can use wget to grab all the images with ease. But there’s some problem here: using wget is hard to rename the images as wget will use the name right in each URL to name each image. Training your own data with CNN on Caffe may not require any naming rules, but if you have intention to use your own data in other places, for example, the state-of-the-art Faster R-CNN, then the naming convention does matter! And as far as I know, we can manually rename all the images while downloading using wget, but it requires some experience in Linux commands, and to be honest, I tried and failed. But don’t worry, I found the solution for that! You know that Caffe provides us so many useful tools, to help us do all the heavy things so that we can use all the pre-trained Models without worrying about the data preparation, which means that, if you want to play with MNIST, Caffe provides you the script to download MNIST, if you want to play with CIFAR-10, Caffe got a script to download CIFAR-10 too. So, we can make use of the tools Caffe provides, and modify a little to make it work with our data. Not so bad, right? All you have to do, is to make use of the tool which Caffe uses to download Flickr’s images for fine-tuning (I will tell you about fine-tuning in the second part, so don’t care about that term). Open your terminal, and type the commands below (make sure that you are in the root folder of Caffe): cd data mkdir DogsCats cd ../examples mkdir DogsCats sudo cp finetune_flickr_style/* DogsCats/* What we just did, is to create the neccessary folders for storing the script (./examples/DogsCats) and the images (*./data/DogsCats), then we copied the script to download Flickr’s images to our new folder. Obviously, we have to make some changes in order to make it work properly, just some minor changes. First, let’s go to ./examples/DogsCats folder, unzip the flickr_style.csv.gz to get a CSV file named flickr_style.csv. Open it up, take a look at the file. There are five columns but just three of them are actually used: image_url, label and _split. The image_url column stores all the URLs to all the images, the label column stores the label values, and the _split column tells whether each image is used for training or evaluating purpose. As I mentioned earlier, we are not only downloading the images, but also renaming it, so we will use an additional column to store the name associating with each image URL, which I chose column A for that task. Before making any changes, let’s deleting all the records, except the first row. Then, let’s name cell A1 image_name. Next, in the image_url column, paste all the URLs of each class. Note that we won’t paste all the URLs of all classes at once, since we have to labeling them. After pasting all the URLs of one class, let’s say the Dogs class with n02084071 synset, we will fill the image_name column. Start from cell A2, let’s fill that it will n02084071_0 then drag until you see the last URL in image_url column. Don’t forget to add the .jpg extension when you finish (just use the CONCATENATE function). Next, we will label the images we have just added URLs for. In the label column, let’s fill with 0 until the last row containing URL. Since all URLs we pasted belong to Dogs, so they will have the same label. And lastly, let’s fill in the _split column. In case of the Dogs’ images, we have 1603 images in total, so let’s fill train for the first 1200 images and test for the rest (here the train:test ratio I chose is 0.75:0.25). After all, your CSV should look similar to this: And we can continue with other classes’ images, don’t forget to increase the value of label column each time you add another class’s URLs. So, we have done with the CSV file, let’s go ahead and modify the Python script (make sure that you are in the root folder of Caffe): cd examples/DogsCats sudo vim assemble_data.py First, let’s replace all the phrase data/finetune_flickr_style with *data/DogsCats. That value tells where to store the downloaded images, so we have to point to our new created folder. Next, make some changes like below: # Line 63 df = pd.read_csv(csv_filename, index_col=None, compression='gzip') # Line 77 os.path.join(images_dirname, value) for value in df['image_name'] That’s it. And now we are ready to download the images, and have them renamed the way we wanted: python assemble_data.py It will take a while for the script to run. Note that many of the URLs are inaccessible at the time of writing, since many of them were added quite so long ago. So if you notice that the number of downloaded images is not equal to the number of URLs, don’t be confused. As soon as the script finished running, then your images are all stored on your drive. So now your dataset is ready for the next stage! 2. Preparing the data before training So we just managed to have the desired dataset stored on your hard disk. And believe me or not, we have just completed the most time-consuming task! Before we can train our Network using the data we have just downloaded, there’s some things we need to do. First, we need to convert the downloaded images into the format that the Networks can read. In fact, the Networks in Caffe accepts not just one kind of input data. As far as I know, there are three different ways to prepare our images so that the Networks can read them, and I’m gonna tell you about two of them: normal format and LMDB format. And second, we need to provide one special image called the mean image. Okay, let’s get into each of them. Creating the train.txt and test.txt files Let’s first talk about the data conversion. As I said above, we have two choices. You can choose whether to use the normal format (leave the images untouched after downloaded), or to convert them to LMDB format. In both cases, you have to create two files called train.txt and text.txt. What the two files do is to tell our Network where to look for each image and its corresponding class. To understand better, let’s go and create them. I’m gonna use the Dogs vs Cats dataset which we downloaded from Kaggle (because we haven’t touched it yet, have we?). Let’s create two similar folders just like we did above with ImageNet’s images, one for storing the images, and one for storing the necessary scripts: cd examples mkdir DogsCatsKaggle cd ../data mkdir DogsCatsKaggle Then, let’s place the zip file which we downloaded from Kaggle into ./data/DogsCatsKaggle folder and upzip it. After unzipped, all of the images will be stored into the subfolder called train. Next, we’re gonna create the train.txt and test.txt files. Let’s go into the ./examples/DogsCatsKaggle folder and create a Python file, name it create_kaggle_txt.py and fill the codes below: import numpy as np import os CURRENT_DIR = os.path.abspath(os.path.dirname(__file__)) DATA_DIR = os.path.abspath(os.path.join(CURRENT_DIR, '../../data/DogsCatsKaggle/train')) TXT_DIR = os.path.abspath(os.path.join(CURRENT_DIR, '../../data/DogsCatsKaggle')) dog_images = [image for image in os.listdir(DATA_DIR) if 'dog' in image] cat_images = [image for image in os.listdir(DATA_DIR) if 'cat' in image] dog_train = dog_images[:int(len(dog_images)*0.7)] dog_test = dog_images[int(len(dog_images)*0.7):] cat_train = cat_images[:int(len(cat_images)*0.7)] cat_test = cat_images[int(len(cat_images)*0.7):] with open('{}/train.txt'.format(TXT_DIR), 'w') as f: for image in dog_train: f.write('{} 0\\n'.format(image)) for image in cat_train: f.write('{} 1\\n'.format(image)) with open('{}/text.txt'.format(TXT_DIR), 'w') as f: for image in dog_test: f.write('{} 0\\n'.format(image)) for image in cat_test: f.write('{} 1\\n'.format(image)) Then, all you have to do is to execute the Python script you have just created above: python examples/DogsCatsKaggle/create_kaggle_txt.py Now let’s jump into ./data/DogsCatsKaggle, you will see train.txt and test.txt has been created. And that’s just it. We have finished creating the two mapping files for Dogs vs Cats dataset from Kaggle! So, what about the Dogs and Cats images from ImageNet? Well, you may want to take a look at ./data/DogsCats. Voila! When were the two files created? - You may ask. They were created when you ran the script to download the images! So with ImageNet’s dataset, you don’t have to create the mapping files yourself. That was great, right? Now we got the images, the mapping text files ready, there’s only one step left to deal with the data: create the mean image. The need of computing the mean image Why do we need the mean image anyway? First, that’s just one type of Data Normalization, a technique to process our data before training. As I told you in previous post, the final goal of the learning process is finding the global minimum of the cost function. There’s many factors that affect the learning process, one of which is how well our data was pre-processed. The better it is pre-processed, the more likely our Model will learn faster and better. The goal of computing the mean image is to make our data have zero mean. What does that mean? For example, we have a set of training data like this: \\(x^{(1)}, x^{(2)}, \\dots, x^{(m)}\\). Let’s call \\(x_\\mu\\) the mean value, which means: Next, a new set of data will be created, where each \\(x_{new}^{(i)}=x^{(i)}-x_\\mu\\). It’s easy to see that the mean value of the new dataset is zero: So, above I just showed you a short explanation about one type for Data Normalization, which subtracting by the mean value to get a new dataset with zero mean. I will talk more about Data Normalization in future post. Now, how do we compute the mean image? As you may guess, of course Caffe provides some script to deal with some particular dataset. And we’re gonna make use of it with some modifications! But before we can compute the mean image, we must convert our images into LMDB format first. Converting data into LMDB format But first, why LMDB? Why is LMDB converting considered recommended, especially when we are working with large image database? To make it short, because it helps improving the performance of our Network. At present, performance is not all about accuracy anymore, but required to be both fast and accurate. With a same Network and a same dataset, how the data was prepared will decide how fast our Network learns. And LMDB conversion is one way (among many) which helps accomplish that. And the trade-off? The converted LMDB file will double the size of your downloaded images, since your images were decompressed before being converted (that’s one reason why our Network performs faster with LMDB file, right?) Next, let’s copy the necessary script that we will make use of. I will use Dogs vs Cats dataset from Kaggle for example. sudo cp examples/imagenet/create_imagenet.sh examples/DogsCatsKaggle/ To convert the downloaded Dogs vs Cats dataset to LMDB format using the script above, we will have to make some changes. But it’s not a big deal at all because all we have to change is just the correct path to our images and the mapping text files. Below is the lines which I have applied changes for your reference: EXAMPLE=examples/DogsCatsKaggle DATA=data/DogsCatsKaggle TOOLS=build/tools TRAIN_DATA_ROOT=data/DogsCatsKaggle/train/ VAL_DATA_ROOT=data/DogsCatsKaggle/train/ ... RESIZE=true ... GLOG_logtostderr=1 $TOOLS/convert_imageset \\ ... $EXAMPLE/dogscatskaggle_train_lmdb echo \"Creating val lmdb...\" GLOG_logtostderr=1 $TOOLS/convert_imageset \\ ... $DATA/text.txt \\ $EXAMPLE/dogscatskaggle_val_lmdb echo \"Done.\" Next, let’s go ahead and run the script above: ./examples/DogsCatsKaggle/create_imagenet.sh It will take a while for the conversion to complete. After the process completes, take a look at ./examples/DogsCatsKaggle folder, you will see two new folders which are named dogscatskaggle_train_lmdb and dogscatskaggle_val_lmdb, and new LMDB files were placed inside each folder, created from the training data and test data respectively. Making the mean image After creating LMDB files, making the mean image is no other than one last simple task to complete. All we have to do is to copy and apply some tiny changes into the script which computes the mean image. sudo cp examples/imagenet/make_imagenet_mean.sh examples/DogsCatsKaggle/ And here’s what it looks after modified: EXAMPLE=examples/DogsCatsKaggle DATA=data/DogsCatsKaggle TOOLS=build/tools $TOOLS/compute_image_mean $EXAMPLE/dogscatskaggle_train_lmdb \\ $DATA/dogscatskaggle_mean.binaryproto And, only one last command to execute: ./examples/DogsCatsKaggle/make_imagenet_mean.sh And that’s it. Let’s go into ./data/DogsCatsKaggle folder, you will see one new file called dogscatskaggle_mean.binaryproto, which means that the mean image was created successfully! 3. Training with your prepared data So now you nearly got everything ready to train the Network with the data prepared by yourself. The last thing is, of course, the Network! At this time, you may want to create a Network of your own, and train it using the data above (of your own, too!). But I recommend you try some available Networks which is provided by Caffe, some of which are very famous such as VGG16 or AlexNet. Let’s pick AlexNet for now since it’s quite simpler than VGG16, which will make it train faster. We need to create one new folder and copy the necessary files for Network definition. And for your information, Caffe uses the protobuf format to define the Networks, which you can read for details here: Protocol Buffers. cd models mkdif dogscatskaggle_alexnet sudo cp bvlc_alexnet/solver.prototxt dogscatskaggle_alexnet/ sudo cp bvlc_alexnet/train_val.prototxt dogscatskaggle_alexnet/ Let’s first modify the solver.prototxt first. This file stores the necessary information which the Network needs to know before training, such as the path to the Network definition file, the learning rate, momentum, weight decay iterations, etc. But all you need to do is just to change the file paths: net: \"models/dogscatskaggle_alexnet/train_val.prototxt\" ... snapshot_prefix: \"models/dogscatskaggle_alexnet/caffe_alexnet_train\" Next, we will make change to the Network definition file, which is the train_val.prototxt file. In fact, it was nearly set up and we only need to modify a little bit. First, we have to tell it where to look for your prepared data. And second, we must change the output layer, since our dataset only contains two classes (change this accordingly if you have a different dataset with me). Now open up the file, you will see the first two layers are the data layers, which provide the input to the Network. Stanford University has an excelent tutorial on defining the Network in Caffe at here: Caffe Tutorial. Let’s change the path to the mean image and two LMDB folders which we created above: name: \"AlexNet\" layer { name: \"data\" type: \"Data\" top: \"data\" top: \"label\" include { phase: TRAIN } transform_param { mirror: true crop_size: 227 mean_file: \"data/DogsCatsKaggle/dogscatskaggle_mean.binaryproto\" # MODIFIED } data_param { source: \"examples/DogsCatsKaggle/dogscatskaggle_train_lmdb\" # MODIFIED batch_size: 256 backend: LMDB } } layer { name: \"data\" type: \"Data\" top: \"data\" top: \"label\" include { phase: TEST } transform_param { mirror: false crop_size: 227 mean_file: \"data/DogsCatsKaggle/dogscatskaggle_mean.binaryproto\" # MODIFIED } data_param { source: \"examples/DogsCatsKaggle/dogscatskaggle_val_lmdb\" # MODIFIED batch_size: 50 backend: LMDB } } And there’s only one place left to change: the output layer. Let’s look through the file to find the layer named fc8, that’s the last layer of our Network. It now has 1000 outputs because it was created to train on full ImageNet’s images. Let’s change the number of output according to our dataset: layer { name: \"fc8\" ... inner_product_param { num_output: 2 # MODIFIED ... } Then save the file and that’s it, you can now train the Network with your own dataset! We can’t wait to do it, can we? ./build/tools/caffe train --solver=models/dogscatskaggle_alexnet/solver.prototxt Our Network should be running flawlessly now. And all we have to do is wait until it’s done! We have come a long way until this point. So I think we deserve a cup of coffee or something. That was so fantastic! You all did a great job today. Summary So in today’s post, I have shown you how to train the Network in Caffe, using your own dataset. We went through from how to download the data from URLs file (or directly from host), how to prepare the data to be read by the Network and how to make change to the Network to make it work using our dataset. As you could see, it was not so hard, but it did require some time to dig into. I hope this post can save you quite some of your previous times, and instead, you can spend them on improving your Network’s performance. And that’s all for today. Thank you for reading such a long post. And I’m gonna see you again in the coming post!",
    "tags": "machine-learning deep-learning caffe installation gpu training fine-tuning own data essential Project",
    "url": "/mahaveer0suthar.github.io/project/Training-Your-Own-Data-On-Caffe/"
  },{
    "title": "Machine Learning Part 10: Linear Support Vector Machine",
    "text": "Hi guys! It’s been while since my last tutorial post about Regularization. And today, as I promised, I’m gonna talk about one supervised learning algorithm which took the throne of Neural Network a decade ago. It was fast, agile and outperformed almost the algorithms back in the days. Guys, today I want to tell you about Support Machine Learning, or SVM for short. Many of you may have heard about the term SVM. For example, if you have experience in Computer Vision, especially using OpenCV to accomplish the task, you may have seen something like this on OpenCV’s page in HOG Descriptor section: OpenCV provides an Linear SVM Model for People detection And you will likely come across in other places with the same content, which gives us some proof of the irresistible power of SVM. Despite the fact that there are a great deal of supervised learning algorithms out there nowadays, SVM is still among the mostly applied algorithms. And everytime I face a new Machine Learning problem, the first algorithm I apply is SVM, not only for its performance, but also for its speed and easy-to-implement mechanism, which can give me an overview of the problem as fast as I expect. Above is a brief introduction about SVM. Now let’s go finding the anwser for the question we are longing for: What is SVM? SVM is a supervised learning algorithm which is mostly used for classification problems. It can perform well no matter our dataset is linear or non-linear distributed. But first, to make it easy to understand, in today’s post I’m gonna talk only about how SVM work when dealing with linear data, which can also be called Linear SVM algorithm. And you may remember that I had made a post about one learning algorithm which can give awesome result when dealing with linear dataset. Yeah, I’m talking about Logistic Regression. So, to have a better understanding about Linear SVM, it’s a great idea to recall a little bit about Logistic Regression, and see what they differ from each other. For ones who haven’t skimmed through my post about Logistic Regression, you can find it right below: Machine Learning Part 6: Logistic Regression When we talk about Logistic Regression, we may all think of the sigmoid function, which we use as the activation function. Below is what a sigmoid function looks like: As I told you before, using sigmoid function will ensure that the output will be restricted in the range between \\(0\\) and \\(1\\), which then assigned to either \\(0\\) or \\(1\\) depends on its value and the threshold value. And of course, after getting the predictions with the help of the sigmoid function, we cannot evaluate the Model’s performance without a cost function (or loss function). And the cost function we use in Logistic Regression is the cross-entropy function, which can also be called the log-likelihood cost function: So now if I set \\(cost_1(\\theta^TX)=-log\\left(\\frac{1}{1+e^{-\\theta^TX}}\\right)\\), and \\(cost_0(\\theta^TX)=-log\\left(1-\\frac{1}{1+e^{-\\theta^TX}}\\right)\\) in case of \\(\\theta^TX&gt;=0\\) and \\(\\theta^TX&lt;0\\) respectively, we can re-write the cost function in a simple form like below: Not let’s consider the graph of each seperate part which I divided above: \\(cost_1(\\theta^TX)\\) As you can see, the \\(cost_1(\\theta^TX)\\) term will be very large when \\(\\theta^TX\\) is close to zero, and decrease toward zero as the value of \\(\\theta^TX\\) increases. What does this mean? Before answering that question, let’s consider the other one: \\(cost_0(\\theta^TX)\\) Similar to the \\(cost_1(\\theta^TX)\\) above, the \\(cost_0(\\theta^TX)\\) term will be extremely large when \\(\\theta^TX\\) is close to zero, but this time decrease toward zero as \\(\\theta^TX\\) goes toward to the left. The two terms above were divided from our cost function, which means that their values will be accumulated to the cost function. And our target is to minimize the cost function, you remember that? So, the smaller the two terms are, the smaller the cost function becomes. The smaller the cost function is, the closer our Predictions are comparing to the Label \\(y\\). Now, let’s consider the \\(cost_1(\\theta^TX)\\) term. We compute this term only when the corresponding label \\(y=1\\). As we saw in the graph above, when \\(\\theta^TX\\approx0\\), \\(cost_1(\\theta^TX)\\) becomes very large. That is because we now have the probability that our Model predict the label \\(y=1\\) is very small, and may be even worse if it predict the label to be \\(0\\). As the result, the cost function will become large as a penalty. In contrast, if \\(\\theta^TX\\) is much greater than \\(0\\), then the probability that \\(y=1\\) will be higher. And as the probability becomes nearly \\(1\\), we will have a nearly \\(0\\) cost value. You can explain the \\(cost_0(\\theta^TX)\\) term in the same way. As a conclusion, we will have a result like this: The conclusion above is something which I had shown you in the end of the Logistic Regression tutorial, right? This time I just want to make it more clear if we explain it with considering the effect toward the cost function. Now let’s move to the case of Linear SVM. Linear SVM’s Cost Function After doing some revision on Logistic Regression. Let’s see what the cost function of Linear SVM looks like. First, let me re-write the cost function above, with the use of the two \\(cost_1(\\theta^TX)\\) and \\(cost_0(\\theta^TX)\\) terms, but this time without omitting the regularization term: To make it even simpler, I will omit the \\(\\frac{1}{m}\\) factor. It may affects the value of our cost function, but it doesn’t affect the way our algorithm works, since we are just eliminating a constant from a function. Now, the new cost function looks like above. We can see that it has the form of: \\(\\mathbf{A}+\\lambda\\mathbf{B}\\). Let’s talk a little bit about the \\(\\lambda\\) term, as we call it the weight of regularization, which control how much we want to regularize our parameters. If it’s large, then our parameters will become much smaller and vice versa. Skipped my previous post? You can find it right below: Machine Learning Part 9: Regularization Now how about saying we want to put more weight on the actual cost value? The answer may be: just decrease \\(\\lambda\\). It’s a little bit confusing to someone, so instead of using the form of \\(\\mathbf{A}+\\lambda\\mathbf{B}\\), many people prefer the \\(\\mathbf{C}\\mathrm{A}+\\mathrm{B}\\) form. So now we can say, if we want to emphasize on the actual cost value, we can do it by increasing \\(\\mathbf{C}\\). And that way of expression is also the standard which the scikit-learn library are using. For example, here’s the full description when initializing Logistic Regression I grabbed on scikit-learn’s homepage: With that approach, let’s re-write our cost function again, using the inverse of regularization weight \\(\\mathbf{C}\\) instead of \\(\\lambda\\), here’s what we will have: And as I told you above about deciding the value of the Predictions by considering its effect on the value of our cost function, which I can also say that the way we compute our cost function, or the two \\(cost_1(\\theta^TX)\\) and \\(cost_0(\\theta^TX)\\) terms will affect the way our Model predicts the output. And here’s what Linear SVM differs from Logistic Regression. We will modify the two cost terms a little bit, to have the new graphs like below: Linear SVM’s \\(cost_1(\\theta^TX)\\) graph Linear SVM’s \\(cost_0(\\theta^TX)\\) graph Obviously, as you can see, the two cost terms of Linear SVM looks different from what we saw in Logistic Regression. It the difference in how we define the cost terms in Linear SVM makes it predict in a different way. Telling you about this now may makes you feel confusing a little. But in fact, many cannot tell the difference between Linear SVM and Logistic Regression, since they seem to work the same way. So before I talk further, I think it’s good to notice the difference right this time, so you won’t make any unexpected assumption. Predictions of Linear SVM So, as you see from the graphs of the Linear SVM’s cost terms above. They look pretty much like what we saw in Logistic Regression except two things. First: instead of the non-linear graph which we obtained by the logarithmic function, now we have a new graph with two parts, one part which the cost values are \\(0\\), and the other part which values are not \\(0\\) is now linear. The second thing that Linear SVM differs from Logistic Regression is, the constraint to decide the value of the Prediction is now a little bit harder. Linear SVM requires a safety margin when deciding the Prediction, which we can express like below: Intuitively, this safety margin is the reason why SVM is called the Maximum Margin Classifier, which I told you earlier in this post. What the algorithms does is to find a decision boundary which can obtain the maximum margins from the nearest point of each class. We will have a better visualization of maximum margin right below, in the Implementation section. So, that’s all about Linear SVM. As you can see, if you have already known about Logistic Regression, it’s pretty easy to understand Linear SVM since they have some similar behavior in between. And the main point which drives Linear SVM apart from Logistic Regression is how we define the cost terms in Linear SVM, which then affects the way it decides the value of our Predictions. And now, after reading through a great deal of “lecture”, let’s jump into the Implementation section! Implementation So, we finally come to the Implementation of Linear SVM. And just like Logistic Regression and Decision Tree, scikit-learn library provides us a well pre-implemented Linear SVM. All we have to do is… just use it! You may now became very familiar with scikit-learn library as well as some Python codes we used for data initialization or graph drawing, so I won’t talk about those so much this time. And now, let’s get our hands dirty, by import all the necessary stuff we’re gonna use: import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.svm import SVC from sklearn.datasets import make_classification import numpy as np Next, we will create our data using make_classification method: X, y = make_classification(n_samples=60, n_informative=2, n_features=2, n_redundant=0, random_state=94) To make it simple, this time we just created a dataset with two features and two classes only. Next, let’s create our Linear SVM model, and train it using the data created above: clf = SVC(kernel='linear') clf.fit(X, y) You may wonder what the kernel parameter means. But I will talk about it more in the later post, so now just implement as I did. We set its value “linear” so that scikit-learn knows we want to create a Linear SVM Model. Next, we will draw the decision boundary which seperates the points of two classes for better visualization of the Model’s performance. To help you recall a little bit, the decision boundary in Logistic Regression seperates the coordinate plane into two parts like below (in case we have a dataset of two features and two classes): And through the coef_ and intercept_ attributes of the trained Model, we can use them to draw the decision boundary, just like what we did with Logistic Regression. xx = np.linspace(x1_min, x1_max) w = clf.coef_[0] a = -w[0] / w[1] yy= a * xx - (clf.intercept_[0]) / w[1] And remember that Linear SVM is different from Logistic Regression by the way it defines the cost terms, which then affects the way it decides the value of our Predictions. Concretely, SVM will tend to keep a safety margin when making Predictions, so we’re gonna compute the upper boundary and the lower boundary to help visualize the term maximum margin better: margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2)) yy_down = yy + margin/w[1] yy_up = yy - margin/w[1] And finally, we now have everything ready, let’s go ahead and plot everything on: ax = plt.gca() ax.set_ylim(x2_min, x2_max) plt.scatter(X[:, 0], X[:, 1], c=y, cmap='rainbow') plt.plot(xx, yy, 'k-') plt.plot(xx, yy_down, 'k--') plt.plot(xx, yy_up, 'k--') plt.show() And here’s the result I received: As you could see in the graph above, what Linear SVM did is to find a decision boundary which can keep the maximum margins between the nearest point of each class. And that’s the reason why SVM is usually called the maximum margin classifier. And through implementing Linear SVM as well as drawing both the upper and lower boundaries, I hope you now have a better visualization of what Linear SVM does. Summary So, thank you for staying with me until the end, in my longest post ever. We have talked about Linear SVM, and of course, a little bit deeper about Logistic Regression, just to help you understand better how the two algorithms differ from each other. I hope after this post, you can both have a deep understand about Logistic Regression, and add Linear SVM, one of the most powerful algorithm to your Machine Learning toolbox. In the next post, I will continue with Support Vector Machine, but there won’t be any linear data any more. Next time we will see how SVM can deal with non-linear distributed data, by using something called: the kernel trick. Until then, stay tuned and I will be right back!",
    "tags": "machine-learning support vector machine svm classification regularization SVC essential Tutorial",
    "url": "/mahaveer0suthar.github.io/tutorial/Support-Vector-Machine/"
  },{
    "title": "Solving problem when running Faster R-CNN on GTX 1070",
    "text": "Hello guys, it’s great to be here with you today (why do I keep saying that boring greeting, you may ask). To be honest, there are a lot of things I want to share to you, especially since I built my own machine for Deep Learning. Of course, having my own machine is great, it allows me to try every crazy idea which has ever crossed through my mind, without giving a damn thought about the payment. However, good thing is followed by troubles, as it always be. You might read my last post about my experience in installing Caffe on Ubuntu 16.04, with CUDA and cuDNN to make use of the great power of the GPU. Yeah, we went through so many steps to install so many necessary things. And fortunately, things worked flawlessly in the end. For ones who haven’t read it yet, you can find it right below: Installing Caffe on Ubuntu 16.04 And as you can guess, the next thing I did right after having Caffe installed on my machine, is grabbing the latest Python implementation of Faster R-CNN. I once talked about how to compile and run Faster R-CNN on Ubuntu in CPU Mode, you can refer to it here: Compiling and Running Faster R-CNN on Ubuntu (CPU Mode) Obviously, once you are able to run Faster R-CNN in CPU Mode, making it work with GPU may not sound like a big deal. Why? Because you had successfully installed Caffe, which means you had gone through all the most confusing steps to get CUDA and cuDNN libraries ready. But to tell the truth, I failed to, on the first try! It was a shock to me, which took me a while to overcome. Then I soon realized that, it’s all on me now because it was me who built my own machine. So I had no choice, but to figure it out myself. And after just more than one hour, the problem was solved. The problem always looks harder than it’s supposed to be. I’ve been taught that simple thing so many times in my life, and I just kept forgetting about it. And that’s also the reason why I’m writing this post, to share with you some experience to deal with this kind of troubles. Let’s start from the beginning. I was so excited to get everything ready right after installing Ubuntu, so I immediately jumped into installing Caffe without any consideration. And as you may guess, I grabbed all in the latest version, which means that at first, I installed cuDNN v5.1 to CUDA installation folder. Things worked just right with Caffe, until it came to Faster R-CNN. To make it more clear, I downloaded the latest Python implementation of Faster R-CNN from their GitHub as before: git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git After that, I compiled the libraries by jumping into lib folder and run: cd lib make Luckily, there’s no need to change anything this time. Phew! Then I went backward, then jumped into the caffe-fast-rcnn folder, create Makefile.config from Makefile.config.example file, then applied some necessary modifications to it. That was exactly the same as what we did with Caffe’s installation. You can refer to the link I showed above. And it’s time to compile Caffe for Faster R-CNN: cd ../caffe-fast-rcnn make -j8 &amp;&amp; make pycaffe And I received some unexpected result, like below: Obviously, as the errors were self-explained, there was something wrong with the cuDNN v5.1 library. Did I just say v5.1? That was all my bad, since I forgot that Faster R-CNN is still incompatible with cuDNN v5.1. This wouldn’t happen if I considered carefully before installing cuDNN. But that was an easy fix, since it seemed like replacing with cuDNN v4 helps fix the problem. So I gave that thought a try. The installation of cuDNN v4 is exactly same as cuDNN v5.1 so I omit it from here. After re-installing cuDNN, I ran it again to see if it works: make clean # to delete the previous progress make -j8 &amp;&amp; make pycaffe No errors shown this time. Thank God, seems like it works now, I thought. So I moved on to prepare the pre-trained model, just like the instructions on their GitHub repository: cd .. ./data/scripts/fetch_faster_rcnn_models.sh It took some minutes to complete, since the model is quite large in size. Now, I got everything ready and couldn’t wait any longer to run the demo: python ./tools/demo.py And here’s the result I had, nothing seems to go wrong, I guessed: To tell the truth, I think I’m a very patient guy. So I just left it there for a while. “So, where are all the images?”, I nearly talked to the screen. The reason why I couldn’t stay calm is that, this time there were no errors shown, and no images came out to screen, either. It took me another while to admit that something was going wrong somewhere, and I had to figure it out. Since we ran the demo.py file, then looking at that file first may help find something. Since I had read the paper of Faster R-CNN before, so it was somehow easy to understand what each part of the code is doing. To recap a little bit, as I shown you in the previous post, here’s the result we want to see: So what we want is an image (at least), in which each detected object was bounded by a rectangle, with some text to indicate which class it belongs to. Therefore, it’s not the complicated code used for detecting, but the two parts below that I want you to focus into: You can refer to the paper of Faster R-CNN to find some more details. To make it easy to understand, Faster R-CNN searched for some regions which likely contains an object, then each object was detected with probabilities to indicate how likely that object belongs to a particular class. Let’s open the demo.py file, and make the following modification for a better understand: def demo(net, image_name): \"\"\"Detect object classes in an image using pre-computed object proposals.\"\"\" # Load the demo image im_file = os.path.join(cfg.DATA_DIR, 'demo', image_name) im = cv2.imread(im_file) # Detect all object classes and regress object bounds timer = Timer() timer.tic() scores, boxes = im_detect(net, im) print(scores.shape) # ADD THIS LINE!!! timer.toc() Add one line like above inside the demo method in demo.py file so that we can know the shape of the output scores, then run it again, here’s what I received: Loaded network /home/mahaveer/py-faster-rcnn/data/faster_rcnn_models/VGG16_faster_rcnn_final.caffemodel ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Demo for data/demo/000456.jpg (300, 21) Detection took 0.069s for 300 object proposals ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Demo for data/demo/000542.jpg (259, 21) Detection took 0.064s for 259 object proposals ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Demo for data/demo/001150.jpg (223, 21) Detection took 0.054s for 223 object proposals ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Demo for data/demo/001763.jpg (201, 21) Detection took 0.052s for 201 object proposals ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Demo for data/demo/004545.jpg (172, 21) Detection took 0.051s for 172 object proposals The model used for demo file was fine-tuned so that it can classify 20 classes, plus one background class, so we have 21 classes in total. Let’s look at the result of the first image. The algorithm proposed 300 object regions (or we can say 300 rectangles can be drawn at max), and the corresponding scores had the shape of (300, 21). It means that each proposal, the algorithm computed all 21 probabilities for all classes. And we will base on those probabilities to decide which class it belongs to. Now let’s look at the first part which I highlighted above. What it’s doing is getting the first probability which is greater than the threshold value (it was set to 0.5 this time). So, let’s print out the probabilities to see why it couldn’t get through the if condition which followed: def vis_detections(im, class_name, dets, thresh=0.5): \"\"\"Draw detected bounding boxes.\"\"\" inds = np.where(dets[:, -1] &gt;= thresh)[0] print(dets[:, -1]) # ADD THIS LINE!!! if len(inds) == 0: return Let’s add the line above, right before the if condition, then run it again to see what happens: And here’s the result: And at this time, I thought I knew where the problem came from. None of the probabilities was greater than 0.5, so obviously I ended up with no images being shown. Then, an idea came through my mind. “Why don’t I try to run on CPU?”, I thought. python ./tools/demo.py --cpu The result was like below. I was totally speechless. So I got to go through such a long way, to find out the place which may cause the problem. And thanks to the CPU, or I have to say: I had to make use of the CPU to be sure whether I got it right. Anyway, at least I knew that the compilation was successfully got through. And, the problem is likely from the GPU. Wait, what? The most expensive part of the machine caused the problem? That was ridiculous, I thought. But I couldn’t admit that, so I ran into the net right after. The bad news is: yeah, it was caused by the GPU. And the good news? The good news is: the problem happened because the new GPUs don’t support cuDNN older than v5. Well, you gotta be kidding me. Caffe doesn’t work with cuDNN newer than cuDNN v4, and my GTX 1070 doesn’t support cuDNN older than v5. So does it mean that I won’t be able to run Faster R-CNN on my machine? Thinking it that way did depress me a lot. But fortunately, it turns out that re-compiling Faster R-CNN without cuDNN help solve it. So I moved on and gave it my last shot. I opened the Makefile.config, comment out the USE_CUDNN option, then compile it again. After the compilation completes, I ran it again, hoped that it works for me this time: It finally worked! That was like you woke up after a nightmare. But it was fantastic! I mean, the feeling when you could finally make things done is hard to express, right? Summary In today’s post, I have shown you a problem when running Faster R-CNN with the GTX-1070, which caused the images not being shown when it’s done. Going through every step above, it seemed like the problem was not that hard, and that I didn’t have to make such a long post. As I mentioned earlier, sometimes you will have to deal with the problems yourself, and sometimes, searching Google for the answer in the very beginning is not a good practice at all. As an developer, especially a Machine Learning developer, it’s likely that you are working with not only codes, but also many hard-to-understand algorithms, at least trying to figure things out yourself in the beginning will help you understand the problem more deeper so that the community can get involved efficiently, it also help you gain some very precious experience. And through times, you will be less likely to give up when struggling problems. That’s all I want to tell you today. I’ll be more than happy if you find this post helpful. Now, goodbye and see you in the next post!",
    "tags": "machine-learning faster r-cnn gpu gtx 1070 image not showing caffe compile cpu mode essential Project",
    "url": "/mahaveer0suthar.github.io/project/Problem-Faster-RCNN-GPU/"
  },{
    "title": "Installing Caffe on Ubuntu 16.04 (GPU Mode with CUDA)",
    "text": "It’s great to be with all you guys again in today’s post. As you already knew, it’s been a while since I built my own desktop for Deep Learning. Or for ones who missed that post, you can have a look at my build here: Building up my own machine for Deep Learning And in the last few days, I was like a kid who had just received some new toys from his parents (I bought my desktop by my own money, though). I was so excited that I couldn’t wait any longer to get started. So right after I put all the parts right into their places, the first thing I got to do was installing the OS, of course. I’m running Ubuntu 16.04 on my laptop, so I couldn’t find any reason for not installing the latest Long-Term-Support version of Ubuntu on my desktop. The OS installation was quite easy, especially Ubuntu or any Linux based OS. The next thing to do was to install the necessary drivers. Actually nearly all the drivers were installed during the installation of Ubuntu, so I only had to manually install the GTX 1070 driver, but it was a piece of cake and you would laugh at me if I write it down here. In this post, I want to talk about the three main points below: Installing Caffe on Ubuntu 16.04 in GPU Mode Comparing the performance between CPU and GPU using MNIST and CIFAR-10 datasets As you may notice that I once talked about the first one in my previous posts. Actually I didn’t have myself a desktop with GPU in it, so that post was mainly about how to make things work only by using CPU. And obviously I can’t just do the same thing this time if I want the GTX 1070 to be on the field. In short, there’s a great deal of extra work to do if you want to make use the power of your GPU. And in this post I’m gonna show you how. Installing Caffe on Ubuntu 16.04 in GPU Mode The first thing to do before installing Caffe was to install OpenCV, because I wanted to compile Caffe with OpenCV. Installing OpenCV wasn’t a big deal at all. You can refer at my previous post here: Installing OpenCV &amp; Keras To make it more convenient for you without having to switch between your browser tabs, I think it’s better if I write out the steps for installing OpenCV in this post, too. First, because I got in my desktop a fresh and new OS, I had to perform the commands below to make sure everything is updated to the latest version: sudo apt-get update sudo apt-get upgrade Type the password if prompted. When the process completes, let’s install all the necessary packages: sudo apt-get install build-essential cmake git pkg-config libjpeg8-dev \\ libjasper-dev libpng12-dev libgtk2.0-dev \\ libavcodec-dev libavformat-dev libswscale-dev libv4l-dev gfortran sudo apt-get install libtiff5-dev And we need a library for computing optimization purpose. We will use BLAS just like before: sudo apt-get install libatlas-base-dev Next, we will install pip, a useful tool for managing all Python packages: wget https://bootstrap.pypa.io/get-pip.py sudo python get-pip.py In order to use OpenCV and Caffe, we need to install Python Development Tools package: sudo apt-get install python2.7-dev And of course, a powerful module for dealing with arrays, Numpy: pip install numpy In the next step, let’s download OpenCV 3.0 and its contribution modules: cd ~ git clone https://github.com/Itseez/opencv.git cd opencv git checkout 3.0.0 cd ~ git clone https://github.com/Itseez/opencv_contrib.git cd opencv_contrib git checkout 3.0.0 Note that you have to tell git to checkout to 3.0.0 branch. Now we have everything ready, let’s go and make it: cd ~/opencv mkdir build cd build cmake -D CMAKE_BUILD_TYPE=RELEASE \\ -D CMAKE_INSTALL_PREFIX=/usr/local \\ -D INSTALL_C_EXAMPLES=ON \\ -D INSTALL_PYTHON_EXAMPLES=ON \\ -D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib/modules \\ -D BUILD_EXAMPLES=ON .. make sudo make install sudo ldconfig It should take a while to complete the installation. Try to have yourself a cup of coffee or something, because we are just half way there, lol. After the installation finishes, let’s check if everything works: python &gt;&gt;&gt; import cv2 &gt;&gt;&gt; cv2.__version__ '3.0.0' If you got a result like above, then OpenCV 3.0 was successfully installed on your machine. If something goes wrong, try to do it all over again, this time make sure that you don’t miss any line above. If you still can’t make it work, please let me know by dropping me a line below. I’ll be glad to help. Unlike the previous post, I will skip the installation of Keras this time, and focus on installing Caffe instead. If you want to install Keras, please give the link above a look. So now we have OpenCV 3.0 successfully installed. Next we will continue with Caffe. I’m assuming that you have at least one GPU installed. If you don’t, please refer to the post below: Installing Caffe on Ubuntu (CPU-ONLY) This time we want to make use the power of GPU, we can tell Caffe that we want to use GPU, by commenting out the CPU_ONLY option, do you remember that? Unfortunately, it’s not that simple. Caffe is just a framework which helps us handle the Network, which means that with Caffe, we can define the Network structure, we can define rules, then Caffe will train and evaluate our Model. In fact, Caffe makes use of CUDA, a superb library provided by NVIDIA, to handle the communication with our GPU. So, in the next step, we will install the CUDA Toolkit. Let’s go to the CUDA Toolkit download page, choose your OS, the OS Distribution and version carefully. The rest is simple, just follow the guide on the download page, and it’s done. The installation file’s size is pretty large, so it’s likely to take a while, so don’t lose your patience, lol. Next, we will download cuDNN, which is a GPU-accelerated library of primitives for deep neural networks provided by NVIDIA. With cuDNN, the computation speed will be significantly accelerated. All we have to do is going to cuDNN home page, register to the Accelerated Computing Developer Program (it’s free, but it’s a must to download cuDNN). After registering and completing their short survey, you will be redirected to the download page like below: You may want to download the latest version, as NVIDIA claimed that it’s much faster than its predecessor. If you don’t have any intention to play around with Faster R-CNN, then you can grab the latest version. But if you want to play around with the most outstanding Object Detection algorithm out there, then I highly recommend you to choose the v4 Library. I will tell you why in later post. Before downloading, make sure that you choose the right version for Linux, the upper most one below the install guide: After the download process completes, let’s extract the downloaded file (assuming that you’re placing it under Downloads folder): cd ~/Downloads tar -xvf cudnn-7.0-linux-x64-v4.0-prod.tgz In the next step, you just have to copy the two extracted folders to where CUDA was installed, which is most likely at /usr/local/cuda: sudo cp lib64/* /usr/local/cuda/lib64 sudo cp include/* /usr/local/cuda/include That’s it. You have just successfully installed CUDA and cuDNN. Let’s move on and install Caffe. There won’t be much difference with the installation in CPU_ONLY mode. First, we have to install all the necessary packages and libraries: sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libboost-all-dev libhdf5-serial-dev \\ libgflags-dev libgoogle-glog-dev liblmdb-dev protobuf-compiler sudo apt-get install -y --no-install-recommends libboost-all-dev Next, let’s go to BVLC GitHub repository and grab the latest version of Caffe: git clone https://github.com/BVLC/caffe cd caffe Then we will create the Makefile.config file from the example file, just like before: cp Makefile.config.example Makefile.config Let’s apply those modifications below: # cuDNN acceleration switch (uncomment to build with cuDNN). USE_CUDNN := 1 # Uncomment if you're using OpenCV 3 OPENCV_VERSION := 3 # We need to be able to find Python.h and numpy/arrayobject.h. PYTHON_INCLUDE := /usr/include/python2.7 \\ /usr/local/lib/python2.7/dist-packages/numpy/core/include # Uncomment to support layers written in Python (will link against Python libs) WITH_PYTHON_LAYER := 1 # Whatever else you find you need goes here. INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu/hdf5/serial/ If you went through my post about installing Caffe in CPU_ONLY mode before, then all the modifications above should sound familiar with you. If you didn’t, you might want to take a look at that post to know why we have to make those changes. The only difference with what we did in the previous post is, instead of uncommenting the CPU_ONLY := 1 line, we uncomment the USE_CUDNN := 1 to take advantage of cuDNN. At this point, we can go through the compilation of Caffe without any error: make all &amp; make test &amp;&amp; make runtest &amp;&amp; make pycaffe Next, in order to use Caffe inside our Python code, we have to add pycaffe to the PYTHONPATH: sudo vim ~/.bashrc export PYTHONPATH=$HOME/Downloads/caffe/python:$PYTHONPATH Then execute the command below to make the change take effect: source ~/.bashrc Now let’s check if we have things work properly: python &gt;&gt;&gt; import caffe &gt;&gt;&gt; If it don’t show any import error, then Congratulations, you have just successfully installed Caffe. The installation itself was confusing a little, but it didn’t require any complicated modifications, so somehow we still made it till the end. We can finally exhale now, lol. Comparing the performance between CPU and GPU So we have Caffe compiled, and with the support from CUDA &amp; cuDNN, we can take avantage of our GPU to speed up the learning process significantly. But, that’s just what we have been told so far. When we speak about the performance term, the words “good”, “faster”, “much faster” or even “significantly faster” are way too subtle and not much informative. In order to answer the question “How faster?”, it’s better to consider the difference in computing time between CPU Mode and GPU Mode. I will use two datasets which Caffe provided the trained models: MNIST and CIFAR-10 for comparing purpose. Note that in this post, I just consider the size of the dataset for simplicity, without considering the complexity of the Networks. I will dig more further about it on future posts on Neural Network. MNIST Dataset First, make sure you are in the root folder of Caffe, and run the commands below to download the MNIST dataset: cd $CAFFE_ROOT ./data/mnist/get_mnist.sh ./examples/mnist/create_mnist.sh That’s all we have to do to prepare the data. Let’s see how much time the CPU need to run each iteration: ./build/tools/caffe time --model=examples/mnist/lenet_train_test.prototxt And here’s my result on my Intel Core i7-6700K CPU: As you can see, my CPU took approximately 55ms to run each iteration, in which 23ms for forward pass and 32ms for backward pass. Let’s go ahead and see if the GPU can do better: ./build/tools/caffe time --model=examples/mnist/lenet_train_test.prototxt And here’s the result on my GTX 1070. The result came out nearly right after I hit Enter. I was really impressed, I admit. Each iteration took only 1.7ms to complete, in which 0.5ms for forward pass and 1.2ms for backpropagation. Let’s do some calculation here: the computing time when using GPU is roughly 32 times faster than when using CPU. Hmm, not so bad, you may think. Because MNIST dataset is pretty small in size, which each example is just a 28x28 grayscale image, and it contains only 70000 images in total, the CPU still can give us an acceptable performance. Also note that in order to make use of the power of GPU, our computer has to take some times to transfer data to the GPU, so with small dataset and simple Network, the difference between the two may not be easily seen. Let’s go ahead and give them a more challenging one. CIFAR-10 Dataset CIFAR-10 is way larger comparing to MNIST. It contains 60000 32x32 color images, which means CIFAR-10 is roughly three times larger than MNIST. That’s a real challenge for both to overcome, right? Just like what we did with MNIST dataset, let’s first see how much time it takes using CPU: ./build/tools/caffe time --model=examples/cifar10/cifar10_full_train_test.prototxt And here’s the result I got: As you can see, with a larger dataset (and a more complicated Network, of course), the computing speed was much slower comparing with MNIST dataset. It took approximately 526ms to complete one iteration: 238ms for forward pass and 288ms for backward pass. Let’s go ahead and see how well the big guy can do: ./build/tools/caffe time --model=examples/cifar10/cifar10_full_train_test.prototxt --gpu 0 And the result I had with my GTX 1070: Look at the result above. Unlike the significant decrease in performance as we saw when running on CPU, my GTX 1070 still brought me an impressive computing speed. It took only 11ms on one iteration, in which 3ms for forward pass and 8ms for backpropagation. So when running on CIFAR-10 dataset, the GPU really did outperform the CPU, which computed 48 times faster. Imagine you are working with some real large dataset in real life such as ImageNet, using GPU would save you a great deal of time (let’s say days or even weeks) on training. The faster you obtain the result, the more you can spend on improving the Model. That’s also the reason why Neural Network, especially Deep Neural Network, has become the biggest trend in Machine Learning after long time being ignored by the lack of computing power. Obviously not only nowadays, but Deep Neural Network will continue to grow in the future. Summary So in this post, I have just shown you how to install OpenCV and Caffe in GPU Mode with CUDA Toolkit and cuDNN. I really appreciate that you made it to the end with patience. I hope that this post can help you prepare the necessary environment for your Deep Learning projects. And I also did some comparison on performance between GPU and CPU using two most common datasets: MNIST and CIFAR-10. Through the results above, I think you can now see how using GPU on Deep Neural Network can bring up a big difference. Finally, if you come across any compilation error, please kindly let me know. I’ll try my best to help. Can’t wait to see you soon, in the upcoming post.",
    "tags": "machine-learning deep-learning caffe installation gpu cuda cudnn environment essential Project",
    "url": "/mahaveer0suthar.github.io/project/Installing-Caffe-Ubuntu/"
  },{
    "title": "Machine Learning Part 9: Regularization",
    "text": "Welcome back to my 9th tutorial on Machine Learning! I have been kept busy in last weekends, struggling in getting my desktop ready for Deep Learning. You may consider giving that post a look if you are planning to build your own “Monster” too: Building up my own machine for Deep Learning. Of course I got a lot to tell you about things I’ve been doing with my new desktop. But it will be a little bit selfish of mine and unfair to all of you if I let dust cover my Machine Learning tutorial series. So let’s say, I’m back on track. And today, I will talk about Regularization, a technique to deal with Overfitting problem. You may still remember that I mentioned eariler in previous posts, Overfitting is a big headache in all Machine Learning problems. Beside Cross Validation that I told you before, Regularization is a must-know technique that you will nearly apply in all of your Machine Learning problems. Furthermore, applying Regularization is a default setting in all algorithms provided by scikit-learn library. And I’m not going to waste any minute of yours. Let’s go straight into the most likely asked question: What is Regularization? When we hear the word Regularization without anything else related to Machine Learning, we all understand that Regularization is the process of regularizing something, or the process in which something is regularized. The problem is: what is exactly something. In terms of Machine Learning, we talk about learning algorithms or models, and what is actually inside the algorithms or models? That’s the set of parameters. In short, Regularization in Machine Learning is the process of regularizing the parameters. After knowing that Regularization is actually to regularize our parameters, then you may wonder: Why regularizing the parameters help prevent Overfitting? Let’s consider the graph that I had prepared for this tutorial. A picture is worth a thousand words, right? As you can see in the graph I have just shown you, we got two functions represented by a green curve and a blue curve respectively. Both curve fit those red points so well that we can consider they both incur zero loss. And if you followed all my previous tutorials, you would be able to point out that the green curve is likely to overfit the data. Yeah, you are totally right. But have you ever wondered why the green curve (or any curve which is similar to it) is overfitting the data? To understand that in a more mathematical way, let’s consider the two functions that I used to draw the graph above: The green curve: The blue curve: Does it sound similar to you? I once told you about one way to improve the performance of Linear Regression model, that is adding polynomial features. You can refer it here: Underfitting and Overfitting Problems. You knew that by adding more features, we will have a more well learned model which can fit our data far better. But everything has its drawback, if we add so many features, we will be purnished with Overfitting. That was what I told you in the earlier tutorial. Not so hard to recall, right? If you look at each function’s equation, you will find that the green curve has larger coefficients, and that’s the main caution of Overfitting. As I mentioned before, Overfitting can be interpreted that our model fits the dataset so well, which it seems to memorize the data we showed rather than actually learn from it. Intuitively, having large coefficients can be seen as an evidence of memorizing the data. For example, we got some noises in our training dataset, where the data’s magnitude is far difference than the others, those noises will cause our model to put more weight into the coefficient of higher degree, and what we received is a model that overfits our training data! Some of you may think, if adding so many features causes Overfitting, than why don’t we just stop adding features when we got an acceptable model? But think about that this way. If your customer or your boss wants a learned model with \\(95%\\) accuracy, but you can’t achieve that result without adding some more features, which results in overfitting the data. What will you do in the next step? Or think about it in one more other way. You are facing a problem where you are provided with a large dataset, which each of them contains a great deal of features. You don’t know which features to drop, and even worse if it turns out that every feature is fairly informative, which means that dropping some features will likely ruin the algorithm’s performance. What do you plan to do next? The regularization term Therefore, it’s not always a good idea to drop some features just to prevent Overfitting. And as you saw in the example above, it requires further analysis to know whether you can remove some less informative features. So, it’s a good practice that you use all features to build your first model in the beginning. And regularization comes out as a solution. To make it more clear for you, let’s consider our MSE cost function: I once introduced the MSE cost function before in Logistic Regression tutorial. And as you know, the objective of learning is to minimize that MSE function. It means that our parameters can be updated in anyway, just to lower the MSE value. And as I told you above, the larger our parameters become, the higher chance our Model overfits the data. So the question is: can we not only minimize the cost function, but also restrict the parameters not to become too large? The answer is: we CAN, by adding the regularization term like below: , where \\(\\lambda\\) is a constant to control the value of regularization term, \\(n\\) is the number of the features. Take a look at our new cost function after adding the regularization term. What the regularization term does is to penalize large parameters. Obviously, minimize the cost function consists of minimizing both terms in the right: the MSE term and the regularization term. So each time some parameter is updated to become significantly large, it will increase the value of the cost function by the regularization term, and as a result, it will be penalized and updated to a small value. Also note that we only compute the regularization term with the weights only, DON’T include the bias in the regularization term! You may ask why? Well, we can re-write our activation function like below (in case of polynomial function): As you can see, we can think that the bias term goes with the \\(X^0\\) term, which means that it doesn’t affect to the form of our function, so include the bias term into the regularization term doesn’t make any sense. With the new added regularization term, obviously we have to make some change to the way we update the parameters too. But it’s not a big deal at all, just take all the partial derivatives and we will achieve the result below very easily: For weights (\\( \\theta_1, \\ldots, \\theta_n\\)) For bias (\\( \\theta_0 \\)) it remains unchanged: Next, let’s put things together to see how the parameters are updated after adding regularization term: That’s it. All we have to change is just adding the factor \\((1-\\frac{\\lambda}{m})\\) to the parameter when updating. You can prove the result above yourselves as an assignment. It’s very easy, but I recommend you do it yourselves to become more familiar with those mathematical terms like the Chain Rule or partial derivatives, which you will use a lot in the next tutorials. More about regularization Above I have shown you about adding the regularization term in our MSE function, and how to apply regularization in updating parameters, too. But it doesn’t mean that when applying regularization, you always stick to the term I have shown you. In fact, it’s just one among many forms of regularization. It’s just like the way we have many options for the cost function (or you can call it the loss function), we have MSE function, we have log-likelihood cost function, etc. To a particular problem, there will always be more than one approach, and each one will likely work best in a particular set of conditions. Below I will introduce to you the two which are mostly used in real world projects: L2 Regularization: This is actually the one I have shown above. It’s also easy to remember: L2 means degree \\(2\\) regularization term. Before talking any further, let’s consider the other one first: L1 Regularization: Another form of regularization, called the L1 Regularization, looks like above. As you can see, instead of computing mean value of squares of the parameters as L2 Regularization does, what L1 Regularization does is to compute the mean magnitude of the parameters. Also note that in L1 Regularization term, we multiply the sum with a fraction of \\(\\frac{\\lambda}{m}\\), not \\(\\frac{\\lambda}{2m}\\). Remember that the term \\(\\frac{\\lambda}{2m}\\) helps vanish the \\(2\\) factor in the derivative of polynomial of degree 2. So I hope you won’t be confused which one to use. Looking at two cost functions above, adding L1 Regularization term or L2 Regularization term have nearly the same effect, that is to penalize large parameters. As a result, we end up with a learned model with all parameters being kept small, so that our model won’t depend on some particular parameters, thus less likely to overfit. To understand how the two terms differ from each other, let’s see how they affect the parameter update process: L2 Regularization: To make it simple, I just computed for one parameter \\(\\theta_j\\). As you can see, when applying regularization, we have the new \\((1-\\alpha\\frac{\\lambda}{m})\\) factor. What does it affect the update for \\(\\theta_j\\) anyway? After each learning iteration, \\(\\theta_j\\) decreases by an amount of \\(\\alpha\\frac{\\lambda}{m}\\), which means that L2 Regularization tends to shink it by an amount proportional to \\(\\theta_j\\). The larger \\(\\theta_j\\) becomes, the more it will shrink. Now, what it’s like in the case of L1 Regularization: L1 Regularization: When we use L1 Regularization, our parameters shrink in a different way. Because \\(sign(\\theta_j)\\) can only be either \\(-1\\) or \\(1\\), \\(\\theta_j\\) now shrinks by a constant amount, and it tends to move toward zero. This makes the update process different from what we saw in L2 Regularization. Therefore, we can easily see that L1 Regularization tends to penalize small parameters more than L2 Regularization does. In short: If \\(\\theta_j\\) is large, L2 Regularization shinks \\(\\theta_j\\) more than L1 Regularization does. If \\(\\theta_j\\) is small, L1 Regularization shinks \\(\\theta_j\\) more than L2 Regularization does. Last, but not least… Above I have just shown you two mostly applied forms of regularization, and how each of them affect the learning process. The last thing I want to tell you in this post is about the constant \\(\\lambda\\). As I mentioned earlier, \\(\\lambda\\) controls how much we want to regularize our parameters. But what is really behind this? Let’s consider the two scenarios below: \\(\\lambda\\) is very small (nearly zero): When \\(\\lambda\\) is nearly zero, then the regularization term will become nearly zero. As a result, the cost function mostly depends on the MSE term just like before applying regularization. Or we can say that, when \\(\\lambda\\) is nearly zero, the regularization term won’t have any significant effect on shrinking the parameters. Therefore the model is more likely to overfit. \\(\\lambda\\) is very large: Conversely, let’s consider the case where \\(\\lambda\\) becomes extremely large. This time we put much weight in the regularization term. And as a result, the parameters will shrink to a very small values. That approach, however, brings a real problem, that is, rather than preventing Overfitting, our parameters now become so small so that it can’t even fit the training data well, or we can say that: applying so much regularization cause our model to underfit the dataset. Through the two scenarios above, you can see that choosing the right value for \\(\\lambda\\) is not an easy task. But with the right choice of \\(\\lambda\\), the regularization term can have a significant effect on the learning process, which results in a model which both fits the dataset very well, but not likely to overfit. Summary In today’s post, we have talked about Regularization, an important technique applied in every Machine Learning model in the real world to deal with the Overfitting problem. I hope after this tutorial, you can have a deeper understanding about what actually causes Overfitting, and the right way to deal with that headache. In the next post, I will continue with one last supervised learning algorithm, the one that I should have showed you in this post instead. But I soon realized that with the lack of knowledge about Regularization, it will be pretty hard to fully understand that algorithm. I hope you not blame me for this. So stay updated and I will be right back!",
    "tags": "machine-learning overfitting regression classification regularization essential Tutorial",
    "url": "/mahaveer0suthar.github.io/tutorial/Regularization/"
  },{
    "title": "Building up my own machine for Deep Learning",
    "text": "Hello guys, it’s me again being here with all you guys today. There will be no tutorial in this post, so at first I’m sorry for that. But Halloween is coming to town, right? The weather is good out there, too. So there’s no reason to stay at home spending time on some boring Machine Learning stuff, right? But I did! I spent a whole day on building my own desktop for Deep Learning! Of course I still remember telling you about using g2.2xlarge instance of Amazon Web Service. For ones who are considering using some GPU instance with a reasonable price, please have a look at my previous post: Using AWS EC2 Instance. The main reason why I decided to build my own machine is just simple: I needed a more powerful GPU. Of course I can change to g2.8xlarge instance but I will soon get broke. Having a desktop at home also makes sense for one who mainly works with images and videos like me, since I can visualize the output without writing more codes to use the raw results from EC2 instance. The third reason is: because it’s kind of cool, lol. To be honest, I have never done it before, so I was really eager to try. I was thinking about that for a while. I was surfing on Googles, seeing some cool guys doing some reviews on performance of different GPUs. And below is the build I chose for my own, eventually: CPU: Intel Core i7-6700K 4.0GHz Quad-Core Processor CPU Cooler: Scythe Kotetsu 79.0 CFM Motherboard: ASRock Z170 Extreme4 ATX LGA1151 GPU: Gigabyte GeForce GTX 1070 8GB G1 RAM: Corsair Vengeance LPX 16GB (2 x 8GB) DDR4-2666 HDD: Seagate Barracuda 2TB 3.5” 7200RPM The most important part is the GTX 1070 GPU. At first I intended to build with GTX 970, but I soon realized that the performance of GTX 970 is just not much different from GTX 960, so it may not a good choice especially if you mostly work with Convolutional Neural Network. Guys on Deep Learning community recommended to use at least GTX 980 for CNN. And because GTX 1070 is just slightly more expensive than GTX 980, whereas GTX 1080 is still a big deal, I thought GTX 1070 was the best choice for me. Working with Deep Neural Network requires mostly the power of GPU, so we don’t necessarily buy a giant-killer CPU. But a little guy won’t be a good fit (comparing to the giant GTX 1070), so I chose Intel Core i7-6700K. I also needed a good CPU cooler (not a very big one), too. Since GTX 1070 will be with my team, not only the CPU, but I also needed a motherboard which can hold them well, and has an efficient power consumption, too. My choice was Z170 Extreme4, a motherboard mainly used for gaming PC. The next to consider is storage and RAM. CNN requires a lot of memory to hold the network temporary values (all the parameters and gradients from backpropagation), and a great deal of memory to save the Model, so now I started with 16GB of RAM, and 2TB internal hard disk. The last one is the PSU (power supply unit). I chose a 600W PSU from a Japanese brand, and they don’t sell them outside Japan (as I searched on the net and found no result), so I don’t list out the PSU I bought here. You can buy from your local brand, but I recommend that you choose a PSU which can provide from at least 600W, because a big GPU will likely consume a lot of power. So that’s was all about the parts I need. It’s time to put things together (actually it was finished at the time of writing). Here is the Z170 Extreme4, fantastic design, isn’t it? And if there’s something I forgot to mention, that is the big one below: Since the GTX 1070 is 28cm long, obviously I needed a big case in order to put all these things in. I chose the NZXT S340 Mid Tower case. The price wasn’t good at all, but in the end it turned out that it’s worth every yen. Next, let’s first put the motherboard into the case. I heard some guys recommend putting everything on the motherboard first, then put all into the case. I think both ways work well. But I think it’s easier if in the beginning our motherboard is mounted stably somewhere, and there’s no better place than its final shelter, right? Here’s the picture of my Z170 put into the S340 case: Next, let’s take a look at the tiny CPU. Despite of the power it has, it’s literally tiny comparing to other parts. But it was not until I pull it out that I realized it was heavier than it looks. Maybe that was because the metal which helps it transmit temperature adds more extra weight. Next, let’s mount it onto the motherboard. When I tried to close the cover afterward, it was a lot heavier than it was supposed to be. And I kept wondering for a while, whether I should add more strength or something. After watching some guys on Youtube mounting the core i7-6700k, and mentioning about how hard it was to close the cover, I decided to add more force, too. It turns out that the shape of the core i7-6700K, LGA 1151 caused that problem. Next, I put the cooler on the top of the CPU. It was a little bit tall, that I was afraid if it fit my case. Fortunately, it does. And here comes the boss. I had to say, it was so amazing. Fantastic, tremendous design that I couldn’t have been able to imagine a GPU could be! And here it is after put onto the motherboard. It takes a great deal of space and makes everything look small. The rest was so simple: putting the RAMs, the PSU, and carefully plug them into the right places. And my “Monster” is ready to be unleashed: The moment I stood up to plug the PSU’s cord, I realized that I was sitting for nearly 7 hours! My Saturday was just an extended Friday! But at least, it worked in the end. And I can draw a smile in my face now. I also realized that I just ate some noodles for breakfast, and I started to feel hungry now. Few more hours left for Saturday, and I’ve been thinking about curry for a while. So, I’ll go get some beef right now. Thank you all for watching. If you need some experience on building your own machine for Deep Learning, feel free to contact me. I’ll be glad to help! Goodbye and see you in the next tutorial!",
    "tags": "machine learning deep learning PC desktop GPU GTX 1070 Project",
    "url": "/mahaveer0suthar.github.io/project/Building-Desktop-For-Deep-Learning/"
  },{
    "title": "Machine Learning Part 8: Decision Tree",
    "text": "Hello guys, I’m here with you again! So we have made it to the 8th post of the Machine Learning tutorial series. The topic of today’s post is about Decision Tree, an algorithm that is widely used in classification problems (and sometimes in regression problems, too). Decision Tree is well-known not only for its great performance on classification, but also for its easy-to-understand algorithm. As we have already seen up to now, in Linear Regression and Logistic Regression tutorials, understanding how a learning algorithm works is somehow irritated. We got to go through boring theories, boring mathematical explanations and so on. Although I tried my best to make the explanation as simple as I can, but you know, functions is still functions, matrices are still matrices, there is no other way to get rid of those terms. But you won’t have to go through that pain today. Decision Tree can totally be explained using human-understandable natural language. So keep reading, okay? And before we get started, it’s great to know that we have made it to the 8th post of the Machine Learning tutorial series. If you take a look back, I’m quite sure you will be surprised by how much you have progressed. We have learned two learning algorithms: Linear Regression and Logistic Regression respectively. We have worked on some simple dataset and visualized how your Model performed. At the moment, I’m quite sure that you are now familiar with Machine Learning. So in this post, we will use a more complicated set of data and see how our learning algorithms handle it. So, let’s talk about Decision Tree. You are somehow a real Model of Decision Tree algorithm yourself! In your daily life, you make many decisions exactly the same way Decision Tree does, subconsciously (of course). For example, your friend Joe invited you to his party. You may asked him back: “Any girls tonight?”. He said yes. You asked him again: “Will Miley join too?”. “Of course, homie!”, he replied. And you accepted his invitation. The example I have just showed you is one of many situations that you may face everyday, in which you have to make your own decision on something. For that example, I can express it using the graph below: Simply enough, right? You may be wondering: Is this real that such simple algorithm can solve complicated classification problem? The answer is: Yes! To make it more clear to you, let’s consider a bigger one: Weather Temperature Humidity Injure Mood RUN clear &lt;10 &lt;70 slightly happy NO shower 20~30 &gt;80 fit stressed YES storm 10~20 &gt;80 fit happy NO shower 10~20 &gt;80 slightly stressed YES clear &gt;30 70~80 fit lazy YES storm 20~30 &gt;80 fit stressed NO clear &gt;30 70~80 severe happy NO clear 10~20 &lt;70 severe stressed NO shower 10~20 70~80 slightly happy NO shower &gt;30 &gt;80 fit happy YES storm 20~30 70~80 slightly happy NO clear 10~20 &lt;70 slightly happy ? Let’s see what we have here. Here’s the dataset I created for demonstration. I actually created based on my running experience, but some examples may sound weird to you, please ignore them for now, lol. Let’s say we want to predict whether I am gonna go for a run, based on some factors such as Weather, Temperature or even my Mood! And as you has learned so far, Weathers, Temperature, Humidity, Injure, Mood are just the features, and the YES or NO in the RUN column is the targets (or labels). Before going further into an appropriate explanation on how the algorithm works, I will first show you how we can solve the problem using the approach above. First, let’s randomly pick one feature from the features above. We will use that feature as the first condition, exactly the same way you asked “Any girls tonight?” above. To make it easy, let’s pick the first one, Weather. As you can see, Weather can be either clear or shower or even storm. We will then split our data into three groups according to the Weather feature. In terms of Decision Tree, each time we use a feature to split the data, we create one node, and each group is called one subset. Let’s first see how the clear Weather subset looks like: Weather Temperature Humidity Injure Mood RUN clear &lt;10 &lt;70 slightly happy NO clear &gt;30 70~80 fit lazy YES clear &gt;30 70~80 severe happy NO clear 10~20 &lt;70 severe stressed NO Let’s take a look at the RUN column. Obviously, just knowing the Weather is clear is not enough to decide whether to make a run or not. Just like the example above, you didn’t make your decision after just one question, right? So we will have to look for another feature, hoping it will help us decide. Let’s pick the Temperature feature. Now we can omit the Weather column (because it contains only clear), and just like what we did, we see that the Temperature feature can be either &lt;10 or 10~20 or &gt; 30, so we will now have three more subsets, let’s say, subsets of the subset above (where Weather is clear): Temperature Humidity Injure Mood RUN &lt;10 &lt;70 slightly happy NO Temperature Humidity Injure Mood RUN 10~20 &lt;70 severe stressed NO Temperature Humidity Injure Mood RUN &gt;30 70~80 fit lazy YES &gt;30 70~80 severe happy NO Let’s take a look at three new subsets above. The first two subsets are already clear, because the RUN column in each subset contains only one value. In terms of Decision Tree, we call the result in those subsets Leaf Nodes, and when they are now pure, which means that the output contains only one value. Meanwhile, the third subset still requires some further work. But it’s quite easy now. Let’s go ahead and use the Injure feature to create two new subsets: Humidity Injure Mood RUN 70~80 fit lazy YES Humidity Injure Mood RUN 70~80 severe happy NO Up to now, all the nodes in the clear Weather subset are clear, since there’s no any node in which the RUN column contains more than one value. We can now move on to the rest two subsets: the shower Weather subset and the storm Weather subset. By doing exactly the same way, we will get a result in the end, where all the subsets are clear. I have created a another graph for a better visualization: Using the graph above, we can now predict the value of the RUN column for our last row above: So, it’s likely that I’m gonna stay at home with my PlayStation 4 that night, lol. So that’s it. Just easy to understand like I said earlier, right? Of course, we have some kind of mathematical explanation for how Decision Tree actually does. For example, choosing which Feature to split in the beginning is not done randomly, but depends on some considerations. And each time we need to create new subsets from the parent subset, the process is repeated again. Why do we have to make things such complicated, you may ask. Technically say, Decision Tree is a greedy algorithm, which means that it’s likely to fall into local-minimum rather than the desired global-minimum, which means we may get an ugly result if we run out of luck. I will give you a simple explanation for Decision Tree algorithm below for ones who concern. You can skip it to jump directly to the Python Implementation because the explanation is just optional. Decision Tree: ID3 Algorithm So you decided to give Decision Tree’s algorithm a look. I really appreciate that. Knowing what is behind the scenes in somehow unrejectable, right? I won’t waste any other minute of your time. Let’s go straight into the algorithm. Actually, since Decision Tree was introduced quite long ago, the original algorithm has been revised and improved so many times, which the successor became more complex and robust than its predecessor. Among those, ID3 may probably be the best well-known algorithm. I think once you understand ID3, you can understand all its successors without problems. So, how does ID3 algorithm work? To make it as simple as possible, I list out all the steps below: 1/ Call the current dataset \\(S\\). We will then compute the Entropy \\(H(S)\\) on S as follow: , where \\(K\\) is the number of classes, \\(p(y_j)\\) is the proportion of number of elements of \\(y_j\\) class to the number of entire elements in output of \\(S\\): \\(H(S)\\) tell us how uncertain our dataset is. It ranges from \\(0\\) to \\(1\\), which \\(0\\) is the case when the output contains only one class (pure), whereas \\(1\\) is the most uncertain case. And in case you may ask, yes, that’s exactly the same as the entropy cost function that I showed you in Logistic Regression tutorial (except the \\(\\frac{1}{m}\\) term). As you already knew, the smaller the entropy function, the better classification result we can achive. 2/ Next, we will compute the Information Gain \\(IG(A,S)\\). Information Gain is computed seperately on each feature of the current dataset \\(S\\), whose value indicates how much the uncertainty in S was reduced after splitting \\(S\\) using feature \\(A\\). We can see that it looks like some kind of derivative, where we take the difference of the Entropy before and after splitting: , where \\(A\\) is the feature used for splitting, \\(n\\) is the possible number of values of \\(A\\), \\(p(t)\\) is the proportion of number of elements whose values is \\(t\\) to the number of all elements of feature \\(A\\). 3/ After compute all the Information Gains of all features, we will then split the current dataset \\(S\\) using the feature which has the highest Information Gain. 4/ Repeat from step 1 with the new current dataset until all nodes are clear. That’s it. But just skimming through the algorithm may not make any sense about how the algorithm works, right? So, let’s use the dataset above as an example. First, our current \\(S\\) would be the entire original table, right? We will compute its Entropy \\(H(S)\\). Look at the RUN column (which is our output), it has \\(4\\) YES and \\(7\\) NO over \\(11\\) examples, so its Entropy will be as follow: Next, we will compute Information Gain on each feature. Let’s first look at the Weather feature. It has three possible values: clear, shower and storm. clear Weather has \\(4\\) examples, so we have: \\(p(clear)=\\frac{4}{11}\\). In those \\(4\\) examples, we have \\(1\\) YES and \\(3\\) NO, so the Entropy of clear Weather will be: We now can do similarly to the rest two values to obtain values like below: Note that in the case of storm Weather, its output contains only NO value, so its Entropy will be \\(0\\), you don’t need to compute it by hand (and even if you do, you’ll soon realize that it is impossible because of the term \\(\\log_2(0)\\)!). And I would like to talk a little bit about the case when \\(H(t)=1\\), which I mentioned it as the most uncertain case. We can only obtain that value when the proportion of each class is equal to others’. In the case of our current dataset, if the number of YES is equal to the number of NO on the considered subset, then it’s easy to see that there is a big chance that it can’t be fully classified (that’s why we call it the most uncertain case). So now we can compute the Information Gain on the Weather feature as follow: Continue repeat this process with other features, you will likely end up with results like this: From the results above, IG on Weather has the highest value, so use Weather as a splitting condition will have the highest chance to reduce the uncertainty of dataset \\(S\\), and may lead to a good classification in the end. So that’s all I have to tell you about Decision Tree’s ID3 algorithm. Hopely this explanation somehow can help you have a deeper understanding about what was actually done behind the scenes. You may want to read more about its successors such as C4.5 or C5.0 algorithms. And you will find them not so hard to understand at all! Now, let’s jump to the implementation! Can’t wait no more, can you? Decision Tree with scikit-learn As I mentioned in the previous tutorials, the scikit-learn library comes bundled with everything you need to implement Machine Learning algorithms with ease. Furthermore, the library provides us many methods to generate data for learning purpose. And today I will use one of them to create a more complicated dataset, just to see how well Decision Tree can handle that. First, let’s import necessary modules as usual: import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification from sklearn.tree import DecisionTreeClassifier from sklearn.cross_validation import cross_val_score If you went through my last tutorial, you may now know a proper way to evaluate the performance of a Machine Learning Model using the cross_val_score method. And if you didn’t, you can always give it a look here: Cross Validation. Next, let’s using make_classification, a method provided by scikit-learn library to generate data for classification problems. X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, n_classes=2, n_clusters_per_class=1) Let’s plot the data we created to see what it looks like: X1_min, X1_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1 X2_min, X2_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1 plt.scatter(X[:, 0], X[:, 1], c=y, cmap='rainbow') axes = plt.gca() axes.set_xlabel('X1') axes.set_ylabel('X2') axes.set_xlim([X1_min, X1_max]) axes.set_ylim([X2_min, X2_max]) plt.show() In my case, the data looks like below. Note that your graph may be way different, since the make_classification method generates data in some unpredictable way. As you might notice, I also included Logistic Regression. That is because I want to compare the performance of the two classification algorithms that we have known so far. From now on, you will learn some more algorithms, and it’s always a good practice to compare between them, to see what algorithm work best for a particular problem. Let’s train our Model using one algorithm at a time, and print out the mean accuracy of each algorithm respectively. We will, of course, use the cross_val_score method for evaluation: clf = DecisionTreeClassifier() score = np.mean(cross_val_score(clf, X, y, cv=5)) print('Decision Tree: {}'.format(score)) clf = LogisticRegression() score = np.mean(cross_val_score(clf, X, y, cv=5)) print('Logistic Regression: {}'.format(score)) So let’s run our code a few times and see how the two algorithms perform. Note that you have to run the make_classification too, or you will likely get the same results! Here’s my results: Decision Tree: 0.97999 Logistic Regression: 0.99 Decision Tree: 0.98999 Logistic Regression: 0.99487 Decision Tree: 0.91 Logistic Regression: 0.935 As you see in the result above, Decision Tree’s performance is not any better (or even worse) than Logistic Regression’s. Let’s go ahead and try customizing the parameters of make_classification method (like increasing number of features, number of classes or number of samples, etc), you will likely realize that the two algorithms don’t have any significant difference. That is because the data generated by make_classification is linear, which means that Logistic Regression shouldn’t find any difficulty in fitting the data. Also note that we initialized both with default hyper-parameters (don’t mess with our \\(\\theta\\) parameters, I will have a post on tuning hyper-parameter soon!), which means that our algorithms can perform even better! So, let’s try another dataset. This time I will create a non-linear dataset, and see if these two algorithms can handle as well as they did above. Just like make_classification method for generating data for classification problems, scikit-learn library provides us some more methods to generate some particular dataset. One of those is make_circles method, which helps generate data with circular distribution. So let’s use it to create a new dataset, and see how it looks like: from sklearn.datasets import make_circles X, y = X, y = make_circles(n_samples=200, noise=0.2, factor=0.5) The code for drawing the graph is exactly same as above so I omit it for now. I highly recommend you to put everything in a python file, so you just need to modify a few lines, without having to re-run the code for drawing the graph! In my case, I have a graph like below: As you could see, our data is now no longer linearly distributed. I can’t wait to see how the two algorithms handle it. After I run the code three times, I got the result below: Decision Tree: 0.84 Logistic Regression: 0.455 Decision Tree: 0.86 Logistic Regression: 0.465 Decision Tree: 0.83 Logistic Regression: 0.45 Now what? The accuracy of Logistic Regression seems horrible, right? That was somehow predictable, because we have already talked in the previous tutorial that a straight line may not work well with non-linear dataset. But look at the results of Decision Tree, they are satisfying, I think, since we haven’t tuned any hyper-parameters of the algorithm. So now you have your first tool to deal with non-linear dataset! Decision Tree is simply incredible. The algorithm is easy to understand, which you can explain to your friends just like telling them a story, and yet the performance is impressive. Summary So today, we have just talked about Decision Tree and used it to solve a few classification problems. We have seen an outstanding performance on different kinds of data. It doesn’t matter whether our dataset is linear or non-linear, the algorithm can do the jobs just fine. That is definitely a great tool that you have just added to your Machine Learning toolbox! In the next post, I will tell you about another powerful classification algorithm, the one which nearly turned Neural Network into history (Neural Network was introduced quite very long ago, though). If you want to know what it is, stay updated for the next post! See you!",
    "tags": "machine-learning decision-tree classification essential Tutorial",
    "url": "/mahaveer0suthar.github.io/tutorial/Decision-Tree/"
  },{
    "title": "Machine Learning Part 7: Cross Validation",
    "text": "Hello there, so I’m here again to bring you the 7th post on my Machine Learning tutorial series. Today I will talk about Cross Validation, a term that you must know and mostly apply in all Machine Learning problems. So, to go straight to the main topic, what is Cross Validation anyway? And why must we know about it? Why must we apply it to our Machine Learning problems? The simple answer is: because of Overfitting. Maybe my answer is so straight, and somehow confusing. Okay, let me fix that. Remember in the previous post on Underfitting and Overfitting Problems, I talked about the two most common problems in Machine Learning? Underfitting is the problem when our algorithm failed to create a model that can fit the data, which means that our Model tends to give bad predictions even on the training dataset. Conversely, Overfitting is the problem which our Model fit training data so well that it tends to memorize all the features, therefore it performs badly on the data it has never seen. So, how do we at least foresee those problems before we can think about any solutions for them? As you might notice in the end of that post, I mentioned a little bit about splitting the dataset. Through the examples I showed you, it’s clear that we cannot just rely on the performance over the training data. We need a seperate dataset to use for performance evaluation, which assures that our Model has never seen before. That is the main idea of the term Cross Validation, which I will talk more further about today. I think that’s enough of talking. Let’s go down to business. Open your terminal, and get it started by initializing the data used in this post: from sklearn.linear_model import LogisticRegression import numpy as np import matplotlib.pyplot as plt X1_1 = np.linspace(0, 5, 100) X1_2 = np.linspace(0.2, 5.2, 100) X2_1 = np.random.rand(100) * (15 - 3 * X1_1) X2_2 = np.random.rand(100) * 3 * X1_2 + 15 - 3 * X1_2 X1 = np.append(X1_1, X1_2) X2 = np.append(X2_1, X2_2) X = np.concatenate((X1.reshape(len(X1), 1), X2.reshape(len(X2), 1)), axis=1) y = np.append(np.zeros(100), np.ones(100)) Let’s have a look at our data by plotting it onto the coordinate: plt.scatter(X[:, 0], X[:, 1], c=y, cmap='rainbow') plt.show() You can see that now we have data of two classes which are linearly seperated (which means you can seperate them by a straight line). I don’t want to use any complicated data today, because I have just shown you two basic learning algorithms. Furthermore, the thing I want you to focus is cross validation, not how to create a sophisticated Model. Now with the data all set. Now what to do next? As I mentioned before, we will split our original dataset into two portions: one for training and one for testing. The standard splitting ratio is 70% for training data and 30% for test data (in other places, you can see they use another ratio such as 6:4, etc). So let’s do it. Here I will use the first 140 elements as training data, and the rest as test data. X_train = X[:140, :] X_test = X[140:, :] y_train = y[:140] y_test = y[140:] Next, let’s create our Logistic Regression object and train the model using the split data above: clf = LogisticRegression() clf.fit(X_train, y_train) The training should take less than one second as usual. Now we have our Model learned. Let’s see how well it performs over the training dataset: clf.score(X_train, y_train) Using the training data to evaluate our Model gave me an accuracy of approximately 92.14%. Not bad, right? But I think you got experience now. You won’t speak a word until you see the performance over the test data, right? So let’s go on and grab the result on the test data: clf.score(X_test, y_test) In my case, I got a result of 40%. That was a real NIGHTMARE, wasn’t it. You may shout out loud. Why do we still get that frustrating result, despite the fact that we used cross validation? Well, let’s calm down and figure out why. Obviously, our current Model performs unacceptably bad. In real life projects, that will be a headache that we must stay calm and find where the problem is (though it may not be easy). There are a lot of reasons which cause a bad performance over the test data, but the main reason is the data itself. So it’s a good idea to take a look at the test data, maybe we will find something wrong with it. y_test array([ 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) Wow! Since this is a two-class classification problem, having a test dataset which only contains labels from one class is somehow unsual. To make it more clear, let’s print out the training data: y_train array([ 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) I’d better show the training data, I think. As you can see, the training data above was split unequally between two classes. It means that one class has much more data than the other. What does it affect the learning process? The more data you provide, the better it learns. So our Model is likely to learn the \\(0\\) class better than the \\(1\\) class. It is even worse that our test data contains only data of the \\(1\\) class, which resulted in a very bad predicting accuracy as you saw. So obviously, it’s necessary to split the data equally between classes, or we will achive some Model that we cannot trust. Randomly data shuffling One simple way to get our data equally split between classes is shuffling it. Shuffling data is a good practice, not only to make sure the data is split equally, but also make our learning algorithm work properly in other places such as stochastic gradient descent or something (you will know about it in later posts). Fortunately, scikit-learn library comes bundled with train_test_split method, which help us split the original data into training data and test data with a specific ratio, and of course, it makes sure that our data is well shuffled. So let’s import train_set_split method and use it to split our data: from sklearn.cross_validation import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) You can specify the random_state parameter to get a specific random sample. Here I omitted it so that I got a different set of data each time I call the train_test_split method. Let’s go ahead and make sure our data is now split equally between classes: len(np.where(y_test==0)[0]) 32 len(np.where(y_test==1)[0]) 28 It’s not necessary that the number of data of each class is exactly the same as the other’s. So in my case, \\(33:27\\) is a reasonable ratio to me.If you find your data is not okay (let’s say \\(35:25\\)), then just re-run the method to get a better split. Simple enough, right? Next, let’s train the Model using our new dataset: clf.fit(X_train, y_train) Check the result again: clf.score(X_train, y_train) 0.9357142857142857 clf.score(X_test, y_test) 0.9166666666666663 Obviously, that’s just the result that we longed for. You may now realize that, it’s not just the algorithm which is important, the data itself does matter. With our dataset properly prepared, we can obtain a better result without tuning any learning parameters. I’m now so excited. Let’s do it one more time: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) clf.fit(X_train, y_train) clf.score(X_train, y_train) 0.94285714285714284 clf.score(X_test, y_test) 0.84999999999999999 The result varied a little bit. But it does make me concern. Is there any chance that we missed something? Because we randomly split our data, what if we have been lucky to have a “good” split? Obviously, we cannot tell anything unless we have some way to cover all the cases, which means that all the data was split into test data at least once. Well, we will do that with a for loop. Since we are splitting with the ratio 7:3, we will call the train_test_split method four times in order to cover all the cases: scores = [] for i in range(4): X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) clf.fit(X_train, y_train) scores.append(clf.score(X_test, y_test)) print(scores) [0.93333333333333335, 0.96666666666666667, 0.83333333333333337, 0.98333333333333328] Doing this way, we can have a better visualization of testing accuracy over the whole testing data. Obviously, the results varied a little, but that was something we could predict. And since there’s no significantly difference between those numbers, we can now feel relieved a little, right? Here comes another question. Will the train_test_split perform well with multi-class classification problem? Let’s say now we have five classes, let’s see how well it can do: y = np.append(np.zeros(40), np.ones(40)) y = np.append(y,2 * np.ones(40)) y = np.append(y,3 * np.ones(40)) y = np.append(y,4 * np.ones(40)) Here we got five classes from \\(0\\) to \\(4\\), each class has 40 examples. Now we will use the train_test_split to split our data. Let’s see if it can split equally between five classes: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) len(np.where(y_test==0)[0]) 11 len(np.where(y_test==1)[0]) 5 len(np.where(y_test==2)[0]) 20 len(np.where(y_test==3)[0]) 8 len(np.where(y_test==4)[0]) 16 You might not expect it, but the performance was poor. Now we may wonder it the scikit-learn provides us some other cool module instead. Yeah, it does! StratifiedKFold and cross_val_score scikit-learn library provides us a module called StratifiedKFold. As its name is self-explained, it will divide the original data into f folds, make sure that each fold contains the same amount of data from all classes. from sklearn.cross_validation import cross_val_score from sklearn.cross_validation import StratifiedKFold Let’s see how our data is split using StratifiedKFold. Since we have 200 examples, with 40 examples each class, we will set n_folds equal to 5: k = StratifiedKFold(y, n_folds=5) for i, j in k: print(y[j]) Here’s the first line of the output I got: [ 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 4. 4. 4. 4. 4. 4. 4. 4.] As you can see, using StratifiedKFold we got our data split equally between five classes. With the data well split, we can be sure that our Model will likely learn much better. scikit-learn library also provides us a method called cross_val_score. What it does is first, splitting our Model using StratifiedKFold, and second, looping through all k folds then compute the corresponding accuracy score, which means that we don’t have to write the loop ourselves anymore. So great, right? All we have to do is to pass our Model, our original \\(X\\), our original \\(y\\), number of folds we want it to split through the cv parameter. And it does all the rest for us! Note that before that, we must restore our \\(y\\) vector, since I used it above for demonstration. My bad, sorry! y = np.append(np.zeros(100), np.ones(100)) scores = cross_val_score(clf, X, y, cv=4) print(scores) [ 0.78 0.96 0.98 0.9] The result we got here is not much different than the one above, since this is a two-class classification problem, using StratifiedKFold is not likely to improve that much. But it is recommended using StratifiedKFold (through the cross_val_score method), rather than use the train_test_split and then write the loop yourselves. Summary So, today I have told you about cross validation. Now you know the right way to split the data for training and testing purposes. You also know the need of shuffling the dataset before splitting. And after all, you know how to efficiently split the data using StratifiedKFold and use cross_val_score method to help us perform testing over the whole dataset. With that, you will know have a valuable tool to evaluate your trained Model, and can tell whether it tends to overfit or not. In future post, after I show you some more powerful algorithms, we will continue with some techniques to prevent overfitting. Until then, stay updated and I will be back soon! See you!",
    "tags": "machine-learning cross validation data splitting overfitting dianosic stratifiedkfold cross_val_score train_test_split Tutorial",
    "url": "/mahaveer0suthar.github.io/tutorial/Cross-Validation/"
  },{
    "title": "Machine Learning Part 6: Logistic Regression",
    "text": "Hello there, I am back with you today. In the 6th post on Machine Learning tutorials series, I will tell you about Logistic Regression, a very important and must-know algorithm. Before I go any further, there is one thing I want you to be clear at first. The algorithm’s name, Logistic Regression, is somehow confusing a little bit, but we are not dealing with a regression problem, but a classification problem. But what is the difference between regression problems and classification problems? - You may ask. I am not a statistical expert, and you may not want any detailed academic explanations, so I will make it simply like below. A regression problem is where you have a continuous dataset of label \\(y\\), and your goal is to make predictions for unlabeled data. Because \\(y\\) is continuous, it can take any value between a specific range. Concretely, if our problem’s output range is \\(1 ~ 10\\), then \\(y\\) can be any value between that range, such as \\(1.2, 2.5\\) or even \\(4.23424324\\), etc. A regression problem’s graph will mostly look like the graph I showed you in Linear Regression tutorial: A classification problem, as you may guess, is where our labels \\(y\\) can only take a particular value, or we can say that \\(y\\) is a discrete dataset. For example, if we want to solve a spam mail detecting problem, obviously we will have only two labels which is either spam or non-spam. Another example, you are a magician who is trying to guess the suit of a randomly picked card, so you only have four choices among Hearts, Diamonds, Clubs and Spades, right? So we are now dealing with a different problem from the one we did before. Our graph will now look like this: Okay, so I hope you know can see the difference between a regression problem and a classification problem. Let’s see how we will do to deal with a classification problem. To make it easily for you to understand, it is better that we should now consider a two-class classification problem like above. Activation Function You still remember how Machine Learning actually works? It is important for any Machine Learning algorithms to have a way to compute the Predictions from the feature data \\(X\\) which we called Activation Functions. In the case of Linear Regression, the activation function is just as simple as below: As you can see, the function above is suitable for regression problems, as it doesn’t apply any restriction onto the output. Obviously, this function won’t work well with our classification problem. But we can also see that, Logistic Regression is somehow similar to Linear Regression (that is why there is Regression in its name), so we may be able to solve the Logistic Regression if we somehow find a way to restrict the output value of the function above. One simple way we can think about is, to use a threshold value. The idea is like below: And we will obtain a graph like this: Seems OK, huh? At least we can ensure that the activation function now only outputs \\(0\\) or \\(1\\). But in fact, using a threshold directly on regression’s activation function is not a good idea at all. The reason is, now we have a sharp change in our graph, which means that the value of \\(y\\) will change immediately in an unpredictable way, which results in a bad Model in the end. So now we know what to do next. We need an activation function which not only restricts the output value but also contains no sharp change. That shouldn’t be a big problem, in fact, we have quite a lot of functions which can satisfy both conditions. And the one which is mostly used is Sigmoid function, which has the form as below: And the graph of sigmoid function looks like this: Perfect, right? That is exactly what we need. So how are we supposed to apply this to our case. It’s very simple, we just do like below: Next, I want to talk a little bit about the output of the function about, \\(h_\\theta(X)\\). \\(h_\\theta(X)\\) is interpreted as the probability that \\(y=1\\) on input \\(X\\), which can be expressed in mathematical terms like this: And obviously, since we always have: So we can also rewrite the probability that \\(y=0\\) like this: The output of activation function is the probability that \\(y = 1\\). Why did I emphasize that? Do you still remember that I left one mystery unrevealed in my very first post about What is Machine Learning?. I labeled two classes Dog and Not-a-dog, not Dog and Cat, or Dog and Bird either. And the reason is clear now, I think. In a classification problem, the output is usually interpreted as the probability that the tested object belongs to a particular class. Obviously, if one object is not a Dog, then I would rather say that it is Not-a-dog than say that it is a Cat or it is a Bird, right? Although in real world projects, let’s say we have ten classes, then each class will be labeled as a specific name, rather than an obscure name like Not-something, but that’s OK in case you already had some basic understanding about Machine Learning (I will talk about multiclass classification later). And I thought that it would be better to name two classes Dog and Not-a-dog in the very first example on Machine Learning, so that you would make no misunderstanding about what is actually outputted from a Machine Learning Model. So we have done with sigmoid function, the function which we chose as our Activation Function. To recall a little, sigmoid function is a great choice here because it can satisfy both conditions: restricting the output’s values and ensuring there is no sharp changes in graph. Cost function After choosing the activation function, let’s move to our next target: the Cost Function. As you may remember, the cost function evaluate our Model performance based on the difference between its Predictions and the actual Labels. In the case of Linear Regression, our cost function looks like this: Can we use the same cost function in Logistic Regression? The answer is: definitely NO. You got the right to ask me why. Well, in the learning process, what the computer tries to do is to minimize the cost function. Therefore, our cost function must be some kind of convex functions so that we can find the minimum by using Gradient Descent. In the case of Logistic Regression, if we use the same cost function like above, we will end up with a non-convex function. And for sure, the computer has no way to find the minimum. So we are likely to find a new form of cost function which can both evaluate our Model’s performance and tends to converge to some minimum. I don’t want to talk deeper into how they found the appropriate cost function for Logistic Regression because it requires a lot of mathematical explanations. To make it as easily to understand as possible, you can see that our activation function contains an exponential element \\(e^{-z}\\), and one way to linearize that kind of function is to use logarithm. That is why the cost function for Logistic Regression was defined like below, and called the log-likelyhood cost function or the cross-entropy cost function: Let’s talk a little bit about the cost function above. First, note that now we divide the sum by \\(\\frac{1}{m}\\), because our new cost function is no longer a quadratic function. And don’t forget the minus sign, either! It’s very easy to explain why there is minus sign there. The logarithm of a number whose value is from \\(0\\) to \\(1\\) is a minus number, so adding a minus sign will make sure our cost function will always greater than or equal to \\(0\\). Next, you can see that our new defined cost function has two seperate part: \\(y^{(i)}\\log(h_\\theta(X^{(i)}))\\) and \\((1 - y^{(i)})\\log(1 - h_\\theta(X^{(i)}))\\). Since the Label \\(y\\) can only be \\(0\\) or \\(1\\), so one of the two terms above will be \\(0\\). So we have a cost function which can cover both two cases: \\(y=0\\) and \\(y=1\\). Furthermore, our new cost function can also accumulate the output errors in each case properly, as explained above. Gradient Descent So I have just talked about the cost function used in Logistic Regression problems. After we have a cost function, we will compute Gradien Descent. Do you still remember what Gradien Descent is? We need to compute Gradient Descent in order to update the parameter \\(\\theta\\) (After assuring our cost function is convex, we need a way to go downhill, right?). So, let’s compute Gradient Descent. Our new cost function seems very complicate this time, which you may think that it would take a day to compute all its partial derivatives. Don’t worry, things are not that bad. In fact, computing the log-likelihood cost function’s partial derivatives is very easy, all you have to do is using the chain rule which I mentioned before in earlier post. Writing it our here will make this post long and boring, so I leave it for you, lol. Here, I just want to show you the result. And it may make you surprised. Yeah, it looks just like what we had with Linear Regression: For weights (\\( \\theta_1, \\ldots, \\theta_n\\)) For bias (\\( \\theta_0 \\)) As I mentioned above, you can compute the partial derivatives yourselves (I highly recommend you to do so, though). And with a little patience, I’m quite sure that you will have the same result. So now you know everything you need to know about Logistic Regression: the sigmoid function, the log-likelihood cost function and its gradient descent. Note that not only Linear Regression and Logistic Regreesion, knowing these three terms also help you understand and use any other Machine Learning algorithms as well (even those complicated algorithms such as Neural Network!). Decision Boundary I will be more than happy if you are still keeping my page open. I really appreciate your attitude and persistence, too. And we will do something more interesting right now: programming! But before getting our hands dirty. Let’s consider the figure below: I have just prepared some data. This time we will work with two-feature dataset. In practice, it’s normal that our data has a great deal of features (can be up to thousands in case of image data), so working with one-feature data makes it irritating when dealing with some real problems. On the other hand, working with many-feature data when learning (let’s say, data which has more than three features) will make you unable to visualize the result. So, it’s a good idea to use data with two features (or three features) when learning. So as you saw in the figure above, our data has two features: \\(X_1\\) and \\(X_2\\) with the label \\(y=0\\) or \\(y=1\\). Since we are dealing with a classification problem, where our label can take a particular value only (in this case \\(0\\) of \\(1\\), we don’t need to plot the label \\(y\\). It would be better if we just plot a graph of \\(X_1\\) and \\(X_2\\) only like below: So what about \\(y\\)? We can use colors to indicate different values of \\(y\\). Doing this way, we can have a better visualization of our data, and make it possible to visualize three-feature data as well. With the figure above, we can see that our data is linearly distributed, which means we can seperate them out by a straight line. In a classification problem, that straight line is called Decision Boundary. Imagine we have drawn a decision boundary to the graph above, then to every new point, if it lies above the decision boundary, then we will label it with blue triangle. Conversely, if the new point lies below the decision boundary, it will become a red circle. Simple, right? A picture is worth a thousand words! But what is exactly behind the decision boundary? Just to recall you, the value of the activation function \\(h_\\theta(X)\\) is the probability that \\(y=1\\), we will now define a new variable to hold the prediction made by our Model like below: But the activation function is a sigmoid function, which means that: So we can have the relation between the prediction \\(y_{predict}\\) and \\(\\theta^TX\\) like this: Now what’s next? How the hell can all of this lead to a decision boundary we need? Well, let’s consider the example below: Let’s pick some random parameters for our activation function, like \\(\\theta_0 = -1\\), \\(\\theta_1 = 2\\) and \\(\\theta_2 = 3\\). So we will have \\(z = -1 + 2X_1 + 3X_2\\). Let’s compute the prediction \\(y_{predict}\\): Let’s visualize the result above: As you can see, with the constraint above, we can draw a straight line, in this case: the \\(2X_1 + 3X_2 - 1 = 0\\) line and have it seperate our hyperplane into two parts. The area above the line, which have \\(2X_1 + 3X_2 - 1 \\ge 0\\) is where our prediction \\(y_{predict} = 1\\) and the other one is where \\(y_{predict} = 0\\). So I hope you now understand what decision boundary is. It is a very important concept in classification problems. In the near future, you will see that the decision boundary is not necessarily a straight line. Depending on the algorithm you use, you can achieve a curve decision boundary (as I already shoed you, a curve line is fit the data much more better). Now, we are ready to code! Let’s first import all the necessary modules: import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LogisticRegression Next, we will create the data to be trained: x1_1 = np.linspace(0, 2, 100) x1_2 = np.linspace(0.1, 2.1, 100) x2_1 = np.abs(np.random.rand(100))*4 x2_2 = np.abs(np.random.rand(100))*4 + 4 y1 = np.zeros(100) y2 = np.ones(100) x = np.ones((200, 2)) x[:, 0] = np.append(x1_1, x1_2) x[:, 1] = np.append(x2_1, x2_2) y = np.append(y1, y2) If you plot the data above, you will get a graph that looks nearly the same as the one above. Now let’s train the Logistic Regression model: clf = LogisticRegression() clf.fit(x, y) It should take no longer than one second to complete training. Let’s see how well our Model performs on the training dataset: clf.score(x, y) 0.98 Note that your result may vary, since the values of \\(X_2\\) were randomly initialized. I will omit the overfitting problem in this tutorial for simplicity. And similar to Linear Regression, after finishing training, the Logistic Regression object now contains the final parameters in coef_ and intercept_ attributes, we will use them to draw our decision boundary: t = np.linspace(0, 2, 100) y_pred = (-clf.intercept_ - clf.coef_[0][0]*t) / clf.coef_[0][1] X1_min, X1_max = X[:, 0].min(), X[:, 0].max() X2_min, X2_max = X[:, 1].min(), X[:, 1].max() plt.scatter(X[:, 0], X[:, 1], c=y, cmap='rainbow') plt.plot(t, y_pred, 'g-') axes = plt.gca() axes.set_xlabel('X1') axes.set_ylabel('X2') axes.set_xlim([X1_min, X1_max]) axes.set_ylim([X2_min, X2_max]) plt.show() We will obtain a decision boundary like below, note that your result may be different from mine: As you can see, the final decision boundary somehow can seperate the blue triangles and the red circles pretty well. And obviously, we see that there is much room for improvement, but I will leave it for the later post. After typing some codes and visualizing the result, I hope you now know how to implement your code to use the Logistic Regression algorithm. Summary So today, we have talked about Logistic Regression. We talked about the difference between a regression problem and a classification problem. We talked about the activation function, the cost function used in Logistic Regression and how to compute its gradient descent as well. In the next post, I will continue with Cross Validation, another very important technique that you must know to deal with Overfitting problem, as you will use that technique nearly in every Machine Learning problem you may face in the future! So stay updated, and I will be back with you soon!",
    "tags": "machine-learning logistic-regression classification essential Tutorial",
    "url": "/mahaveer0suthar.github.io/tutorial/Logistic-Regression/"
  },{
    "title": "Compiling and Running Faster R-CNN on Ubuntu (CPU Mode)",
    "text": "So today I am gonna tell you about how to compile and run Faster R-CNN on Ubuntu in CPU Mode. But there is a big chance that many of you may ask: What the hell is Faster R-CNN? In my previous posts, I have done a project Real-time Object Recognition (you can find it here: Real-time Object Recognition). The result received was pretty good, but as you might notice, that it got a problem (a big problem). The problem is that the trained Model could recognize one object per frame. So if I want it to recognize two or more objects (and even tell me where each of them locates), it will raise the white flag! So it came to my next mission that I have to find a way to deal with Object Detection. Of course I knew some of them before, but what I wanted is something which applied Convolutional Neural Network. Among some great papers people had done out there, I chose Faster R-CNN. In today’s post, I have no intention to talk about how Faster R-CNN works. I just leave it for a future post, when I finish my task. Today I am just going to talk about how to compile and run Faster R-CNN on Ubuntu - in CPU Mode. And if you ever wonder why I am doing this post although I can find a great deal of tutorials on the net, the answer is: just like Caffe in CPU Mode, compiling Faster R-CNN was hard like hell too! Faster R-CNN was originally implemented in MATLAB, but they also provided a Python reimplementation code (phew!). So let’s grab it from GitHub: git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git Just make sure that you didn’t forget the –recursive flag. After the download completes, jump to the lib folder: cd py-faster-rcnn/lib Here we are compiling Faster R-CNN for CPU Mode, so we have to make several changes. Let me guide you through this tough guy. First, let’ open the setup.py file, then comment out all the lines below: From this: CUDA = locate_cuda() self.set_executable('compiler_so', CUDA['nvcc']) Extension('nms.gpu_nms', ['nms/nms_kernel.cu', 'nms/gpu_nms.pyx'], library_dirs=[CUDA['lib64']], libraries=['cudart'], language='c++', runtime_library_dirs=[CUDA['lib64']], # this syntax is specific to this build system # we're only going to use certain compiler args with nvcc and not with # gcc the implementation of this trick is in customize_compiler() below extra_compile_args={'gcc': [\"-Wno-unused-function\"], 'nvcc': ['-arch=sm_35', '--ptxas-options=-v', '-c', '--compiler-options', \"'-fPIC'\"]}, include_dirs = [numpy_include, CUDA['include']] ), To this: #CUDA = locate_cuda() #self.set_executable('compiler_so', CUDA['nvcc']) #Extension('nms.gpu_nms', # ['nms/nms_kernel.cu', 'nms/gpu_nms.pyx'], # library_dirs=[CUDA['lib64']], # libraries=['cudart'], # language='c++', # runtime_library_dirs=[CUDA['lib64']], # this syntax is specific to this build system # we're only going to use certain compiler args with nvcc and not with # gcc the implementation of this trick is in customize_compiler() below # extra_compile_args={'gcc': [\"-Wno-unused-function\"], # 'nvcc': ['-arch=sm_35', # '--ptxas-options=-v', # '-c', # '--compiler-options', # \"'-fPIC'\"]}, # include_dirs = [numpy_include, CUDA['include']] #), After saving the file, now we can compile the lib folder. Let’s execute the command below: make You should now go through without any error. If some error still occurs, make sure that you didn’t miss any line above. Next, we will compile caffe and pycaffe. You may remember that I made a post about how to install caffe on Ubuntu in CPU Mode, so why don’t we just use the results we got, instead of doing the same thing over again? Well, that is because they had made some changes to the caffe original codes (implementing some necessary classes, methods, …), we have to compile a new set in order to run their codes. But don’t worry, there are things which we can re-use. I will tell you below. Now let’s jump to the caffe-fast-rcnn folder: cd ../caffe-fast-rcnn Similar to what we did with caffe before, this time we are likely to make some change to the Makefile files. But actually, they are just files which tell the compile how to compile things (like where to look for include files, library files, etc), so we can re-use our old Makefile which we have already modified. In case you haven’t had a look at my old post yet, you can find it here: Installing Caffe on Ubuntu. So what we are gonna do is going to where we placed caffe, copy the Makefile and Makefile.config files, and paste them into the caffe-fast-rcnn folder. But even after we did that, there is still one tiny change we have to make, let’s open the Makefile.config file, uncomment the line below: From this: # WITH_PYTHON_LAYER := 1 To this WITH_PYTHON_LAYER := 1 And we have done. Now go ahead and make it: make &amp;&amp; make pycaffe This time should run smoothly too. Congratulations, you have successfully compiled caffe for Faster R-CNN. It seems like we have just made some tiny changes up to now. But don’t be complacent too soon, although we have sucessfully compiled caffe for Faster R-CNN, we still cannot run their demo code now. There are some other places we have to modify too. cd .. ./data/scripts/fetch_faster_rcnn_models.sh The command above will download the pre-trained model in order to run the demo code. After the download finishes, let’s apply the changes below: File: lib/fast_rcnn/nms_wrapper.py, from this: from nms.gpu_nms import gpu_nms To this: # from nms.gpu_nms import gpu_nms File: lib/fast_rcnn/config.py, from this: __C.USE_GPU_NMS = True To this: __C.USE_GPU_NMS = False That is it. We should now be able to run the demo project now. So let’s do it: ./tools/demo.py --cpu Note that you must provide the –cpu flag to tell it to run in CPU Mode. It will take a while to run, because it will process all images before outputting the results (in my case it took approximately 22s per image). The output should look like this: So, I have shown you how to compile and run the demo code of Faster R-CNN. It was not so hard especially if you experienced the caffe installation before. I will be glad if you find this post helpful. And even if you followed all the instructions above but you still couldn’t make it through the frustrating errors, don’t hesitate to leave me a comment below, I will help you as soon as possible, I promise! And as I promised above, I will do some posts related to my current work using Faster R-CNN, and I will tell you more about Faster R-CNN too. So stay tuned! I’ll be back. Reference: Faster R-CNN GitHub page: https://github.com/rbgirshick/py-faster-rcnn",
    "tags": "machine-learning faster r-cnn caffe compile cpu mode essential Project",
    "url": "/mahaveer0suthar.github.io/project/Running-Faster-RCNN-Ubuntu/"
  },{
    "title": "Machine Learning Part 5: Underfitting and Overfitting Problems",
    "text": "Here we are again, in the fifth post of Machine Learning tutorial series. Today I will talk about two common problems you may face in Machine Learning: Underfitting and Overfitting. Wait! There is something wrong, isn’t it? - You may wonder… Of course I remember promising you in the previous post, that today I will dig deeper into Linear Regression, and together we will do some coding. Actually, I intended to name today’s post “Implementing Linear Regression” or something, but I soon realized that it would be inappropriate. Good news is today’s post will mainly focus on implementation of Linear Regression, and what we can do to improve the quality of the Model. By doing that, I am actually leading you to go into some concept which is more general, and can be applied not only in Linear Regression, but every spot which Machine Learning takes place. That is enough of talking. First, let’s get our hands dirty. If you went through my previous post, you would now have everything set up. But if you didn’t, you might want to take a look at it here: Setting Up Python Environment. Implementing Linear Regression Open Terminal and go into Python console mode: python Now let’s import sklearn module for Linear Regression. sklearn is a shortname of scikit-learn, a great Python library for Machine Learning. from sklearn.linear_model import LinearRegression import numpy as np import matplotlib.pyplot as plt We are not using only LinearRegression. We will work with arrays, so here I also imported numpy for dealing with arrays. We will also draw some graphs to visualize the data, so that’s why I imported pyplot, a great module for graph drawing. Remember the data I used in the previous post on Linear Regression? I will show it right below for you: X y 1 7 2 8 3 7 4 13 5 16 6 15 7 19 8 23 9 18 10 21 Now let’s use that to prepare our training data: X = np.arange(1, 11).reshape(10, 1) y = np.array([7, 8, 7, 13, 16, 15, 19, 23, 18, 21]).reshape(10, 1) Nearly every Machine Learning library requires data to be formatted in the way which each row is one training example (or testing example), and each column represents one feature’s data. So we have to reshape our data accordingly. Now let’s plot our training data. You will receive the same figure with the one in the previous post: plt.plot(X, y, 'ro') plt.show() Next, let’s initialize the Linear Regression model: model = LinearRegression() Then we will train our Model, using the training data above. You can do that by simply calling the fit function, which takes feature matrix \\(X\\) and label vector \\(y\\) as parameters: model.fit(X, y) Our training data is quite simple, so the learning process finished so fast as if it never happened. All the change during training (like weights and bias), was stored in the model object. Let’s see what we got: model.coef_ array([[ 1.77575758]]) model.intercept_ array([ 4.93333333]) Obviously, you can get more information through other attributes of model object, but now we will only focus on coef_, which stores the weight parameter, and intercept_, which stores the bias parameter. Next, let’s compute the prediction vector a, using the obtained weight and bias: a = model.coef_ * X + model.intercept_ Now let’s draw all \\(X\\), \\(y\\) and \\(a\\) on the same plot. Here we got a straight line, which fit the data better than what we did before (which is easy to understand, since we only went through 4 iterations). plt.plot(X, y, 'ro', X, a) axes = plt.gca() axes.set_ylim([0, 30]) plt.show() So simple, right? Just a few lines of code, we have just prepared our training data, trained our Model, and visualized the result we got! Yeah, scikit-learn helps us do all the heavy things. In later posts, you will see that it can even handle more complicated jobs. Improving the performance of Linear Regression Obviously, we can see that the straight line above fits pretty well, but not good enough. And we need a more suitable approach. But first, let’s evaluate how well the Model is performing numerically, by computing the accuracy over the training data: print(model.score(X, y)) 0.84988070842366825 We cannot always evaluate something just by seeing it, right? We need something which is more concrete, yeah, a number. By looking at numbers, we will have a better look, and easily compare different things. scikit-learn provides us the score function, whose parameters are similar to the fit function. And you can see that, our Model now has the accuracy of 85% over the training data. Commonly, we demand a higher accuracy, let’s say 90% or 95%. So by looking at the current accuracy, we can tell that our Model is not performing as we are expecting. So let’s think about an improvement. But how can we do that? Remember I told you about Features in the first Post? Features are something we use to distinguish one object from others. So obviously, if we have more Features, then we will likely have a better fit model, since it can receive more necessary information for training. But how we can acquire more Features? Polynomial Features The easiest way to add more Features, is to computing polynomial features from the provided features. It means that if we have \\(X\\), then we can use \\(X^2\\), \\(X^3\\), etc as additional features. So let’s use this approach and see if we can improve the current Model. First, we have to modify our \\(X\\) matrix by adding \\(X^2\\): X = np_c[X, X**2] X array([[ 1, 1], [ 2, 4], [ 3, 9], [ 4, 16], [ 5, 25], [ 6, 36], [ 7, 49], [ 8, 64], [ 9, 81], [ 10, 100]]) Similar to previous step, let’s train our new Model, then compute the prediction vector \\(a\\): model.fit(X, y) x = np.arange(1, 11, 0.1) x = np.c_[x, x**2] a = np.dot(X, model.coef_.transpose()) + model.intercept_ Mathematically, we will now have \\(a=\\theta_0 + \\theta_1X + \\theta_2X^2\\). Note that now we have more complicated matrix X, so we will have to use the dot function. An error will occur if we just use the multiply operator like above. I also created a new \\(x\\) variable, which ranges from 1 to 10, but with 0.1 step. Use the new \\(x\\) to compute \\(a\\) will result in a smoother graph of \\(a\\), since \\(a\\) is no longer a straight line anymore. Now let’s plot things out and see what we got with new feature matrix: plt.plot(X[:, 0], y, 'ro', x[:, 0], a) plt.show() As you can see, now we obtain a curved line, which seems to fit our training data much better. To be more concrete, let’s use the score function: model.score(X, y) 0.87215506914951546 You see that? Now we got a new accuracy of 87%, which is a huge improvement right? At this point, you may think that we can improve it a lot more by continuing to add more polynomial features to it. Well, don’t guess. Let’s just do it. This time we will add up to degree 9. X = np.arange(1, 11) X = np.c_[X, X**2, X**3, X**4, X**5, X**6, X**7, X**8, X**9] x = np.arange(1, 11, 0.1) x = np.c_[x, x**2, x**3, x**4, x**5, x**6, x**7, x**8, x**9] model.fit(X, y) a = np.dot(x, model.coef_.transpose()) + model.intercept_ plt.plot(X[:, 0], y, 'ro', x[:, 0], a) axes = plt.gca() axes.set_ylim([0, 30]) plt.show() Now we just obtained a new curve which fit our training data perfectly. Let’s use the score function again to get an exact number: model.score(X, y) 0.99992550472904074 Wow, let’s see what we have here, an accuracy of 100%. This is real magic, you may think. But that is just where the tragic begins… OVERFITTING &amp; UNDERFITTING Now let imagine our data has total 15 examples, and I just showed you the first 10. I will reveal the last 5 examples like below: X y 11 24 12 23 13 22 14 26 15 22 So actually our data will look like this: X = np.arange(1, 16) y = np.append(y, [24, 23, 22, 26, 22]) plt.plot(X, y, 'ro') plt.show() Let’s see what happens if we use the Model obtained from degree 9 polynomial features: plt.plot(X, y, 'ro', x[:, 0], a) axes = plt.gca() axes.set_ylim([0, 30]) plt.show() Do you see what I am seeing? What a tragic! It doesn’t seem to fit the new data at all! We don’t even feel the need of computing the accuracy on the new data! So what the hell this is all about? As I told you before, in the first post, that we only provided a fixed set of training data, and the Model will have to deal with new data which it has never seen before. New data, which may vary in unpredictable way in real life, penalized our trained Model this time! In Machine Learning term, we call it OVERFITTING problem (or High Variance). Overfitting, as the name is self-explained itself, means that the Model fits the data very well when we prodived a set of data containing a lot of features. We can see that the Model tends to memorize the data, rather than to learn from it, which makes it unable to predict the new data. In contrast, what will happen if we use just one feature like we did in the beginning (or we can say that we provided a set of data which is poorly informative)? You have already seen that it resulted in a very low accuracy, which is not what we expected, either. We call this problem UNDERFITTING (or High Bias). Overfitting &amp; Underfitting, in both cases, are something that we try to avoid. And you will mostly face these problems all the time you work with Machine Learning. Of course, there are many ways to deal with them, but I will leave all the details for a future post. This time I will tell you the simplest way, which can be seen as a “must-do” in the very first step of any Machine Learning problem. Splitting dataset for training and testing The first thing to do to prevent the problems above, is always splitting the dataset into training data and testing data. Never just count on the accuracy on training data! Why? Because even though we obtained a high accuracy, it does not mean that our Model is doing a good job. Conversely, we need to watch out for Overfitting problem. By splitting our dataset into two seperate parts, we will use one part for training, and the other for evaluating the trained Model. Because we evaluate the performance on a separate data, we can know if our Model can work well with new data that it has never seen. And we can somehow tell whether our Model has Overfitting problem or not. Underfitting, on the other hand, can easily be discovered just by looking at the accuracy over the training data, because if our Model has Underfitting problem, then it will perform poorly on both dataset. Finally, we will pick the Model which has the highest accuracy on the testing data. With the approach I have shown you, let’s decide which Model to choose among three models above. We will use first ten examples as training data, and the last five examples for testing. Model 1 X = np.arange(1, 16).reshape(15, 1) model.fit(X[:10], y[:10]) model.score(X[10:], y[10:]) -12.653810835629017 a = np.dot(X, model.coef_.transpose()) + model.intercept_ plt.plot(X, y, 'ro', X, a) plt.show() Model 2 X = np.arange(1, 16).reshape(15, 1) X = np.c_[X, X**2] x = np.arange(1, 16, 0.1) x = np.c_[x, x**2] model.fit(X[:10], y[:10]) model.score(X[10:], y[10:]) -0.51814820280729812 a = np.dot(x, model.coef_.transpose()) + model.intercept_ plt.plot(X[:, 0], y, 'ro', x[:, 0], a) axes = plt.gca() axes.set_ylim([0, 30]) plt.show() Model 3 X = np.arange(1, 16).reshape(15, 1) X = np.c_[X, X**2, X**3, X**4, X**5, X**6, X**7, X**8, X**9] x = np.arange(1, 16, 0.1) x = np.c_[x, x**2, x**3, x**4, x**5, x**6, x**7, x**8, x**9] model.fit(X[:10], y[:10]) model.score(X[10:], y[10:]) -384608869887696.81 a = np.dot(x, model.coef_.transpose()) + model.intercept_ plt.plot(X[:, 0], y, 'ro', x[:, 0], a) axes = plt.gca() axes.set_ylim([0, 30]) plt.show() As you can see, both by visualizing and by looking at the accuracy. Our first model is too simple, which didn’t fit our data well. This is an example of Underfitting problem. In contrast, our third model is way too complicated, which performed very well on training data, but failed to fit the testing data. This is what we called Overfitting problem. The second model may not fit as well as the third model, but it is the one that actually learned, which results in good performance over the testing data. And we can somehow say that, it will also predict well with any other data which it has never seen during training. Conclusion So today, through implementing Linear Regression, I led you through the most common problems you may face when working with Machine Learning, which are Underfitting and Overfitting. I also showed you the easiest way to avoid those problems, which is always splitting the dataset into two parts: one for training purpose, and one for testing. Hope you find today’s post helpful and you now put a further step into Machine Learning world, I think. There is no stopping as you has gone this far. In the next post, I will continue with Logistic Regression, which is an extremely important algorithm that you must understand, because it is the key which leads you to the most powerful learning technique nowadays: Neural Network. So stay updated, and I will be with you soon. See you!",
    "tags": "machine-learning linear-regression implementation underfit overfit problems Tutorial",
    "url": "/mahaveer0suthar.github.io/tutorial/Underfit-Overfit/"
  },{
    "title": "Installing Caffe on Ubuntu (CPU-ONLY)",
    "text": "First, to tell you guys the truth, I had no intention to write this post. You know, because I actually don’t have much experience with Caffe. And I am not some kind of experienced tech-guy who can deal with almost developing environment, either. Remember I told you about how to prepare your AWS g2.x2large instance for Machine Learning? You can have a look at this post here. I recommended you to use AWS’s instance because of the fact that most of us can’t afford a proper desktop to run Machine Learning stuff. But of course it is always a good idea to set things up similarly on your local machine. Why? Because we can do all the coding on your machine (like configure your network architecture, or test if your codes can be compiled without errors, etc), and just leave the heavy tasks for the GPU instance. It would help you save some money. With that thought, I decided to install Caffe on my laptop. And guess what? It took me almost a whole working day to finish! What a shame, I thought. But soon I realize how experienced I became after struggling with it. You know, I should have been spending my Saturday wandering around with my guys without thinking a damn thing about Machine Learning related stuff. But I didn’t regret a little bit, not even a little bit. Because at least I got it done. And I’m here to share my experience with all you guys. My laptop is running on Ubuntu 16.04, with OpenCV 3.0.0 installed. The other versions of Ubuntu should work as well, because it is the Caffe and the necessary dependencies matter, not the Ubuntu version. As you may already knew, Caffe is a powerful framework written in C++ for implementing Deep Neural Network and it is being used almost everywhere out there. For that reason, you can just type Caffe installation in Google search bar, and you can find a lot of websites telling you how to do it. But sometimes it may not work to someone, just like my case. So what is the reason then? First, Caffe does require a lot of necessary dependencies to make it work. And errors occur mostly because of dependencies incompatibility. As you may be recommended by other sites that you should use Anaconda, one distribution of Python which can self-manage packages installation. I am not saying that Anaconda is bad. In fact I used it a lot in the past, when I had very little experience in Python package installation. It did help me do most of the installing things. So what is the problem? In my case, having both Python and Anaconda installed caused some kind of a mess, and Caffe struggled with finding the appropriate libraries it needed. Note that Anaconda itself is a completely seperate Python distribution, which means I had two version of Python installed in my machine! So I decided to remove Anaconda from my disk to try some luck. And guess what? The problem was SOLVED! So the first thing I want you to try is: if you have both Python and Anaconda installed, try to remove Anaconda first. sudo rm -rf ~/anaconda2 Note that you will have to change the path according to your Anaconda path. Next, we will install Boost: sudo apt-get install -y --no-install-recommends libboost-all-dev In the next step, we will install all the necessary packages for Caffe: sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libboost-all-dev libhdf5-serial-dev \\ libgflags-dev libgoogle-glog-dev liblmdb-dev protobuf-compiler Next, clone Caffe repository, and make a copy of file Makefile.config.example: git clone https://github.com/BVLC/caffe cd caffe cp Makefile.config.example Makefile.config Next, we will have to install all the necessary Python packages, using pip. Navigate to python folder, and type the line below: sudo pip install scikit-image protobuf cd python for req in $(cat requirements.txt); do sudo pip install $req; done Note that I got to add sudo to make it work on my laptop. Because what it was supposed to install directly to the real environment. Next, we have to modify the Makefile.config. Uncomment the line CPU_ONLY := 1, and the line OPENCV_VERSION := 3. I also commented out the lines which indicate using Anaconda just to make sure that Anaconda doesn’t mess up with the installation: cd .. sudo vim Makefile.config # CPU-only switch (uncomment to build without GPU support). CPU_ONLY := 1 ... # Uncomment if you're using OpenCV 3 OPENCV_VERSION := 3 ... # ANACONDA_HOME := $(HOME)/anaconda2 ... # PYTHON_LIB := $(ANACONDA_HOME)/lib Now you got everything ready. Let’s make it: make all It will take a while to finish. But you would probably get some error message like below: CXX src/caffe/net.cpp src/caffe/net.cpp:8:18: fatal error: hdf5.h: No such file or directory compilation terminated. Makefile:575: recipe for target '.build_release/src/caffe/net.o' failed make: *** [.build_release/src/caffe/net.o] Error 1 Seems like a mess, huh? It is because hdf5 library and hdf5_hl library actually have a postfix serial in their names, the compiler cannot find them. To fix this, we just have to make a link to the actual files. Remember, we are not changing their names! But first, let’s check out the actual name of the libraries. It may vary on your machines, though. cd /usr/lib/x86_64-linux-gnu/ ls -al ... libhdf5_serial.so.10.1.0 libhdf5_serial_hl.so.10.0.2 ... You may find the two files like above. Note again that the version may be different. Just take note the ones you saw. Then we will make a link to them: sudo ln -s /usr/lib/x86_64-linux-gnu/libhdf5_serial.so.10.1.0 /usr/lib/x86_64-linux-gnu/libhdf5.so sudo ln -s /usr/lib/x86_64-linux-gnu/libhdf5_serial_hl.so.10.0.2 /usr/lib/x86_64-linux-gnu/libhdf5_hl.so And note that the postfixes of hdf5 and hdf5_hl are not always the same. In my case, I assumed that they are the same (10.1.0) and it made me pay the price with another error (of course it took me another while to figure out what I did wrong). After doing that, try make all again, this time there should be no more errors! Next, we will run two other commands: make test make runtest And these two should be working as well. And you will likely see the result like below: If you saw something similar, then Congratulations! You have successfully installed Caffe! Now you can get your hands dirty with some real Deep Neural Network projects and become a part of Caffe community! Next step is optional but I highly recommend because we are using Python for our works. We will compile the Python layer so that we can use caffe directly in our Python source code. make pycaffe Here, most of your machines will compile without error. But someone may see some error like below (I did too). CXX/LD -o python/caffe/_caffe.so python/caffe/_caffe.cpp python/caffe/_caffe.cpp:10:31: fatal error: numpy/arrayobject.h: No such file or directory compilation terminated. Makefile:501: recipe for target 'python/caffe/_caffe.so' failed make: *** [python/caffe/_caffe.so] Error 1 The error indicates that it can not find a header file named arrayobject.h. It was caused because numpy was installed in a different path, and we must manually point to it. Actually, this problem was solved at the time of writing, but the installation path varies, so not everyone will get through it. For ones who encountered the error above, all you have to do is to make a small change to your Makefile.config from this: PYTHON_INCLUDE := /usr/include/python2.7 \\ /usr/lib/python2.7/dist-packages/numpy/core/include to this: PYTHON_INCLUDE := /usr/include/python2.7 \\ /usr/local/lib/python2.7/dist-packages/numpy/core/include After that, let’s do make pycaffe again, and it should work now. Next, we will have to add the module directory to our $PYTHONPATH by adding this line to the end of ~/.bashrc file. sudo vim ~/.bashrc export PYTHONPATH=$HOME/Downloads/caffe/python:$PYTHONPATH Note that you have to change your caffe directory accordingly. We are nearly there, next execute the command below to make things take effect: source ~/.bashrc At this time, you can import caffe in Python code without any error. Not so hard, right? python &gt;&gt;&gt; import caffe &gt;&gt;&gt; But we are not done yet. Caffe provides us some examples of the most well-known models. We will use the LeNet model to train the MNIST dataset. Everything was already set up. All we have to do is just make it work: cd ~/Downloads/caffe ./data/mnist/get_mnist.sh ./examples/mnist/create_mnist.sh Note that you have to change the path to where you put caffe accordingly. Next, we will train the model: ./examples/mnist/train_lenet.sh At this point, there should be an error telling you that it was trying to use GPU in CPU-only configuration: I1009 18:51:42.646926 22536 caffe.cpp:217] Using GPUs 0 F1009 18:51:42.647065 22536 common.cpp:66] Cannot use GPU in CPU-only Caffe: check mode. *** Check failure stack trace: *** @ 0x7fd00383f5cd google::LogMessage::Fail() @ 0x7fd003841433 google::LogMessage::SendToLog() @ 0x7fd00383f15b google::LogMessage::Flush() @ 0x7fd003841e1e google::LogMessageFatal::~LogMessageFatal() @ 0x7fd003c38c00 caffe::Caffe::SetDevice() @ 0x40ad33 train() @ 0x4071c0 main @ 0x7fd0027b0830 __libc_start_main @ 0x4079e9 _start @ (nil) (unknown) Aborted (core dumped) Well, that’s OK. We just have to apply a tiny fix to the file examples/mnist/lenet_solver.prototxt, replace GPU with CPU, and save it. Try to run the command above again, then everything should work just fine! It will take a while to finish the training (maybe long since we are using CPU). When it completes, you may see something like this: Your result may vary but we will likely achieve an accurary value of approximately 99%. Congratulations again! Caffe is now working perfectly on your machine! After today’s post, I hope that you all had Caffe installed successfully on you machines. But if you still had troubles installing Caffe, like some frustrating errors keep coming or something, then feel free to leave me a comment below, and I will try to help you figure out why. See you!",
    "tags": "machine-learning deep-learning caffe installation environment essential Project",
    "url": "/mahaveer0suthar.github.io/project/Installing-Caffe-CPU-Only/"
  },{
    "title": "Machine Learning Part 4: Setting Up Python Environment for Computer Vision and Machine Learning",
    "text": "So, I am here with you again, in the new post of Machine Learning tutorial series. Next time, I told you about Linear Regression and promised to talk further more in the next post. And I am also gonna ask you to do some coding, remember? But before doing that, we need more than our hands. If you are gonna have a show, then you must have the stage prepared, right? Similarly, if you want to write some lines of code, then you need to prepare the developing environment. Today, I will show you how to do that. In this post, I am assuming that you are using a computer running on Linux based OS (like Ubuntu, Red Hat, ect). For ones who are sticking to Windows, I am so sorry. I will write a post for Windows users soon. We will install OpenCV for Computer Vision, and Keras (Theano backend) for Machine Learning. I will make it short and clear, just to make sure that you will have a working environment, without spending so much time on struggling with unknown errors. 1. INSTALL OPENCV Firstly, open your Terminal, and type the lines below to Update and Upgrade the packages: sudo apt-get update sudo apt-get upgrade From now on, just hit y when prompted. Next, we will install the necessary packages for OpenCV: sudo apt-get install build-essential cmake git pkg-config libjpeg8-dev \\ libjasper-dev libpng12-dev libgtk2.0-dev \\ libavcodec-dev libavformat-dev libswscale-dev libv4l-dev gfortran sudo apt-get install libtiff5-dev Next, we will need library to optimizing purpose (there will be a lot of simultaneous computation in Computer Vision and Machine Learning which mainly bottleneck the performance). We will use ATLAS (among many other BLAS implementation): sudo apt-get install libatlas-base-dev After that, we will install pip, a tool used for installing Python packages by PyPA: wget https://bootstrap.pypa.io/get-pip.py sudo python get-pip.py In the next step, we will need to install virtualenv and virtualenvwrapper, which are tools for isolating Python environments. You may ask why we have to do that. Imagine you are using Python for many purposes (like game developing, computer vision, machine learning, etc). Because each requires different configuration, it will be a good idea to work seperately to avoid confliction. sudo pip install virtualenv virtualenvwrapper To make virtualenv work, we will have to apply these lines to our ~/.bashrc by typing sudo vim ~/.bashrc (I prefer Vim, you can use whatever you want). Then add these lines to the end of the file: export WORKON_HOME=$HOME/.virtualenvs source /usr/local/bin/virtualenvwrapper.sh Now, let’s create our virtual environment for installing OpenCV source ~/.bashrc mkvirtualenv opencv From this step, make sure you are in opencv environment (you can know by seeing if there is (opencv) before the $ mark). Next we need to install python development tools package: sudo apt-get install python2.7-dev Then we will install numpy, a very powerful module for dealing with array computation: pip install numpy Note that you can not add sudo all the time. Because sudo means that you are execute as superuser, it will install directly into your system environment, not the virtual environment! Next, we will download OpenCV, then checkout to 3.0.0 branch: cd ~ git clone https://github.com/Itseez/opencv.git cd opencv git checkout 3.0.0 cd ~ git clone https://github.com/Itseez/opencv_contrib.git cd opencv_contrib git checkout 3.0.0 Now we got everything ready to be installed, let’s compile and install OpenCV: cd ~/opencv mkdir build cd build cmake -D CMAKE_BUILD_TYPE=RELEASE \\ -D CMAKE_INSTALL_PREFIX=/usr/local \\ -D INSTALL_C_EXAMPLES=ON \\ -D INSTALL_PYTHON_EXAMPLES=ON \\ -D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib/modules \\ -D BUILD_EXAMPLES=ON .. make sudo make install sudo ldconfig In the last step, we only need to link the installed OpenCV to our virtual environment: cd ~/.virtualenvs/cv/lib/python2.7/site-packages/ ln -s /usr/local/lib/python2.7/site-packages/cv2.so cv2.so Now let’s test if OpenCV was installed successfully (make sure that you are still standing in opencv virtual environment): python &gt;&gt;&gt; import cv2 &gt;&gt;&gt; cv2.__version__ '3.0.0' If you see the output like above, then Congratulations! You have successfully installed OpenCV! 2. INSTALL KERAS Now let’s move on to install Keras. Actually we will almost use the scikit-learn library, not Keras in the near future, but Keras is not so hard to install, so I decided to add it here so that I won’t have to make another post for Keras. As I mentioned earlier, you may not want to mess up your environments. So let’s create another virtual enviroment for Keras. # Exit the current virtual environment deactivate mkvirtualenv keras Now let’s install necessary packages for Keras: pip install numpy scipy scikit-learn pillow h5py Keras actually works on top of Theano and Tensorflow (if you don’t give a damn about what they are, then that’s just fine!). Suppose that you are using Keras with Theano backend, so you will have to install Theano first: pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git Wait until the Theano installation completes, then we can install keras: pip install keras We have just finished our Keras installation. If you want to use OpenCV in keras virtual environment, you can link it the way we did above: cd ~/.virtualenvs/keras/lib/python2.7/site-packages/ ln -s /usr/local/lib/python2.7/site-packages/cv2.so cv2.so Now let’s test our installation. Again, make sure that you are still in keras enviroment. python &gt;&gt;&gt; import keras Using Theano backend. If you had the exact output as above, then Congratulations again! Wait… what if you didn’t, but received Using TensorFlow backend error instead. Well, sometimes Keras fails to choose the right backend, and you must do it manually. But don’t worry, it should be just a piece of cake! All you have to do is modify the file ~/.keras/keras.json, replace ‘TensorFlow’ with ‘Theano’, then try to test it again. It should work just fine! So now you have installed all you need for learning Computer Vision and Machine Learning. Great job guys! It was not as hard as you imagined right? So now you have swords and shields. We are ready to be on the field. I know you guys don’t want to wait no more. So I will post the next post soon, I promise. Before that, make sure you revise all the stuff about Linear Regression. And I will be back. See you!",
    "tags": "machine-learning computer-vision development python environment essential Tutorial",
    "url": "/mahaveer0suthar.github.io/tutorial/Setting-Up-Python-Environment-For-Computer-Vision-And-Machine-Learning/"
  },{
    "title": "Preparing AWS’s instance to run Machine Learning’s projects",
    "text": "Hello there, it’s been a while since my previous Real time Object Recognition project. I currently have some cool stuff ongoing and I’ll share it to you when it’s ready (it won’t be long). At the moment, I’m struggling with a frustrating performing problem. As I told you before in the previous Project, the training process requires a powerful desktop which you have to spend a lot of money on, and may take a lot of time and resources. But this headache can be solved with a pre-trained model, like the one I used. And luckily, because using that model for recognizing object is not a big deal (as it took approximately 1.5 ~ 2 seconds per image on my PC), I still somehow felt satisfied. But life is not that easy. Things get tougher as you keep moving on. Now instead of 2 seconds, I have to wait for minutes for each image to be 100% processed, which is simply unacceptable! Of course, I can just throw $2000 on Amazon for a giant desktop with everything set up, rather than complaining to you on my blog (which may be boring you, I’m sorry). Yeah, I wish I had that money, dude! No way am I stopping. There must be some way out, I supposed. And I finally found it, Amazon Web Service (AWS). You can simply think of AWS as a place which offers desktop for rent. Of course, it costs! But it won’t cost you as much as buying a powerful giant desktop. You know, you are free to have our own choices. But I choose AWS, for now. And this post is for ones who consider to use AWS for running their own Machine Learning projects, just like me. Setting things up in AWS won’t be hard, but takes some time. Firstly, you have to register for AWS. You can do it by access to AWS Home Page, then choose “Sign In to the Console”, you will be redirected to the page like this: After the registration completes, go back to AWS Home Page and sign in with your registered account. Here’s what you will see: On the top right, make sure you’re choosing N. California region. If it is somewhere else, change it to N. California. We will use EC2 instance, so choose EC2 (the first one in Compute category). You will see the following page showing up: Click “Launch Instance”. Here’s where we will create our new instance, or you can also choose to use pre-configured one. You may wonder why. A new created instance is just like a new computer, with just OS installed. If you want to work with Python, you have to install Python, if you want to work with Caffe, you have to install and configure Caffe. Sounds challenging, right? In my case, although I got a few experiences working with environment configuration on my own PC, I don’t feel like doing it all over again, especially on a computer belonging to someone else. So I choose to use the pre-configured one! And I recommend you to do so. On the next page after clicking “Launch Instance”, choose “Community Instance” on the left, and type “cs231n” to the search bar. That’s the instance configured by Stanford University, which have Caffe, Torch, Theano, Keras, Lasagne installed, which means it’s already up and ready. Click “Select”, on the next screen, scroll down to “g2.2xlarge” type. Select it and click “Review and Launch”. We are working on Machine Learning projects, which requires a great deal of parallel computing. And a GPU Instance’s type will be the best fit for that. Simply click “Launch” on the next screen. At this step, AWS will require a key-pair authentication. Because this is the first time you launch, just select “Create a new key pair”, and type whatever you want to name that key. And click “Launch Instances”. You should then receive a .pem file which holds your key-pair information. Keep it safe because there’s no way to get it back once you lose it! At this point, you may receive an error, telling you that your account in under verification. Just give it about 2 hours and try to launch again. Here’s what you will see after your account is verified: After launching into the instance, it will take a while to initialize, so you have to wait until the status changes to “2/2 checks passed”: Then take note the instance’s public IP address, open your local machine’s terminal, type: ssh -i PEM_FILE ubuntu@PUBLIC_IP You will probably get an error like this: Permissions 0664 for PEM_FILE are too open. It is required that your private key files are NOT accessible by others. This private key will be ignored. Load key PEM_FILE: bad permissions That’s because your PEM file is accessible by all users. You must change the file’s permissions by typing: chmod 600 PEM_FILE Note that you must be in the folder where you placed you PEM_FILE. After that, you will now be able to log in and use the GPU instance everytime you want. Just don’t forget to Stop it when you finished your work, or your credit card bill payments will shock you till death. And note that there’s two different options: Stop and Terminate. Stop means to Shut down, while Terminate means to Delete the instance. You can read more here if you want to change the initiated Shutdown behavior, or here to disable the Terminate option. If you encounter any error during working with your instance, try to Terminate and start it all over again. Then everything should work just fine. That’s it. Simple enough, right? Hope you find this post helpful. And I’ll be back soon to reveal the cool stuff I’ve been doing to you.",
    "tags": "machine-learning AWS AMI project Project",
    "url": "/mahaveer0suthar.github.io/project/Prepare-AWS-Instance/"
  },{
    "title": "Machine Learning Part 3: Linear Regression",
    "text": "Here we are again, in the third post of Machine Learning’s tutorials. Today I’m gonna tell you about Linear Regression, the most common and understandable learning algorithm. This time I will dig more deeper so that after this post, you will know what actually happened during the learning process. So no more Dogs and Cats today, but algebraic stuff. Yeah, we’re gonna work with matrices, from now on. But don’t worry, it’s not gonna be so hard today. As I told you before, we got some training data containing Features and Labels. Features are somewhat distinct which can be used to distinguish between things. Remember this? To recall a little bit, I called ‘X’ Features, ‘y’ Labels, and ‘a’ Prediction. It may get your attention here, but it’s just a naming convention, which X is always in uppercase, and y is lowercase. So after collecting a great deal of training data (X, y), we have them learned by the computer. Then we show the data which the computer has never seen before, which contains only X this time, and the computer will give us a prediction, called ‘a’. I just helped you to recall about what Machine Learning is. But I think it would be better if you have a further look about Machine Learning on my first post here: What is Machine Learning? So from the image above, the first thing coming to our minds is, how the computer can compute y from X during the learning process, and how it can compute prediction a from X? Well, we will continue from what we were doing on the first post. The answer to the question above is: we need an learning algorithm. And in order to make an algorithm work, we need something called Activation Function. Activation Function Here’s where we left off in the first post. The reason why I mentioned it to you because you will face the term Activation Function not only in Linear Regression, or Logistic Regression in later post, but also in the more complicated algorithms which you will learn in the future. So, what is Activation Function? That’s a function which takes X as variable, and we use it to compute the prediction a. In case of Linear Regression, the Activation Function is so simple that it’s not considered an Activation Function at all! I’ll explain the equation above. First, the superscript and the subscript on each X, what do they mean? Imagine we have a training dataset which has 10 examples, each example has 4 features, so the superscript will indicate the ith example, and the subscript will indicate the jth feature. You will be used to this convention soon, so don’t worry if you can’t get that right now. For the sake of simplicity, let’s consider an example, where we have 10 examples, each example only contains one feature. So the Activation Function will look like this: Does it look similar to you? Yeah, that’s exactly a linear equation with one variable which you learned a lot at high school. If we plot it on the coordinate plane, we will obtain a straight line. That’s the idea of Linear Regression. Imagine our training data look like this: X y 1 7 2 8 3 7 4 13 5 16 6 15 7 19 8 23 9 18 10 21 If we plot them on the coordinate plane, we will obtain something like this: So, our mission now, is to find an appropriate function which can best fit those points. In case of Linear Regression with one variable, because the activation function is actually a straight line, so we will have to find a straight line which can almost go through all those points, intuitively. But, how do we start? Well, we will start by randomizing all the parameters, which means \\( \\theta_0,\\theta_1 \\). So let’s set them both to 1. Now we can compute a by activation function: \\(a=1+x\\). Now if we plot X, y, and the straight line \\(a=1+x\\), we will have something like this: Obviously, the straight line we obtain from \\(a=1+x\\) doesn’t fit our training data well. But that’s OK because we just began by randomizing the parameters, and no learning was actually performed. So here comes the next question: how can we improve the activation function so that it can fit the data better? Or I can say it differently: how can we make the computer learn to fit the data? Obviously, we must think of some way to evaluate how well the current function is performing. One way to accomplish this task is to compute Cost Function, which takes the difference between the Label and the prediction as its variable. And among many types of Cost Function, I will introduce to you the Mean Squared Error Function, which is the most appropriate approach for Linear Regression, and yet maybe the simplest one for you to understand. Mean Squared Error Function Firstly, let’s see what Mean Squared Error Function (MSE) looks like: Now everything has just become clear. It computes the mean value of the squared errors, which are the differences between the Prediction \\( h_\\theta(X^{(i)}) \\) and the Label \\( y^{(i)} \\). You can see that if the value of \\( J(\\theta) \\) is large, it means that the difference between the Prediction and the Label is also large, which causes the straight line can not fit the training data well. In contrast, if the value of \\( J(\\theta) \\) is close to zero, it means that the Prediction and the Label lie very closely in the coordinate plane, which we can tell that the straight line obtained from the activation function fits the training data pretty well. Here you may have a question: why don’t we just take mean value of the difference between Prediction and Label? Why must we use the squared value instead? Well, there’s no “must” here. In this case the squared error works just fine, so it was chosen. There’s no problem if we just use the difference instead. But let’s consider this case. Imagine you have \\( a^{(1)}=2 \\), \\( y^{(1)}=4 \\) and \\( a^{(2)}=5 \\), \\( y^{(1)}=3 \\). What will happen in both cases? No squared error: With squared error: As you can see, the MSE will accumulate the error without caring the sign of the error, whereas the Mean Error is likely to omit the error like the example above. Of course, in another place, using the Mean Error instead of MSE will somehow make some sense, but that’s beyond the scope of this post, so I’ll talk about it when I have chance. So, through MSE value, we can somehow evaluate how well the activation function is performing, or how well the straight line obtained by the same function is fitting our training data. Then what will we do in the next step? We’ll come to a new concept called, Gradien Descent. Keep going, you’re half way there! Gradient Descent Before digging deeper into Gradient Descent. Let’s look back our MSE function: Note that our cost function takes \\( \\theta \\) as its variable, not \\( X^{(i)} \\). For the sake of simplicity, let’s say \\( \\theta \\) contains only \\( \\theta_1 \\). Then our cost function will look like below: As you can see, our cost function now becomes a quadratic function with \\( \\theta_1 \\) variable. Let’s take one more step, when we plot a quadratic function, we will obtain some figure like this: A picture’s worth a thousands word, right? Our learning objective is to find the parameter \\( \\theta \\) so that we can draw a straight line which can almost go through all the points in the coordinate plane. In order to accomplish that, we compute a Cost Function (in this case we use the MSE function). We want the value of the Cost Function to be as small as possible. As we can see in the figure above, our Cost Function is now a quadratic function \\( A\\theta^2+B\\theta+C \\). Because we have \\( A&gt;0 \\), so that our Cost Function is a convex function. You can think of a convex function as some function which has one or many minima. In the case of quadratic function with one variable, our Cost Function only has one minimum. Obviously, all we have to do now, is to find that minimum’s value. But how will we do that? Let’s consider the next figure: You may remember that earlier in this post, we started by randomize the parameter \\( \\theta \\). So with that randomized value, let’s say some value which is far from the minimum. How is it supposed to go down to the minimum? Oops, you already got it right. It just simply goes down, step by step. But mathematically, how can we force it to go down? Look at the first arrow to the right a little bit. You may find it very familiar, there’s something that is equal to the slope of the tangent line of the Cost Function of the starting point. I’ll help you this time: that is Derivatives. To be more exact, when \\( J(\\theta) \\) is the function of multiple variables, instead of saying Derivatives, we will use the term: Gradient. Gradient is actually a vector whose elements are Partial Derivatives. Find it hard to understand? Derivative (one-variable function): Gradient (multiple-variable function): I think I’m not going any further in explaining what is behind the Gradient. You can just think of it as a way to tell the computer: which direction to move its next step from the current point. With the right direction, it will gradually make it closer and closer to the minimum, and when it’s finally there, we will have our Cost Function reach its minimum value, and as a result, we will have our final Activation Function which can best fit our training data. So now you might understand how exactly the learning process occurs. Here comes the final step: how does the computer update the parameters to move towards the next point on the Cost Function \\( J(\\theta) \\) graph? Parameter Update With the Gradient \\( \\nabla J(\\theta) \\) obtained above, we will perform update on the parameters like below: Note that bote \\( \\theta \\) and \\( \\nabla J(\\theta) \\) are vectors, so I can re-write the equation above like this: You may see the newcomer \\( \\alpha \\). It’s called learning rate, which indicates how fast the parameters are updated at each step. Simply, if we set \\( \\alpha \\) to be large, then it’s likely to go down faster, and reach the desired minimum faster, and vice versa, if \\( \\alpha \\) is too small, then it will take more time until it reach the minimum. So you may ask, why don’t we just make \\( \\alpha \\) large? Well, learning with large learning rate is always risky. Consider the figure below: As you might see, if we set our learning rate too large, then it will behave unexpectedly, and likely never reach the minimum. So my advice is, try to set \\( \\alpha \\) to be small at first (but not too small), then see whether it worked or not. Then you can think about increasing \\( \\alpha \\) gradually to improve the performance. After you know what the learning rate \\( \\alpha \\) is. The last question (I hope) you may ask is: how do we compute the Gradient? That’s pretty easy, since our MSE function is just a quadratic function. You can compute the Partial Derivatives using the Chain Rule. It may take some time to compute, so I show you the result right below. You can confirm it yourselves afterwards. For weights (\\( \\theta_1, \\ldots, \\theta_n\\)) There’s one more thing I want remind you: Weights and Bias. As I said in the first post, Weights are parameters which associate with X, and Bias is parameter which stands alone. So I show you above how to update the Weights. What about the Bias? Because Bias doesn’t associate with X, so its Partial Derivative doesn’t, either. For bias (\\( \\theta_0 \\)) Until this point, you may understand why we use \\( \\frac{1}{2m} \\) instead of \\( \\frac{1}{m} \\) in the MSE function. Because it’s a quadratic function, using \\( \\frac{1}{2m} \\) will make it easier for computing Partial Derivative. Everything happens for some reason, right? Now let’s put things together. Here’s what we have: Seems like a mess, huh? The best way to understand something is doing something with it. Now let’s solve the problem above, we will try to make the straight line to fit our training points. Parameters Initialization: As above, we initialized \\(\\theta_0=1\\), \\(\\theta_1=1\\). We will compute our Cost Function \\( J(\\theta) \\). At starting point, our Cost Function’s value is unacceptably large. So we have to update our parameters using Gradient Descent. Let’s do it now. Let’s set the learning rate \\( \\alpha=0.03 \\), here’s the new parameters after we performed the update: With the new parameters, let’s re-compute the Cost Function: You can see that our Cost Function’s value has dropped significantly after just one update step. Let’s plot our new Activation Function to see how well it improved: Eh, there’s still much room for improvement. Doing similarly until the forth step, here’s what we got: Let’s plot it onto the coordinate plane: As you can see, we now have our straight line which can fit our training data fairly well. And if we run the update few more rounds, you may see that our Cost Function still keeps decreasing but with just slight changes. And we can barely visualize the improvement in the graph. Obviously, we just can’t ask for more in case of a straight line. And since this is the simplest algorithm that I used to explain the learning process, I hope you can now understand what is actually “behind the scenes” and visualize the learning process. In the next post, I’ll continue with the second part on Linear Regression. I’ll show you how we can improve the performance of Linear Regression, and we will, finally, using Python’s powerful libraries to help us complete the implementation. That’s it for today. Drop me a line if you have any question. See you in the next post!",
    "tags": "machine-learning linear-regression cost-function gradient-descent Tutorial",
    "url": "/mahaveer0suthar.github.io/tutorial/Linear-Regression/"
  },{
    "title": "Real Time Object Recognition (Part 2)",
    "text": "So here we are again, in the second part of my Real time Object Recognition project. In the previous post, I showed you how to implement pre-trained VGG16 model, and have it recognize my testing images. You can take a look at the first part here: Real Time Object Recognition (Part 1). The model performed pretty well, although I didn’t even have to do any further pre-processing (such as object localization, or image enhancement, etc). In the next step, I will use the same model, to predict object from a continuous input, such as video file or input from a camera. If you have ever done some work with Computer Vision before, you will see find it extremely easy. Let’s say, just a slight improvement over the last one. Yeah, that’s true! For ones who have no experience in Computer Vision, I’ll explain a little bit here. So, if you have an image, and want to have it recognized. Here is what you do: # Load the image # Use the model to have the image recognized # Show the result Now, you have a video file, how to make it work? You know that movies, videos are just hundreds or thousands of images shown continuously. So what you have to do, is just read through a video, from its first image, to the last one. And you see that it works exactly the same way with the one above. # Load the video for image in video: # Use the model to have the image recognized # Show the result Easy, right? As shown above, the result is shown continuously to your eyes. And your eyes will treat them as a video, not seperate images. Here comes something to consider about. Remember the conflict between OpenCV and Keras that I mentioned in the last post? To recall a little bit, After reading the input image, OpenCV and Keras turn it into arrays in different ways. So basically, if we want the model to recognize the input image, we have two choices to pick. Use the Keras’ method: from keras.preprocessing import image as image_utils image = image_utils.load_img(file, target_size=(224, 224)) image = image_utils.img_to_array(image) Or, use the OpenCV’s method, then converting to Keras’ format: import numpy as np image = cv2.imread(file) image = image.transpose((2, 0, 1)) As you can see, Keras’ method reads the image from file, then performs conversion. In case of reading from a camera, it may be a little bit odd to read each frame, then immediately save to disk, then have it read by Keras. Doing like so will slow down the performance, and it is obviously not an efficient solution. Remember I said that I prefered the approach which using OpenCV’s method? Because I just needed to perform one matrix transpose, and I had everything ready for Keras, without saving and reading from disk anymore. So, by just adding two lines of code made things done. Why the hell must I split it into two parts? Well, because there’s a tiny problem in performance. Obviously, everyone does prefer a perfect show, right? What’s the problem then? I told you in the first part, that our model is quite a big guy (it can distinguish between 1000 classes). Big guys tend to move slowly, so does the VGG16 model. It took approximately 1.5 ~ 2 seconds to recognize ONE image (on my PC). So what will it be like when dealing with an input from a camera (or a video file)? Definitely, it will be a very horrible scene that you never wish to see. Therefore, we come to a fact that we must do something here, or we are likely to ruin the show. There may be many tricks out there, I think. But in my case, I just simply, put the recognition task in another thread. Whenever the recognition on a frame is ready to deliver, I updated the result. On the other side, the output was smoothly shown since it no longer had to wait for the recognition result. Ones with little experience in programming, especially multi-threading, would find what I said hard to understand. Don’t worry, I’ll explain right below: In case of no multi-threading: # Load the video for image in video: # Use the model to have the image recognized # 2 SECONDS LATER... # Show the result With multi-threading: # Load the video for image in video: # Show the image with the last result put on it # Somewhere else on Earth # Load the image to recognize # Perform recognition # 2 SECONDS LATER... # Output the result Do you get the trick here? When implementing multi-threading, the codes which deal with recognition task is seperated from the output task. It will return the result each 2 seconds. On the other side, because the output won’t have to wait for the recognition result, it just simply puts the last received result on, and updates when the new one is delivered. You make ask, so the recognition process is actually skipping everything between the 2-second periods, so what we see in the output may not be the exact result. For example, your camera was on a dog, and you passed the frame containing the dog to the recognition code, 2 seconds later, the Dog label was delivered, but you are now looking at a lion! What a shame on a real time recognition app! Well, it sounds like an interesting theory. But I think, no one moves their cameras that fast, let’s say, abruptly change the view each second. Am I right? So, you now know about the problem you may face working with real time object recognition, and I showed you how to deal with it by implementing multi-threading. But… If there’s something I need you to know about me, it’s that I am not a professional programmer, which means I prefer working on theoretical algorithms to spending hours on coding. To me, coding is simply a way to visualize the theory. Therefore, my code may seem like a mess to you guys. So feel free to tell me if you find something wrong or something which can be improved. I will definitely appreciate that. Tired of reading? So sorry to make it long. I’ll show you the result I got right below: Let’s talk a little bit about the result above. You can see that, despite of the bad light condition (my room is only equipped with one damn yellow light!), the model still performed pretty well and totally satisfied me. Sometimes it got wrong (couldn’t recognize my G-Shock, or having trouble in distinguishing whether it was a screen or a TV!), but that was far more than expected. And finally, here’s the code in case you need: Object Recognition Code. Most of them was cloned from François Chollet’s repository. I just coded two files below: For recognizing images seperately: test_imagenet.py For real time object recognition with camera: camera_test.py Hope you enjoy this project. Feel free to leave me some feedbacks or questions. I will be more than pleased to help. So we are done with this Real time Object Recognition project, but not with Machine Learning! And I’ll see you soon, in the next projects!",
    "tags": "machine-learning VGG object recognition learning project camera real time recognition Project",
    "url": "/mahaveer0suthar.github.io/project/Real-Time-Object-Recognition-part-two/"
  },{
    "title": "Real Time Object Recognition (Part 1)",
    "text": "Technology sometimes seems like magic, especially when we don’t have any idea about how it was done, or we even think it can’t be done at all. When I was a kid, I was a huge fan of Sci-Fi Films, which were on every TV channel in the 1990s in my country. They did inspire me a lot, help grow up an “engineering mindset” in me. Being an engineer sometimes makes me feel so great, because I can mostly understand many things that seem to be magical to others. But to be honest, there are still lots of things out there can make me feel like a fool. Just let me tell you one of them. It was sometime in 2015, I watched a demo video from a startup company in Japan. It was about Object Recognition using Deep Learning. At that time, I didn’t even have any idea about what Machine Learning is, not to mention Deep Learning! My thought then? “That was impossible. How did they do that?” I kept questioning myself for a while. I just couldn’t get it out of my mind. But unfortunately, because my first thought was “That was impossible”, or at least “That was impossible to me”, I gave it up, left the mystery unrevealed. It was not until I paid serious attention to Machine Learning that I somehow made the mystery become clear. As I was working around with Convolutional Neural Network, then I suddenly thought of the mystery I gave up one year ago. I mean, that was really exciting and crazy. (Feels like Mission 6 accomplished to me, lol) Seems like a pretty long beginning (I didn’t mean to write a preface for my book or something, though). So I am here to share my work. In fact, it was not something impresssive (especially when comparing to outstanding projects out there). But I really hope this post can help, especially for ones who have enthusiasm in Machine Learning and once came accross something like me before. In this project, I will try to reproduce the result I saw in the demo video one year ago. Concretely, I will move the camera around a table, with many objects put on it. The computer will try to recognize each object, and print the object’s label in the output of the camera. Writing it all in one post may hurt, so I separate this project into two parts like below: Part 1: Go through the folder containing the images, recognize object in each. Part 2: Real time object recognition through input from camera Before I begin, let’s talk a little bit about how Machine Learning works. I had post about what Machine Learning exactly is, and how it exactly works here: What is Machine Learning? The main part of any Machine Learning system, is the Model. It contains the algorithms and all the associated parameters. A good Model will produce a good prediction. Because of that, the training process (i.e. the process which produces the Model) can be heavily computational and take a lot of resources (you know, RAM and memories to store the parameters). Fortunately, a great work from François Chollet helps make this problem become much more relaxing than ever. I mean that you won’t have to spend a great deal of money on a giant computer (with powerful CPU and GPUs), you won’t have to spend time on training the data yourself, thanks to François Chollet for providing us the pretrained Model on the most outstanding CNN architectures. Sounds cool, right? Among those, I will use the VGG16 Model for this project. So, let’s get down to business. Firstly, I downloaded some images for testing purpose, and put them in the same folder. ap = argparse.ArgumentParser() ap.add_argument(\"-f\", \"--folder\", required=True) args = vars(ap.parse_args()) files = [os.path.join(args[\"folder\"], f) for f in os.listdir(args[\"folder\"])] random.shuffle(files) As written in the code above, I passed the folder name, then got the list of file names stored in files variable. I shuffled the list for demonstration purpose (you don’t want the same order everytime, right?) Next, I initiate the VGG16 Model, provided by François Chollet. You will have to clone his repository first, from here from vgg16 import VGG16 model = VGG16(weights=\"imagenet\") As you can see we are passing the weights=”imagenet” parameter, to tell the Model to initialize with pretrained parameter set. It is quite a large file (approximately 550MB), so it will take some time to download on its first run. (The parameter set is saved somewhere in keras’ temporary folder) Then, I looped through the file list created above, in each iteration: from keras.preprocessing import image as image_utils image = image_utils.load_img(file, target_size=(224, 224)) image = image_utils.img_to_array(image) Keras provides us a method for loading image for training and testing purpose. Note that OpenCV and Keras treat the input image in different ways, so we cannot use image loaded by OpenCV’s imread method for Keras. Concretely, let’s say we have a 3-channel image (a common color image). OpenCV’s imread will produce an (width, height, 3) array, whereas Keras requires an (3, width, height) format. Another way to solve this is to use Numpy’s tranpose method: import numpy as np image = cv2.imread(file) image = image.transpose((2, 0, 1)) I prefer the second approach, you will understand why when we come to the second part of this project. Next, we need to add one more dimension to the array obtained above. Why we have to do that? If you have experience with Neural Network, you may find the term mini_batch similar. The additional dimension will tell the Model the number of input arrays (for example, you have 70,000 data, so you need to pass an array with shape (70000, depth, width, height) for the Model to run on, let’s say SGD or RMSprop or something). If you don’t have any idea about what I’ve just talked, you can ignore it for now. I’ll talk about Neural Network in later posts, I promise. After converting the input image to Keras’s format, I passed it through a pre-processing method: from imagenet_utils import preprocess_input image = preprocess_input(image) The pre-processing method came along with the models provided by François Chollet. It simply subtracts each channel with its mean value, and solve the ordering conflict between Theano and Tensorflow. So we now have things ready. Let’s pass the preprocessed array to the Model and get it predicted: preds = model.predict(image) (inID, label) = decode_predictions(preds)[0] Let’s talk a little bit about ImageNet’s image database (the database on which VGG16 was trained). It was organized according to WordNet hierarchy (you can find more details here). But the predicting result is always numerical (Neural Network only works with numerical labels), so we have to map between the numerical result, and the noun provided by WordNet. Once again, François Chollet provided a method to do that: decode_predictions method. It simply map the predicted result with a JSON file, and return the associated noun instead. Here’s what the JSON file looks like: {\"0\": [\"n01440764\", \"tench\"], \"1\": [\"n01443537\", \"goldfish\"], \"2\": [\"n01484850\", \"great_white_shark\"], \"3\": [\"n01491361\", \"tiger_shark\"], \"4\": [\"n01494475\", \"hammerhead\"], \"5\": [\"n01496331\", \"electric_ray\"], \"6\": [\"n01498041\", \"stingray\"], \"7\": [\"n01514668\", \"cock\"], \"8\": [\"n01514859\", \"hen\"], \"9\": [\"n01518878\", \"ostrich\"], \"10\": [\"n01530575\", \"brambling\"], ...} Finally, the easiest part: load the input image with OpenCV’s imread, then put the label found above on it, then show the result. And we are DONE! origin = cv2.imread(file) cv2.putText(origin, \"Predict: {}\".format(label), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2) cv2.imshow(\"Result\", origin) Here are some results I got from my test images: So I’ve just walked through the first part of the Real time Object Recognition project. You may think that it’s not a big deal anyway. But in my opinion, it did help a lot in visualizing some real work on Object Recognition using Convolutional Neural Network. Just implementing the code to use the model may not take you some months to work on, but importantly, it now doesn’t seem like magic anymore. That’s all for part 1. Hope you enjoy it. In part 2, I will continue with real time object recognition through continuous input from camera. So stay updated and I’ll see you there. You can find the second part of this post here: Real Time Object Recognition (Part 2).",
    "tags": "machine-learning VGG object recognition learning project Project",
    "url": "/mahaveer0suthar.github.io/project/Real-Time-Object-Recognition-part-one/"
  },{
    "title": "Machine Learning Part 2: Types of Learning",
    "text": "So here we are again, to continue what we dared to begin: challenging Machine Learning. Before we move on, let’s spend a minute to look back on the last post. You now know what Machine Learning is, it’s something that contains a Model, some (Features, Labels) as inputs (remember we needed many Dog’s images?) and some Activations as outputs (Dog or Not a Dog). You also know what exactly Learning means. It’s the process of Trial-and-Error repeatedly. Trial makes big mistakes in the first time, but help the computer (as well as human’s brain) to learn faster, to become more confident on the next time. Learning is doing something repeatedly, guys. So don’t be ashamed to do some revision until you’re ready to move on. You can find the last tutorial here: What is Machine Learning? So, let’s get down to business. I’ll make it easy for you today. It means today’s post won’t be such long as the previous one. Even Cristiano Ronaldo needs some recovery-day, right? Types of Machine Learning What I want to talk to you today, is about types of Learning. So why the hell must we know about it. So imagine you’re trying to learn piano in a mathematical way, it means that you treat those beautifully composed repertoires as some kind of, let’s say those annoying matrices you saw yesterday. You’ll soon realize it doesn’t make any sense, logically. And even if you can play through it, but actually you just can’t get it (if you’re listening to classical music, then you will know what I mean). So we have to admit that, different things require different learning approaches. If you choose an inappropriate way of learning, then it’ll likely result in some unpleasant tragic. Sounds like a headache here, right? But don’t worry, it’s not that complicated in the case of Machine Learning. Above I told you that you won’t be able to get progressed in learning piano with some mathematical approach. In the case of Machine Learning, piano repertoires can be treated as Inputs. So different Input types do require different approach of learning. It does make some sense, right? So, based on Input types, we can divide Machine Learning into three categories like below: Supervised Learning Unsupervised Learning Reinforcement Learning Supervised Learning First, as you may know (in fact I think you don’t even care), that I’m currently living in Japan (I’m not Japanese, though). When I tried to learn some Machine Learning’s vocabularies in Japanese, I discovered some couple of interesting things. One of those is Supervised Learning term. Why am I telling you this? Because I think it may help you understand this term easily. They call it “Learning with a teacher” in Japanese. Does it make some sense to you? When your parents sat by your side and taught you each picture was about, they acted just like you teachers. When you tried to learn for you SAT or GMAT, it was not you who learned by yourselves! Where did you get those vocabularies from? From your teachers? From you instruction books? Doesn’t matter, at least you learned from some sources. Or I can say, you were learning under the supervision of something. It means you knew what were right and what were wrong at the time you were learning. Everything is exactly the same in Machine Learning, Supervised Learning indicates that the computer knows whether it made a right guess or not, using the Label along with each Feature. Remember the Dog Recognition example on the previous post? Yep, it used the Labels to evaluate its work. So, whenever you see a Machine Learning problem where Labels provided along with Features, that’s definitely Supervised Learning. Unsupervised Learning Obviously, Unsupervised Learning is the opposite of Supervised Learning. It means that you will be given a mess of Features without any Labels on them. So it doesn’t make any sense here, you may suppose. How can the computer actually learn without knowing whether it’s doing right? A picture is worth of thousand words. Let me show you: As you can see in the picture above. We have two set of points (just ignore the axises’ names, I just want you to focus on the points). Some are red, and some are blue. And it’s obvious that we can draw a line between them, let’s say the green one. As you might get, it’s exactly an another example of Supervised Learning, where the Labels are Red &amp; Blue. (For ones who find it hard to understand. Just imagine the Red points are images of “Dog”, and the Blue ones are images of “Not a Dog”. But don’t worry, I’ll make it more concrete when we come to Logistic Regression!) So, what about this one: I just simply made them all Blue! So what’s the computer supposed to do now? As you could see from the previous example, the Labels may help in the learning process, but they’re not something which we can’t live without. It’s the distribution of Features which matters! For example, the computer may not see the “Dog” label (or the “Not a Dog” label, either), but it can put things with four-leg features, black-nose features, long-tail features, long-tongue features in the same group. So in the end, we can have the same result with what we got from Supervised Learning. (Of course, actually it can’t be always the same! Imagine that the computer will also have a group for things with two-leg features, two-swing features (for birds’ images); a group for things with no-leg features, long-tongue features (for snakes’ images) and so on. That’s because Unsupervised Learning is not limited by the Input Labels, so the result may vary depending on how the computer learns) It may be too long to write all about these two types of Learning. Actually in this tutorial, I will mainly focus on Supervised Learning because it’s very common, not only on research but also in real life projects. And via Supervised Learning you will understand the algorithms much faster, much deeper without hurting your enthuasiasm. Reinforcement Learning To be honest, I rarely talk about something I don’t know much about. Actually at the time of writing, I’m just in the beginning of my research on Reinforcement Learning. So for the sake of simplicity, Reinforcement Learning is the learning through the interaction with Environment. Supervised Learning and Unsupervised Learning are not. Why are they not? Remember the way they actually learn? We give them a set of Input for them to learn from. And they use what they learned from that Input to make predictions (another way of saying “Guess”, I guess). So the way they predict depends entirely on the Input they learn from. We may see the disadvantage here, right? So Reinforcement Learning appears as a solution to eliminate that disadvantage. You can simply think that the computers will actually learn from dynamic Input, it makes them much more robust, precise and confident. But the trade off, that kind of learning is very complicated, especially for Machine Learning’s newcomers. So just put it in the list, OK? So, now you know about three main types of Machine Learning (there’s a lot out there but you only need to focus on these three, I think). I hope after this post, you can have a (slightly) deeper thought about Machine Learning. In the next post, I’ll talk about Linear Regression, the most common and basic algorithm (let’s say for almost advanced algorithms). And we’ll finally get our hands dirty with some coding (sounds cool?). So stay updated, and I’ll see you there. See ya!",
    "tags": "machine-learning supervised unsupervised reinforcement regression classification Tutorial",
    "url": "/mahaveer0suthar.github.io/tutorial/Types-Of-Learning/"
  },{
    "title": "Machine Learning Part 1: What is Machine Learning?",
    "text": "You’ve been hearing about machine learning in the last few years, especially on the day on which AlphaGo was hotter than Donald Trump. You felt excited, you wanted to know what it is, or even more, you wanted to dig into it. But you just still can’t get it? Let me help you this time. So, the very big question, and maybe the biggest reason which led you here: What exactly is Machine Learning? Well, I’ve got no intention to repeat something that was already written on Wikipedia (you’re tired of it, right?). So my definition of Machine Learning is: we don’t teach the computers to work anymore, we make them learn, to do things itself! Still confused? Let’s see something cool instead. So as you can see in the picture, we have a model (the big one in the middle), an input which takes care of something call (Feature, Label) and an output which gives us something call Activation. So what the hell are all these things? Let’s imagine you want the computer to do you a favor: to tell you whether there is a dog in an image. So it becomes much clearer when I match up everything like the image below: Firstly, what is Features? If this is the first time you hear it, then congratulations, you will see this keyword everywhere, everywhen from now on (as long as you keep up with Machine Learning, of course). So let’s make it simple. Look at the dog. He has four legs (like the other dogs on Earth), his nose is black, he is hanging out his tongue, he has a tail, … Yeah, those are what come up when you see this dog, and importantly, you will see those in other dogs too. So it turns out to be some distinct features which make human’s brain know it just saw a dog. Wait a minute, did I just say Features? Well, I hope you get the point here. So, Features are something distinct that make something different than the others. Human’s brain recognises something by their Features, so why don’t we teach the computer that trick? (Come on, don’t be that selfish man) I know you’re excited now. Ready to move on? So what about Label? Well, it’s much simpler than the Feature. Label is a name of guys who have specific features. So we call guy who have four legs, one black nose, one long tongue and one long tail a DOG. Well, I just can’t make it simpler. So what about the other guys? At this time, I just call them “Not a dog”. Maybe you will ask me why I don’t make it more specific, just like “a Cat”, “a Bird” or something. You will have the answer when we come to Logistic Regression. Oh my God, please don’t care about that name. I’ll remind you when the time is up. So please stay patient :) There is a lot of fun ahead, I promise. So I’ve talked about Features and Labels, but what to do next? Remember when we were kids, our parents bought us some picture books? Then we spent days after days, kept pointing in and speak out loud names of things we saw, with so much pride. Omg, my parents must be so patient with me back in the day. So what the hell does the childhood’s story make sense here? Well, we will do exactly what our parents did. We show them images, a large amount of them, just like this: As you can see, each image is labeled whether the image contains a dog. Just like what we did in the past, the computer will try to learn from the images we shown them. I said it LEARNS, not MEMORIZES. We could learn, so can the computer, right? And lastly, what is Activation? Simply speaking, we can call it a Guess. Imagine your computer knew how to tell whether it saw a dog (we just assume that, we’ll back to it right below), then you show it some image it has never seen before, so it has to Guess, based on its own mind. And you can easily figure it out, the Guess (or Activation) must be one of the Labels we taught them. (We can’t teach them about Dogs, then force them to recognise a Cat, it doesn’t make any sense right?) To make it more concrete, let’s see the image below: Yeah, at least the computer can now distinguish between a Dog and a Crab. Wait! It doesn’t know about a Crab, at this point it only know the second image is NOT a Dog. Okay, whatever, though. It gets better someday, definitely. When we talk about Machine Learning, we talk about Features, Labels, Activations, and … So up to this point, I assume that you can draw your own sketch about what Machine Learning actually is, what Machine Learning actually does (or be done). Quite simple right? Then I have something for you: WAIT! Where’re you going? Grabbing your algebraic book or something? Or just finding your PS4’s controller? Take it easy, man. You won’t even have to grab a pencil to do some maths today, I promise! So why the hell I shown you this? Because I’m afraid after you read this post, you’ll run outside, tell your friends about Machine Learning, yeah you’ll tell them about something like “four-leg feature”, “long-tail feature”, “a Dog”, or “not a Dog”… Please don’t do that. Sorry, I’m just joking, kind of over-joking. Let’s get back to business. We all know about binary number (something like a switch with On-Off state, 0 or 1). And sadly, that’s what computer can understand. So it won’t see something like “four-leg feature”, “long-tail feature”. The things they actually see is just boring repeating 0s and 1s. All I want to say here is, Features, Labels &amp; Activations, they’re all matrices to computers. At this point I just want you to know it. (Maybe I don’t even need to speak that long, right? Sorry, my bad again!) Let me summarize all the things we talked above by the image below: Sounds familiar, right? Let’s imagine the Dog recognition is taken place somewhere in the computer, we call it a Model. The final goal is to help create this. We don’t create, we’re just the helpers, the computer must handle it alone. Don’t forget that it’s the one that learns :) When we talk about Machine Learning, we talk about Features, Labels, Activations, and of course, a Model. So next step is, Training? Sounds like we’re in the middle of the pitch and do some crazy training, just like Cristiano Ronaldo? No, of course we don’t (or actually we can’t). We’re not training muscles, we’re training the brain, the computer brain. That’s the process when the computer learn from the images we shown it. But let’s talk a little bit about how it learns. We won’t even teach them (although we said machine LEARNING, so learning from what?) Imagine the label of each image is printed on the back so the computer can’t see. I know you got it. Exactly what we do with our flashcards, the things you can’t live without to prepare for your SAT, GMAT or something like that. Firstly, the computer will try to guess, without knowing anything about what a Dog should look like. Then it flips the back over, sees the answer, and learns. Yep, it learns from making MISTAKES. Just like human does, the bigger the mistake is, the faster it learns. Then next step is Evaluation? I assume we gave the computer 100 images (roughly 50 images of Dog, and 50 images of Not a dog). It will try to guess each image until it finish the 100th one. Then it counts the number of right guesses. For example, there are 69 right guesses, so it has a probability of 69% in recognising whether there is a dog. Not so bad, right? That’s what we call Evaluation. The computer evaluates its own work: But what if the evaluation result can’t satisfy us? We just tell the computer about that, it will update its own Parameters. But what the hell are ‘Parameters’ here? So much new keywords today! (Maybe many of you want to tell me that!) Remember the Model I said above? Parameters are just something put into the Model, just like machines in the factory. If something goes wrong with the product, they just have them fixed. Exactly the same with Parameters. Simple, right? Just as the image above, let’s call it ‘θ’. Some other places you will see thay use ‘W’ for Weights, and ‘b’ for Biases, but right now for the sake of simplicity, just remember ‘θ’. Don’t ask me about the naming convention, I don’t know! Everyone uses that, and you don’t want everyone to look at you just like “What the hell that guy is talking about!” when you talk about Machine Learning, right? So the loop Training-Evaluating-Parameter Updating is actually the core of Machine Learning. Until now, I think you can accept this easily without hurt. Thanks again, cool Dogs in the beginning :) The core of Machine Learning: Training - Evaluating - Parameter Updating then repeat! So last, but not least, I just want you to do me a favor. I’m definitely sure you can. Did you notice the question mark on the fifth image (The annoying image with some kind of algebraic matrices)? So that’s the last thing I want to reveal in the end of this post. And once again I’ll show you some algebraic stuff, but stay calm, you won’t get your hands dirty today! Okay, such a long function which likely comes from hell. Please ignore it right now. All I need you to know are just: Activation Function, Bias and Weights. I won’t make it long this time. So from Feature to Activation, we’ll go through Activation Function, very easy to remember, right? The parameter standing alone is call Bias, the other parameters which have their own Xs, are called Weights. That’s all for today. Why are they called Bias &amp; Weights, I’ll reveal it in the next tutorial. So, finally we made it till the end. You may ask why I had to make such a long post, and sometimes talked so much about simple things. Well, the later posts will definitely contain a lot of things you don’t want to face (such as algebraic stuff, …), you will even get your hands dirty with some coding work (Cool!). So I have to make sure you don’t make any wrong assumption in the beginning, you know, people make mistakes by inappropriate assumptions, right? Hope this post is helpful to you on the long-run towards your Machine Learning’s targets. So stay updated, and I’ll meet you on the next post of this tutorial. See ya!",
    "tags": "machine-learning model feature learning Tutorial",
    "url": "/mahaveer0suthar.github.io/tutorial/Machine-Learning-Definition/"
  },{
    "title": "Tensorflow Implementation Note: Installing Tensorflow and Keras on Windows",
    "text": "Hello everyone, it’s been a long long while, hasn’t it? I was busy fulfilling my job and literally kept away from my blog. But hey, if this takes any longer then there will be a big chance that I don’t feel like writing anymore, I suppose. So it might not be a bad idea to just make some simple posts (some kind of implementation note like the one I’m writing right now), until life becomes easier, right? That’s enough talking. Let’s get down to business. Today I’m gonna talk about some of my implementation notes when working with Tensorflow, not Tensorflow tutorial exactly :) What is Tensorflow anyway? I think it’s a little bit rude to say this but, maybe Google can handle that question way better than me. To make it short for you guys, Tensorflow is: open source library for deep learning (mainly) developed by Google Brain Team, released in 2015 well documented supposed to replace Theano growing fast like hell! Very promising! I remember when I tried Tensorflow last year, the documents were kind of mess since they weren’t up-to-date with newest release, and as the result, the source code for tutorials didn’t work as expected. But things have become way easier recently after they re-arranged Tensorflow’s repositories and made documents up-to-date. Those it-was-supposed-to-run-but-it-didn’t problem won’t annoy you anymore. Still can’t find it attracting? You can find more on Google and Youtube for more interesting presentations of Tensorflow. And as I don’t intend to make this post unnecessarily long, please DIY :) Installing Tensorflow and Keras For Unix users, there shouldn’t be any problems installing both Tensorflow and Keras, I believe, if you follow the instructions on their pages. Just one thing: Tensorflow has two seperate versions for CPU-only and GPU-accelerated PC, so don’t download the wrong one, or you will end up in trouble! Tensorflow installation: Tensorflow installation Keras installation: Keras installation For Windows users, installing Tensorflow can be done with ease, just like on Linux machine, you can install Tensorflow just by one single command. But it’s a little bit tricky, though. Tensorflow installation (Windows): There’s a couple of ways to install Tensorflow, as you can find here: Tensorflow installation. But there’s a tiny problem: Tensorflow only works with Python 3.5.x on Windows, so as you might guess, other versions won’t work. Well, you’re not wrong. So if you use a Windows machine, I recommend that you stick with Anaconda to manage Python versions as well as its dependencies (you can use the native pip along with Anaconda too). To install Anaconda, please visit their Windows install. After Anaconda is completely installed, it will provide you a customized command prompt (called Anaconda Prompt), where you can run Python shell with ease, and it understands some Linux commands too! So for now, let’s get started. Firstly, you need to create a Python 3.5 environment in order to install Tensorflow: &gt;conda create -n tensorflow_windows python=3.5 anaconda After the new environment is created, let’s activate it (notice that the environment’s name will be added before the prompt): &gt;activate tensorflow_windows (tensorflow_windows)&gt; Next, we will install Tensorflow, let’s suppose we are working on a CPU-only Windows machine (my company’s PCs are all CPU-only): (tensorflow_windows)&gt;pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.1.0-cp35-cp35m-win_amd64.whl It couldn’t be simpler, right? Now, let’s go ahead if Tensorflow is installed properly: (tensorflow_windows)&gt;python &gt;&gt;&gt; import tensorflow as tf &gt;&gt;&gt; sess = tf.Session() &gt;&gt;&gt; print(sess.run(tf.constant('Hello, Guys!') The output should be as follow: b'Hello, Guys!' That’s it. Now you have Tensorflow installed on your machine and you can start your first Tensorflow-based project. But I suggest that you keep reading. Yeah, I guess you don’t want to miss out Keras, right? Keras installation (Windows): Well, it’s a real headache if you want to install Keras into Windows machine. But I highly recommend that! Having a framework built on top of Tensorflow will help your work become much easier. The least tedious way to install Keras (as some of you might have done) requires just two commands like below: (tensorflow_windows)&gt;conda install mingw libpython (tensorflow_windows)&gt;pip install keras But I hardly recommend it! As with Theano, installing Keras like above may result in trouble since the version to be installed is usually not up-to-date with the latest version of Tensorflow. So rather than the ones above, here’s the recommended commands: (tensorflow_windows)&gt;conda install mingw libpython (tensorflow_windows)&gt;pip install --upgrade keras --upgrade flag will ensure that you install the latest version of Keras. But guess what, that’s where the trouble comes from! The installation should be aborted, and you will see an error telling you that scipy failed to install. The reason is, Keras requires SciPi, a library built on top of Numpy (and of course, for numerical computational optimization purposes), and sometimes, it has some problem with MKL, an optimization library from Intel. So what we’re gonna do is, instead of installing Numpy and SciPy using native pip, we will download and install from customized wheel files. The download files’ URLs are below: (Remember to select the right one, which has cp35 in its name!) Numpy: http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy SciPy: http://www.lfd.uci.edu/~gohlke/pythonlibs/#scipy After downloading the two files, navigate to your download folder, and install them like this: (tensorflow_windows)&gt;pip install numpy‑1.11.3+mkl‑cp35‑cp35m‑win_amd64.whl (tensorflow_windows)&gt;pip install scipy‑0.19.0‑cp35‑cp35m‑win_amd64.whl And for now, we can install Keras: (tensorflow_windows)&gt;pip install --upgrade keras The installation should go smoothly without any error. Let’s go ahead and check if it works properly: (tensorflow_windows)&gt;python &gt;&gt;&gt; from keras import backend &gt;&gt;&gt; print(backend._BACKEND) The output would be something like this: tensorflow Then we are done! Summary That’s it for today. It may not be an informative post as you expected, but I hope you guys, especially Windows users can now install Tensorflow and Keras on Windows with no more annoying errors. So, get your hands dirty and begin your journey with Tensorflow! And see you next time.",
    "tags": "machine-learning deep-learning keras tensorflow gpu training note Project",
    "url": "/mahaveer0suthar.github.io/project/Tensorflow-Installation/"
  },{
    "title": "Page Not Found",
    "text": "Sorry, but the page you were trying to view does not exist — perhaps you can try searching for it below.",
    "tags": "",
    "url": "/mahaveer0suthar.github.io/404.html"
  },{
    "title": "About",
    "text": "Hi there, I’m Mahaveer. I am a Senior Deep Learning Engineer at a Geo-spatial AI Startup, 3+ year experience. Expertise in following : Python, PrTorch, Tensorflow Deep learning Computer Vision Machine learning Software Development I’m currently living and working in New Delhi, India. I’m a big fan of Computer Vision, Machine Learning and Deep Learning. I’m doing some works on Machine Learning and Computer Vision. I’m also maintaining this blog to share my knowledge about ML and CV, as well as my work on both fields. You can find all my undergoing projects here on github: My GitHub. Hope this blog help with your learning. And don’t hesitate to drop me a line. I’ll reply as soon as I can, promise!",
    "tags": "",
    "url": "/mahaveer0suthar.github.io/about/"
  },{
    "title": "Posts by Category",
    "text": "Project Installing NVIDIA Docker On Ubuntu 16.04 Updated: February 04, 2018 Hey guys, it has been quite a long while since my last blog post (for almost a year, I guess). Today, I am going to tell you about something that I wish I ha... Creating A Language Translation Model Using Sequence To Sequence Learning Approach Updated: December 20, 2017 Hello guys. It’s been quite a long while since my last blog post. It may sound like an excuse, but I’ve been struggling with finding a new place to move in... Creating A Text Generator Using Recurrent Neural Network Updated: November 14, 2017 Hello guys, it’s been another while since my last post, and I hope you’re all doing well with your own projects. I’ve been kept busy with my own stuff, too... Training With Your Own Dataset on Caffe Updated: November 09, 2017 Hi, everyone! Welcome back to my Machine Learning page today. I have been playing around with Caffe for a while, and as you already knew, I made a couple o... Solving problem when running Faster R-CNN on GTX 1070 Updated: November 03, 2017 Hello guys, it’s great to be here with you today (why do I keep saying that boring greeting, you may ask). To be honest, there are a lot of things I want to ... Installing Caffe on Ubuntu 16.04 (GPU Mode with CUDA) Updated: November 01, 2017 It’s great to be with all you guys again in today’s post. As you already knew, it’s been a while since I built my own desktop for Deep Learning. Or for ones ... Building up my own machine for Deep Learning Updated: October 29, 2017 Hello guys, it’s me again being here with all you guys today. There will be no tutorial in this post, so at first I’m sorry for that. But Halloween is coming... Compiling and Running Faster R-CNN on Ubuntu (CPU Mode) Updated: October 14, 2017 So today I am gonna tell you about how to compile and run Faster R-CNN on Ubuntu in CPU Mode. But there is a big chance that many of you may ask: What the he... Installing Caffe on Ubuntu (CPU-ONLY) Updated: October 09, 2017 First, to tell you guys the truth, I had no intention to write this post. You know, because I actually don’t have much experience with Caffe. And I am not so... Preparing AWS’s instance to run Machine Learning’s projects Updated: October 06, 2017 Hello there, it’s been a while since my previous Real time Object Recognition project. I currently have some cool stuff ongoing and I’ll share it to you when... Real Time Object Recognition (Part 2) Updated: September 30, 2017 So here we are again, in the second part of my Real time Object Recognition project. In the previous post, I showed you how to implement pre-trained VGG16 mo... Real Time Object Recognition (Part 1) Updated: September 26, 2017 Technology sometimes seems like magic, especially when we don’t have any idea about how it was done, or we even think it can’t be done at all. When I was a k... Tensorflow Implementation Note: Installing Tensorflow and Keras on Windows Updated: May 09, 2017 Hello everyone, it’s been a long long while, hasn’t it? I was busy fulfilling my job and literally kept away from my blog. But hey, if this takes any longe... Tutorial What I talk when I talk about Tensorflow Updated: November 02, 2018 Some of my collegues, as well as many of my readers told me that they had problems using Tensorflow for their projects. Something like this: Machine Learning Part 10: Linear Support Vector Machine Updated: November 05, 2017 Hi guys! It’s been while since my last tutorial post about Regularization. And today, as I promised, I’m gonna talk about one supervised learning algorithm... Machine Learning Part 9: Regularization Updated: October 31, 2017 Welcome back to my 9th tutorial on Machine Learning! I have been kept busy in last weekends, struggling in getting my desktop ready for Deep Learning. You ... Machine Learning Part 8: Decision Tree Updated: October 26, 2017 Hello guys, I’m here with you again! So we have made it to the 8th post of the Machine Learning tutorial series. The topic of today’s post is about Decisio... Machine Learning Part 7: Cross Validation Updated: October 24, 2017 Hello there, so I’m here again to bring you the 7th post on my Machine Learning tutorial series. Today I will talk about Cross Validation, a term that you ... Machine Learning Part 6: Logistic Regression Updated: October 18, 2017 Hello there, I am back with you today. In the 6th post on Machine Learning tutorials series, I will tell you about Logistic Regression, a very important an... Machine Learning Part 5: Underfitting and Overfitting Problems Updated: October 12, 2017 Here we are again, in the fifth post of Machine Learning tutorial series. Today I will talk about two common problems you may face in Machine Learning: Und... Machine Learning Part 4: Setting Up Python Environment for Computer Vision and Machine Learning Updated: October 08, 2017 So, I am here with you again, in the new post of Machine Learning tutorial series. Next time, I told you about Linear Regression and promised to talk further... Machine Learning Part 3: Linear Regression Updated: October 01, 2017 Here we are again, in the third post of Machine Learning’s tutorials. Today I’m gonna tell you about Linear Regression, the most common and understandable ... Machine Learning Part 2: Types of Learning Updated: September 22, 2017 So here we are again, to continue what we dared to begin: challenging Machine Learning. Machine Learning Part 1: What is Machine Learning? Updated: September 18, 2017 You’ve been hearing about machine learning in the last few years, especially on the day on which AlphaGo was hotter than Donald Trump. You felt excited, you ...",
    "tags": "",
    "url": "/mahaveer0suthar.github.io/categories/"
  },{
    "title": "Education",
    "text": "Education Master of Technology - Artificial Intelligence Indian Institute of Information Technology &amp; Management, Gwalior, India Major : Computer Vision &amp; Artificial Intelligence Thesis : - Title : “Fast Facial Identification and Recognition both 1:N and 1:1” Advisor - Prof. Rajendra Sahu (Director) Bachelor of Technology - Information Technology *Indian Institute of Information Technology &amp; Management, Gwalior, India Thesis : - Title : Depth Estimation using RGB Imagery Advisor - Prof. Rajendra Sahu Programming Coursework : Algorithm &amp; Data Structure, Operating System, Network, Computer Vision,NLP",
    "tags": "",
    "url": "/mahaveer0suthar.github.io/education/"
  },{
    "title": "Professional Experience",
    "text": "Experience Senior Computer Vision Engineer Attentive AI July 2019 - Current Attentive AI create meaningful insights from remote sensing data using deep learning and computer vision.(like road and building footprint extraction from satellite imagery Responsible for designing, developing and managing different types of Vision Systems for satellite imagery application. Currently Leading and developing InsurTech AI Models for Property Insurance from aerial imagery. Built Street Imagery Feature Extractor which produced street imagery feature One Million Images. Designed a private internal Attentive library for Software and AI Team’s development usages. Lead research &amp; development (R&amp;D) of Street Imagery 2D Feature AI Extraction System. Leveraged Knowledge in Tem Management, Docker, Docker-Compose, Google Compute Engine, GDAL,Faster-RCNN, RFCN Architecture , Utilized Multithreading and Multiprocessing. Computer Vision Engineer Attentive AI July 2018 - July 2019 Responsible for developing different type of Vision Systems for Satellite Imagery Application. Developed a crop classification system from satellite imagery using Sentinel-2 data, and cadastral data from the state government of Madhya Pradesh Developed TargetNet – an object detection module using deep convolutional networks for identifying objects such as cars, planes, tanks etc. in satellite imagery Built a Road Centerline Extractor for satellite imagery, which produced 500K km road centerline (Region US). Developed and Deployed Production Level Pipeline with post processing for End-to-End Road Centerline Extraction from satellite imagery. Leveraged Knowledge in Git, Tensorflow, PyTorch, GDAL, D-LinkNet Architecture, Worked on AWS-EC2-P3 GPU Servers. Deep Learning Researcher Prof. Rajendra Sahu - IIITM Gwalior Jan 2018 – June 2018 Prof. Rajendra Sahu has been associated with teaching profession for the past 30 years and has been closely involved in industrial consultancy projects. He is also Secretary of System Dynamic of India. Responsible for plan, design, and conduct research to aid in interpretation of Computer Vision analysis of RGB Imagery in context of Depth estimation. Compile and Present data relating to the state of a Disparity Map Algorithm and Guided Filter Algorithm. Develop, train and evaluate deep learning models with Guided Filter for Depth Estimation. Archived 82.37 % accuracy on NYU Depth dataset &amp; Here is Demo Video. Leveraged Knowledge in Research Methodology, Data Analysis, Presentation, Python, Transfer learning, Multiband Image Analysis, Guided Filter Architecture. Research Internship Prof. Joy Dip Shar - IIITM Gwalior Jan 2018 – June 2018 Prof. Joy Dip Dhar has been associated with teaching profession for the past 20 years and has been closely involved in Big Data and Data Mining research projects. Responsible for plan, design, and conduct Literature review research on “Understanding Brainwaves and effect of music on brainwaves”. Studied and compile various literature on Brain workings, Brainwave theories and results of brainwave experiments. Finally wrote a paper on “Understanding Brainwaves and effect of music on Brainwaves” check it out here. Leveraged Knowledge in Research Methodology, Brainwaves, Presentation, Deep Sleep Patterns and collaboration. Software Engineer Internship Technology Innovation &amp; Incubation Center May 2016 - June 2016 TIIC is to be an effective interface b/w industry &amp; Academia to foster, promote and sustain commercialization of science &amp; technology in technology in institute for mutual benefits. Responsible for building Virtual Tour of Indian Institute of Information Technology &amp; Management Gwalior. Project Lead of Summer Internship Program. Planned , designed &amp; distributed work for project execution among summer interns. The Virtual Tour was built using 4000 Image and 303 Equirectangular Projections. Leveraged Knowledge in Leadership, Project Management, Web Flash Player, Different type of Photo Sphere Projections, Software Development(Python) Below is demo video of Virtual Tour",
    "tags": "",
    "url": "/mahaveer0suthar.github.io/experience/"
  },{
    "title": "UI Engineer Co-op",
    "text": "Developed and shipped highly interactive web applications for Apple Music using Ember.js Built and shipped the Apple Music Extension within Facebook Messenger leveraging third-party and internal APIs Architected and implemented the front-end of Apple Music’s embeddable web player widget, which lets users log in and listen to full songs in the browser Contributed extensively to MusicKit.js, a JavaScript framework that allows developers to add an Apple Music player to their web apps",
    "tags": "",
    "url": "/mahaveer0suthar.github.io/content/jobs/Apple/"
  },{
    "title": "About Me",
    "text": "Hello! I’Mahaveer, aexperienced computer vision engineer working at the intersection of research and product , for technologies such as Object detection &amp; Recognition, 3D vision and SLAM. I graduated with Master’s from Indian Institute of Information Technology &amp; Management Gwalior (IIIT Gwalior), working with Prof. Rajendra Sahu (Director). Having worked on robust Object based SLAM and Monocular Object tracking . Previously, i worked at IIIT, Gwalior for an year as a Deep Learning Researcher. I worked on outdoor autonomous driving projects for moving object segmentation and localization.",
    "tags": "",
    "url": "/mahaveer0suthar.github.io/content-temp/about/"
  },{
    "title": "Portfolio",
    "text": "Installing NVIDIA Docker On Ubuntu 16.04 Updated: February 04, 2018 Hey guys, it has been quite a long while since my last blog post (for almost a year, I guess). Today, I am going to tell you about something that I wish I ha... Creating A Language Translation Model Using Sequence To Sequence Learning Approach Updated: December 20, 2017 Hello guys. It’s been quite a long while since my last blog post. It may sound like an excuse, but I’ve been struggling with finding a new place to move in... Creating A Text Generator Using Recurrent Neural Network Updated: November 14, 2017 Hello guys, it’s been another while since my last post, and I hope you’re all doing well with your own projects. I’ve been kept busy with my own stuff, too... Training With Your Own Dataset on Caffe Updated: November 09, 2017 Hi, everyone! Welcome back to my Machine Learning page today. I have been playing around with Caffe for a while, and as you already knew, I made a couple o... Solving problem when running Faster R-CNN on GTX 1070 Updated: November 03, 2017 Hello guys, it’s great to be here with you today (why do I keep saying that boring greeting, you may ask). To be honest, there are a lot of things I want to ... Installing Caffe on Ubuntu 16.04 (GPU Mode with CUDA) Updated: November 01, 2017 It’s great to be with all you guys again in today’s post. As you already knew, it’s been a while since I built my own desktop for Deep Learning. Or for ones ... Building up my own machine for Deep Learning Updated: October 29, 2017 Hello guys, it’s me again being here with all you guys today. There will be no tutorial in this post, so at first I’m sorry for that. But Halloween is coming... Compiling and Running Faster R-CNN on Ubuntu (CPU Mode) Updated: October 14, 2017 So today I am gonna tell you about how to compile and run Faster R-CNN on Ubuntu in CPU Mode. But there is a big chance that many of you may ask: What the he... Installing Caffe on Ubuntu (CPU-ONLY) Updated: October 09, 2017 First, to tell you guys the truth, I had no intention to write this post. You know, because I actually don’t have much experience with Caffe. And I am not so... Preparing AWS’s instance to run Machine Learning’s projects Updated: October 06, 2017 Hello there, it’s been a while since my previous Real time Object Recognition project. I currently have some cool stuff ongoing and I’ll share it to you when... Real Time Object Recognition (Part 2) Updated: September 30, 2017 So here we are again, in the second part of my Real time Object Recognition project. In the previous post, I showed you how to implement pre-trained VGG16 mo... Real Time Object Recognition (Part 1) Updated: September 26, 2017 Technology sometimes seems like magic, especially when we don’t have any idea about how it was done, or we even think it can’t be done at all. When I was a k... Tensorflow Implementation Note: Installing Tensorflow and Keras on Windows Updated: May 09, 2017 Hello everyone, it’s been a long long while, hasn’t it? I was busy fulfilling my job and literally kept away from my blog. But hey, if this takes any longe...",
    "tags": "",
    "url": "/mahaveer0suthar.github.io/portfolio/"
  },{
    "title": "Skills &amp; Areas of Interests",
    "text": "Skills Programming Languages Python (Numpy and TensorFlow) JavaScript (React and Node.js) C++ SystemVerilog (Hardware Description Language) Tools, Frameworks Tensorflow, PyTorch, Keras, C++ Docker, Apache Kafka, Caffe2 OpenCV, NumPy, Scikit, GrabCut, GDAL Git, Linux, Bash, Crontab OpenHPC, WMS/WMTS, WFS, WPS Areas of interests/knowledge Image &amp; VIdeo Analysis through Computer Vision Deep Learning &amp; Deep Reinforcement Learning High Performance Computing System Designing for Vector feature processing Large scale Deep learning model deployment Perception: multiple-view geometry, variational methods, statistical inference, Lie theory, multilateration. Neuroscience, Visual Cortex Functioning, Brainwaves Semantic Segmentation, Instance Segmentation, Object Detection, Change Detection and Real time Object tracking Software Windows Linux and Bash Git and SVN Eclipse and NetBeans Doxygen, Javadoc, JSDocs",
    "tags": "",
    "url": "/mahaveer0suthar.github.io/skills/"
  },{
    "title": "Posts by Tags",
    "text": "AMI Preparing AWS’s instance to run Machine Learning’s projects Updated: October 06, 2017 Hello there, it’s been a while since my previous Real time Object Recognition project. I currently have some cool stuff ongoing and I’ll share it to you when... AWS Preparing AWS’s instance to run Machine Learning’s projects Updated: October 06, 2017 Hello there, it’s been a while since my previous Real time Object Recognition project. I currently have some cool stuff ongoing and I’ll share it to you when... GPU Building up my own machine for Deep Learning Updated: October 29, 2017 Hello guys, it’s me again being here with all you guys today. There will be no tutorial in this post, so at first I’m sorry for that. But Halloween is coming... GRU Creating A Language Translation Model Using Sequence To Sequence Learning Approach Updated: December 20, 2017 Hello guys. It’s been quite a long while since my last blog post. It may sound like an excuse, but I’ve been struggling with finding a new place to move in... Creating A Text Generator Using Recurrent Neural Network Updated: November 14, 2017 Hello guys, it’s been another while since my last post, and I hope you’re all doing well with your own projects. I’ve been kept busy with my own stuff, too... GTX 1070 Building up my own machine for Deep Learning Updated: October 29, 2017 Hello guys, it’s me again being here with all you guys today. There will be no tutorial in this post, so at first I’m sorry for that. But Halloween is coming... LSTM Creating A Language Translation Model Using Sequence To Sequence Learning Approach Updated: December 20, 2017 Hello guys. It’s been quite a long while since my last blog post. It may sound like an excuse, but I’ve been struggling with finding a new place to move in... Creating A Text Generator Using Recurrent Neural Network Updated: November 14, 2017 Hello guys, it’s been another while since my last post, and I hope you’re all doing well with your own projects. I’ve been kept busy with my own stuff, too... PC Building up my own machine for Deep Learning Updated: October 29, 2017 Hello guys, it’s me again being here with all you guys today. There will be no tutorial in this post, so at first I’m sorry for that. But Halloween is coming... RNN Creating A Language Translation Model Using Sequence To Sequence Learning Approach Updated: December 20, 2017 Hello guys. It’s been quite a long while since my last blog post. It may sound like an excuse, but I’ve been struggling with finding a new place to move in... Creating A Text Generator Using Recurrent Neural Network Updated: November 14, 2017 Hello guys, it’s been another while since my last post, and I hope you’re all doing well with your own projects. I’ve been kept busy with my own stuff, too... SVC Machine Learning Part 10: Linear Support Vector Machine Updated: November 05, 2017 Hi guys! It’s been while since my last tutorial post about Regularization. And today, as I promised, I’m gonna talk about one supervised learning algorithm... VGG Real Time Object Recognition (Part 2) Updated: September 30, 2017 So here we are again, in the second part of my Real time Object Recognition project. In the previous post, I showed you how to implement pre-trained VGG16 mo... Real Time Object Recognition (Part 1) Updated: September 26, 2017 Technology sometimes seems like magic, especially when we don’t have any idea about how it was done, or we even think it can’t be done at all. When I was a k... caffe Training With Your Own Dataset on Caffe Updated: November 09, 2017 Hi, everyone! Welcome back to my Machine Learning page today. I have been playing around with Caffe for a while, and as you already knew, I made a couple o... Solving problem when running Faster R-CNN on GTX 1070 Updated: November 03, 2017 Hello guys, it’s great to be here with you today (why do I keep saying that boring greeting, you may ask). To be honest, there are a lot of things I want to ... Installing Caffe on Ubuntu 16.04 (GPU Mode with CUDA) Updated: November 01, 2017 It’s great to be with all you guys again in today’s post. As you already knew, it’s been a while since I built my own desktop for Deep Learning. Or for ones ... Compiling and Running Faster R-CNN on Ubuntu (CPU Mode) Updated: October 14, 2017 So today I am gonna tell you about how to compile and run Faster R-CNN on Ubuntu in CPU Mode. But there is a big chance that many of you may ask: What the he... Installing Caffe on Ubuntu (CPU-ONLY) Updated: October 09, 2017 First, to tell you guys the truth, I had no intention to write this post. You know, because I actually don’t have much experience with Caffe. And I am not so... camera Real Time Object Recognition (Part 2) Updated: September 30, 2017 So here we are again, in the second part of my Real time Object Recognition project. In the previous post, I showed you how to implement pre-trained VGG16 mo... classification Machine Learning Part 10: Linear Support Vector Machine Updated: November 05, 2017 Hi guys! It’s been while since my last tutorial post about Regularization. And today, as I promised, I’m gonna talk about one supervised learning algorithm... Machine Learning Part 9: Regularization Updated: October 31, 2017 Welcome back to my 9th tutorial on Machine Learning! I have been kept busy in last weekends, struggling in getting my desktop ready for Deep Learning. You ... Machine Learning Part 8: Decision Tree Updated: October 26, 2017 Hello guys, I’m here with you again! So we have made it to the 8th post of the Machine Learning tutorial series. The topic of today’s post is about Decisio... Machine Learning Part 6: Logistic Regression Updated: October 18, 2017 Hello there, I am back with you today. In the 6th post on Machine Learning tutorials series, I will tell you about Logistic Regression, a very important an... Machine Learning Part 2: Types of Learning Updated: September 22, 2017 So here we are again, to continue what we dared to begin: challenging Machine Learning. compile Solving problem when running Faster R-CNN on GTX 1070 Updated: November 03, 2017 Hello guys, it’s great to be here with you today (why do I keep saying that boring greeting, you may ask). To be honest, there are a lot of things I want to ... Compiling and Running Faster R-CNN on Ubuntu (CPU Mode) Updated: October 14, 2017 So today I am gonna tell you about how to compile and run Faster R-CNN on Ubuntu in CPU Mode. But there is a big chance that many of you may ask: What the he... computer-vision Machine Learning Part 4: Setting Up Python Environment for Computer Vision and Machine Learning Updated: October 08, 2017 So, I am here with you again, in the new post of Machine Learning tutorial series. Next time, I told you about Linear Regression and promised to talk further... cost-function Machine Learning Part 3: Linear Regression Updated: October 01, 2017 Here we are again, in the third post of Machine Learning’s tutorials. Today I’m gonna tell you about Linear Regression, the most common and understandable ... cpu mode Solving problem when running Faster R-CNN on GTX 1070 Updated: November 03, 2017 Hello guys, it’s great to be here with you today (why do I keep saying that boring greeting, you may ask). To be honest, there are a lot of things I want to ... Compiling and Running Faster R-CNN on Ubuntu (CPU Mode) Updated: October 14, 2017 So today I am gonna tell you about how to compile and run Faster R-CNN on Ubuntu in CPU Mode. But there is a big chance that many of you may ask: What the he... cross validation Machine Learning Part 7: Cross Validation Updated: October 24, 2017 Hello there, so I’m here again to bring you the 7th post on my Machine Learning tutorial series. Today I will talk about Cross Validation, a term that you ... cross_val_score Machine Learning Part 7: Cross Validation Updated: October 24, 2017 Hello there, so I’m here again to bring you the 7th post on my Machine Learning tutorial series. Today I will talk about Cross Validation, a term that you ... cuda Installing Caffe on Ubuntu 16.04 (GPU Mode with CUDA) Updated: November 01, 2017 It’s great to be with all you guys again in today’s post. As you already knew, it’s been a while since I built my own desktop for Deep Learning. Or for ones ... cudnn Installing Caffe on Ubuntu 16.04 (GPU Mode with CUDA) Updated: November 01, 2017 It’s great to be with all you guys again in today’s post. As you already knew, it’s been a while since I built my own desktop for Deep Learning. Or for ones ... data splitting Machine Learning Part 7: Cross Validation Updated: October 24, 2017 Hello there, so I’m here again to bring you the 7th post on my Machine Learning tutorial series. Today I will talk about Cross Validation, a term that you ... decision-tree Machine Learning Part 8: Decision Tree Updated: October 26, 2017 Hello guys, I’m here with you again! So we have made it to the 8th post of the Machine Learning tutorial series. The topic of today’s post is about Decisio... deep learning Building up my own machine for Deep Learning Updated: October 29, 2017 Hello guys, it’s me again being here with all you guys today. There will be no tutorial in this post, so at first I’m sorry for that. But Halloween is coming... deep-learning What I talk when I talk about Tensorflow Updated: November 02, 2018 Some of my collegues, as well as many of my readers told me that they had problems using Tensorflow for their projects. Something like this: Installing NVIDIA Docker On Ubuntu 16.04 Updated: February 04, 2018 Hey guys, it has been quite a long while since my last blog post (for almost a year, I guess). Today, I am going to tell you about something that I wish I ha... Creating A Language Translation Model Using Sequence To Sequence Learning Approach Updated: December 20, 2017 Hello guys. It’s been quite a long while since my last blog post. It may sound like an excuse, but I’ve been struggling with finding a new place to move in... Creating A Text Generator Using Recurrent Neural Network Updated: November 14, 2017 Hello guys, it’s been another while since my last post, and I hope you’re all doing well with your own projects. I’ve been kept busy with my own stuff, too... Training With Your Own Dataset on Caffe Updated: November 09, 2017 Hi, everyone! Welcome back to my Machine Learning page today. I have been playing around with Caffe for a while, and as you already knew, I made a couple o... Installing Caffe on Ubuntu 16.04 (GPU Mode with CUDA) Updated: November 01, 2017 It’s great to be with all you guys again in today’s post. As you already knew, it’s been a while since I built my own desktop for Deep Learning. Or for ones ... Installing Caffe on Ubuntu (CPU-ONLY) Updated: October 09, 2017 First, to tell you guys the truth, I had no intention to write this post. You know, because I actually don’t have much experience with Caffe. And I am not so... Tensorflow Implementation Note: Installing Tensorflow and Keras on Windows Updated: May 09, 2017 Hello everyone, it’s been a long long while, hasn’t it? I was busy fulfilling my job and literally kept away from my blog. But hey, if this takes any longe... desktop Building up my own machine for Deep Learning Updated: October 29, 2017 Hello guys, it’s me again being here with all you guys today. There will be no tutorial in this post, so at first I’m sorry for that. But Halloween is coming... development Machine Learning Part 4: Setting Up Python Environment for Computer Vision and Machine Learning Updated: October 08, 2017 So, I am here with you again, in the new post of Machine Learning tutorial series. Next time, I told you about Linear Regression and promised to talk further... docker Installing NVIDIA Docker On Ubuntu 16.04 Updated: February 04, 2018 Hey guys, it has been quite a long while since my last blog post (for almost a year, I guess). Today, I am going to tell you about something that I wish I ha... environment Installing Caffe on Ubuntu 16.04 (GPU Mode with CUDA) Updated: November 01, 2017 It’s great to be with all you guys again in today’s post. As you already knew, it’s been a while since I built my own desktop for Deep Learning. Or for ones ... Installing Caffe on Ubuntu (CPU-ONLY) Updated: October 09, 2017 First, to tell you guys the truth, I had no intention to write this post. You know, because I actually don’t have much experience with Caffe. And I am not so... Machine Learning Part 4: Setting Up Python Environment for Computer Vision and Machine Learning Updated: October 08, 2017 So, I am here with you again, in the new post of Machine Learning tutorial series. Next time, I told you about Linear Regression and promised to talk further... essential Training With Your Own Dataset on Caffe Updated: November 09, 2017 Hi, everyone! Welcome back to my Machine Learning page today. I have been playing around with Caffe for a while, and as you already knew, I made a couple o... Machine Learning Part 10: Linear Support Vector Machine Updated: November 05, 2017 Hi guys! It’s been while since my last tutorial post about Regularization. And today, as I promised, I’m gonna talk about one supervised learning algorithm... Solving problem when running Faster R-CNN on GTX 1070 Updated: November 03, 2017 Hello guys, it’s great to be here with you today (why do I keep saying that boring greeting, you may ask). To be honest, there are a lot of things I want to ... Installing Caffe on Ubuntu 16.04 (GPU Mode with CUDA) Updated: November 01, 2017 It’s great to be with all you guys again in today’s post. As you already knew, it’s been a while since I built my own desktop for Deep Learning. Or for ones ... Machine Learning Part 9: Regularization Updated: October 31, 2017 Welcome back to my 9th tutorial on Machine Learning! I have been kept busy in last weekends, struggling in getting my desktop ready for Deep Learning. You ... Machine Learning Part 8: Decision Tree Updated: October 26, 2017 Hello guys, I’m here with you again! So we have made it to the 8th post of the Machine Learning tutorial series. The topic of today’s post is about Decisio... Machine Learning Part 6: Logistic Regression Updated: October 18, 2017 Hello there, I am back with you today. In the 6th post on Machine Learning tutorials series, I will tell you about Logistic Regression, a very important an... Compiling and Running Faster R-CNN on Ubuntu (CPU Mode) Updated: October 14, 2017 So today I am gonna tell you about how to compile and run Faster R-CNN on Ubuntu in CPU Mode. But there is a big chance that many of you may ask: What the he... Installing Caffe on Ubuntu (CPU-ONLY) Updated: October 09, 2017 First, to tell you guys the truth, I had no intention to write this post. You know, because I actually don’t have much experience with Caffe. And I am not so... Machine Learning Part 4: Setting Up Python Environment for Computer Vision and Machine Learning Updated: October 08, 2017 So, I am here with you again, in the new post of Machine Learning tutorial series. Next time, I told you about Linear Regression and promised to talk further... faster r-cnn Solving problem when running Faster R-CNN on GTX 1070 Updated: November 03, 2017 Hello guys, it’s great to be here with you today (why do I keep saying that boring greeting, you may ask). To be honest, there are a lot of things I want to ... Compiling and Running Faster R-CNN on Ubuntu (CPU Mode) Updated: October 14, 2017 So today I am gonna tell you about how to compile and run Faster R-CNN on Ubuntu in CPU Mode. But there is a big chance that many of you may ask: What the he... feature Machine Learning Part 1: What is Machine Learning? Updated: September 18, 2017 You’ve been hearing about machine learning in the last few years, especially on the day on which AlphaGo was hotter than Donald Trump. You felt excited, you ... fine-tuning Training With Your Own Dataset on Caffe Updated: November 09, 2017 Hi, everyone! Welcome back to my Machine Learning page today. I have been playing around with Caffe for a while, and as you already knew, I made a couple o... gpu What I talk when I talk about Tensorflow Updated: November 02, 2018 Some of my collegues, as well as many of my readers told me that they had problems using Tensorflow for their projects. Something like this: Installing NVIDIA Docker On Ubuntu 16.04 Updated: February 04, 2018 Hey guys, it has been quite a long while since my last blog post (for almost a year, I guess). Today, I am going to tell you about something that I wish I ha... Creating A Language Translation Model Using Sequence To Sequence Learning Approach Updated: December 20, 2017 Hello guys. It’s been quite a long while since my last blog post. It may sound like an excuse, but I’ve been struggling with finding a new place to move in... Creating A Text Generator Using Recurrent Neural Network Updated: November 14, 2017 Hello guys, it’s been another while since my last post, and I hope you’re all doing well with your own projects. I’ve been kept busy with my own stuff, too... Training With Your Own Dataset on Caffe Updated: November 09, 2017 Hi, everyone! Welcome back to my Machine Learning page today. I have been playing around with Caffe for a while, and as you already knew, I made a couple o... Solving problem when running Faster R-CNN on GTX 1070 Updated: November 03, 2017 Hello guys, it’s great to be here with you today (why do I keep saying that boring greeting, you may ask). To be honest, there are a lot of things I want to ... Installing Caffe on Ubuntu 16.04 (GPU Mode with CUDA) Updated: November 01, 2017 It’s great to be with all you guys again in today’s post. As you already knew, it’s been a while since I built my own desktop for Deep Learning. Or for ones ... Tensorflow Implementation Note: Installing Tensorflow and Keras on Windows Updated: May 09, 2017 Hello everyone, it’s been a long long while, hasn’t it? I was busy fulfilling my job and literally kept away from my blog. But hey, if this takes any longe... gradient-descent Machine Learning Part 3: Linear Regression Updated: October 01, 2017 Here we are again, in the third post of Machine Learning’s tutorials. Today I’m gonna tell you about Linear Regression, the most common and understandable ... gtx 1070 Solving problem when running Faster R-CNN on GTX 1070 Updated: November 03, 2017 Hello guys, it’s great to be here with you today (why do I keep saying that boring greeting, you may ask). To be honest, there are a lot of things I want to ... image not showing Solving problem when running Faster R-CNN on GTX 1070 Updated: November 03, 2017 Hello guys, it’s great to be here with you today (why do I keep saying that boring greeting, you may ask). To be honest, there are a lot of things I want to ... implementation Machine Learning Part 5: Underfitting and Overfitting Problems Updated: October 12, 2017 Here we are again, in the fifth post of Machine Learning tutorial series. Today I will talk about two common problems you may face in Machine Learning: Und... installation Training With Your Own Dataset on Caffe Updated: November 09, 2017 Hi, everyone! Welcome back to my Machine Learning page today. I have been playing around with Caffe for a while, and as you already knew, I made a couple o... Installing Caffe on Ubuntu 16.04 (GPU Mode with CUDA) Updated: November 01, 2017 It’s great to be with all you guys again in today’s post. As you already knew, it’s been a while since I built my own desktop for Deep Learning. Or for ones ... Installing Caffe on Ubuntu (CPU-ONLY) Updated: October 09, 2017 First, to tell you guys the truth, I had no intention to write this post. You know, because I actually don’t have much experience with Caffe. And I am not so... keras What I talk when I talk about Tensorflow Updated: November 02, 2018 Some of my collegues, as well as many of my readers told me that they had problems using Tensorflow for their projects. Something like this: Creating A Language Translation Model Using Sequence To Sequence Learning Approach Updated: December 20, 2017 Hello guys. It’s been quite a long while since my last blog post. It may sound like an excuse, but I’ve been struggling with finding a new place to move in... Creating A Text Generator Using Recurrent Neural Network Updated: November 14, 2017 Hello guys, it’s been another while since my last post, and I hope you’re all doing well with your own projects. I’ve been kept busy with my own stuff, too... Tensorflow Implementation Note: Installing Tensorflow and Keras on Windows Updated: May 09, 2017 Hello everyone, it’s been a long long while, hasn’t it? I was busy fulfilling my job and literally kept away from my blog. But hey, if this takes any longe... learning Real Time Object Recognition (Part 2) Updated: September 30, 2017 So here we are again, in the second part of my Real time Object Recognition project. In the previous post, I showed you how to implement pre-trained VGG16 mo... Real Time Object Recognition (Part 1) Updated: September 26, 2017 Technology sometimes seems like magic, especially when we don’t have any idea about how it was done, or we even think it can’t be done at all. When I was a k... Machine Learning Part 1: What is Machine Learning? Updated: September 18, 2017 You’ve been hearing about machine learning in the last few years, especially on the day on which AlphaGo was hotter than Donald Trump. You felt excited, you ... linear-regression Machine Learning Part 5: Underfitting and Overfitting Problems Updated: October 12, 2017 Here we are again, in the fifth post of Machine Learning tutorial series. Today I will talk about two common problems you may face in Machine Learning: Und... Machine Learning Part 3: Linear Regression Updated: October 01, 2017 Here we are again, in the third post of Machine Learning’s tutorials. Today I’m gonna tell you about Linear Regression, the most common and understandable ... logistic-regression Machine Learning Part 6: Logistic Regression Updated: October 18, 2017 Hello there, I am back with you today. In the 6th post on Machine Learning tutorials series, I will tell you about Logistic Regression, a very important an... machine learning Building up my own machine for Deep Learning Updated: October 29, 2017 Hello guys, it’s me again being here with all you guys today. There will be no tutorial in this post, so at first I’m sorry for that. But Halloween is coming... machine-learning What I talk when I talk about Tensorflow Updated: November 02, 2018 Some of my collegues, as well as many of my readers told me that they had problems using Tensorflow for their projects. Something like this: Installing NVIDIA Docker On Ubuntu 16.04 Updated: February 04, 2018 Hey guys, it has been quite a long while since my last blog post (for almost a year, I guess). Today, I am going to tell you about something that I wish I ha... Creating A Language Translation Model Using Sequence To Sequence Learning Approach Updated: December 20, 2017 Hello guys. It’s been quite a long while since my last blog post. It may sound like an excuse, but I’ve been struggling with finding a new place to move in... Creating A Text Generator Using Recurrent Neural Network Updated: November 14, 2017 Hello guys, it’s been another while since my last post, and I hope you’re all doing well with your own projects. I’ve been kept busy with my own stuff, too... Training With Your Own Dataset on Caffe Updated: November 09, 2017 Hi, everyone! Welcome back to my Machine Learning page today. I have been playing around with Caffe for a while, and as you already knew, I made a couple o... Machine Learning Part 10: Linear Support Vector Machine Updated: November 05, 2017 Hi guys! It’s been while since my last tutorial post about Regularization. And today, as I promised, I’m gonna talk about one supervised learning algorithm... Solving problem when running Faster R-CNN on GTX 1070 Updated: November 03, 2017 Hello guys, it’s great to be here with you today (why do I keep saying that boring greeting, you may ask). To be honest, there are a lot of things I want to ... Installing Caffe on Ubuntu 16.04 (GPU Mode with CUDA) Updated: November 01, 2017 It’s great to be with all you guys again in today’s post. As you already knew, it’s been a while since I built my own desktop for Deep Learning. Or for ones ... Machine Learning Part 9: Regularization Updated: October 31, 2017 Welcome back to my 9th tutorial on Machine Learning! I have been kept busy in last weekends, struggling in getting my desktop ready for Deep Learning. You ... Machine Learning Part 8: Decision Tree Updated: October 26, 2017 Hello guys, I’m here with you again! So we have made it to the 8th post of the Machine Learning tutorial series. The topic of today’s post is about Decisio... Machine Learning Part 7: Cross Validation Updated: October 24, 2017 Hello there, so I’m here again to bring you the 7th post on my Machine Learning tutorial series. Today I will talk about Cross Validation, a term that you ... Machine Learning Part 6: Logistic Regression Updated: October 18, 2017 Hello there, I am back with you today. In the 6th post on Machine Learning tutorials series, I will tell you about Logistic Regression, a very important an... Compiling and Running Faster R-CNN on Ubuntu (CPU Mode) Updated: October 14, 2017 So today I am gonna tell you about how to compile and run Faster R-CNN on Ubuntu in CPU Mode. But there is a big chance that many of you may ask: What the he... Machine Learning Part 5: Underfitting and Overfitting Problems Updated: October 12, 2017 Here we are again, in the fifth post of Machine Learning tutorial series. Today I will talk about two common problems you may face in Machine Learning: Und... Installing Caffe on Ubuntu (CPU-ONLY) Updated: October 09, 2017 First, to tell you guys the truth, I had no intention to write this post. You know, because I actually don’t have much experience with Caffe. And I am not so... Machine Learning Part 4: Setting Up Python Environment for Computer Vision and Machine Learning Updated: October 08, 2017 So, I am here with you again, in the new post of Machine Learning tutorial series. Next time, I told you about Linear Regression and promised to talk further... Preparing AWS’s instance to run Machine Learning’s projects Updated: October 06, 2017 Hello there, it’s been a while since my previous Real time Object Recognition project. I currently have some cool stuff ongoing and I’ll share it to you when... Machine Learning Part 3: Linear Regression Updated: October 01, 2017 Here we are again, in the third post of Machine Learning’s tutorials. Today I’m gonna tell you about Linear Regression, the most common and understandable ... Real Time Object Recognition (Part 2) Updated: September 30, 2017 So here we are again, in the second part of my Real time Object Recognition project. In the previous post, I showed you how to implement pre-trained VGG16 mo... Real Time Object Recognition (Part 1) Updated: September 26, 2017 Technology sometimes seems like magic, especially when we don’t have any idea about how it was done, or we even think it can’t be done at all. When I was a k... Machine Learning Part 2: Types of Learning Updated: September 22, 2017 So here we are again, to continue what we dared to begin: challenging Machine Learning. Machine Learning Part 1: What is Machine Learning? Updated: September 18, 2017 You’ve been hearing about machine learning in the last few years, especially on the day on which AlphaGo was hotter than Donald Trump. You felt excited, you ... Tensorflow Implementation Note: Installing Tensorflow and Keras on Windows Updated: May 09, 2017 Hello everyone, it’s been a long long while, hasn’t it? I was busy fulfilling my job and literally kept away from my blog. But hey, if this takes any longe... model Machine Learning Part 1: What is Machine Learning? Updated: September 18, 2017 You’ve been hearing about machine learning in the last few years, especially on the day on which AlphaGo was hotter than Donald Trump. You felt excited, you ... note What I talk when I talk about Tensorflow Updated: November 02, 2018 Some of my collegues, as well as many of my readers told me that they had problems using Tensorflow for their projects. Something like this: Installing NVIDIA Docker On Ubuntu 16.04 Updated: February 04, 2018 Hey guys, it has been quite a long while since my last blog post (for almost a year, I guess). Today, I am going to tell you about something that I wish I ha... Tensorflow Implementation Note: Installing Tensorflow and Keras on Windows Updated: May 09, 2017 Hello everyone, it’s been a long long while, hasn’t it? I was busy fulfilling my job and literally kept away from my blog. But hey, if this takes any longe... nvidia Installing NVIDIA Docker On Ubuntu 16.04 Updated: February 04, 2018 Hey guys, it has been quite a long while since my last blog post (for almost a year, I guess). Today, I am going to tell you about something that I wish I ha... nvidia-docker Installing NVIDIA Docker On Ubuntu 16.04 Updated: February 04, 2018 Hey guys, it has been quite a long while since my last blog post (for almost a year, I guess). Today, I am going to tell you about something that I wish I ha... object recognition Real Time Object Recognition (Part 2) Updated: September 30, 2017 So here we are again, in the second part of my Real time Object Recognition project. In the previous post, I showed you how to implement pre-trained VGG16 mo... Real Time Object Recognition (Part 1) Updated: September 26, 2017 Technology sometimes seems like magic, especially when we don’t have any idea about how it was done, or we even think it can’t be done at all. When I was a k... overfit Machine Learning Part 5: Underfitting and Overfitting Problems Updated: October 12, 2017 Here we are again, in the fifth post of Machine Learning tutorial series. Today I will talk about two common problems you may face in Machine Learning: Und... overfitting Machine Learning Part 9: Regularization Updated: October 31, 2017 Welcome back to my 9th tutorial on Machine Learning! I have been kept busy in last weekends, struggling in getting my desktop ready for Deep Learning. You ... overfitting dianosic Machine Learning Part 7: Cross Validation Updated: October 24, 2017 Hello there, so I’m here again to bring you the 7th post on my Machine Learning tutorial series. Today I will talk about Cross Validation, a term that you ... own data Training With Your Own Dataset on Caffe Updated: November 09, 2017 Hi, everyone! Welcome back to my Machine Learning page today. I have been playing around with Caffe for a while, and as you already knew, I made a couple o... problems Machine Learning Part 5: Underfitting and Overfitting Problems Updated: October 12, 2017 Here we are again, in the fifth post of Machine Learning tutorial series. Today I will talk about two common problems you may face in Machine Learning: Und... project Preparing AWS’s instance to run Machine Learning’s projects Updated: October 06, 2017 Hello there, it’s been a while since my previous Real time Object Recognition project. I currently have some cool stuff ongoing and I’ll share it to you when... Real Time Object Recognition (Part 2) Updated: September 30, 2017 So here we are again, in the second part of my Real time Object Recognition project. In the previous post, I showed you how to implement pre-trained VGG16 mo... Real Time Object Recognition (Part 1) Updated: September 26, 2017 Technology sometimes seems like magic, especially when we don’t have any idea about how it was done, or we even think it can’t be done at all. When I was a k... python Machine Learning Part 4: Setting Up Python Environment for Computer Vision and Machine Learning Updated: October 08, 2017 So, I am here with you again, in the new post of Machine Learning tutorial series. Next time, I told you about Linear Regression and promised to talk further... real time recognition Real Time Object Recognition (Part 2) Updated: September 30, 2017 So here we are again, in the second part of my Real time Object Recognition project. In the previous post, I showed you how to implement pre-trained VGG16 mo... recurrent neural network Creating A Language Translation Model Using Sequence To Sequence Learning Approach Updated: December 20, 2017 Hello guys. It’s been quite a long while since my last blog post. It may sound like an excuse, but I’ve been struggling with finding a new place to move in... Creating A Text Generator Using Recurrent Neural Network Updated: November 14, 2017 Hello guys, it’s been another while since my last post, and I hope you’re all doing well with your own projects. I’ve been kept busy with my own stuff, too... regression Machine Learning Part 9: Regularization Updated: October 31, 2017 Welcome back to my 9th tutorial on Machine Learning! I have been kept busy in last weekends, struggling in getting my desktop ready for Deep Learning. You ... Machine Learning Part 2: Types of Learning Updated: September 22, 2017 So here we are again, to continue what we dared to begin: challenging Machine Learning. regularization Machine Learning Part 10: Linear Support Vector Machine Updated: November 05, 2017 Hi guys! It’s been while since my last tutorial post about Regularization. And today, as I promised, I’m gonna talk about one supervised learning algorithm... Machine Learning Part 9: Regularization Updated: October 31, 2017 Welcome back to my 9th tutorial on Machine Learning! I have been kept busy in last weekends, struggling in getting my desktop ready for Deep Learning. You ... reinforcement Machine Learning Part 2: Types of Learning Updated: September 22, 2017 So here we are again, to continue what we dared to begin: challenging Machine Learning. seq2seq Creating A Language Translation Model Using Sequence To Sequence Learning Approach Updated: December 20, 2017 Hello guys. It’s been quite a long while since my last blog post. It may sound like an excuse, but I’ve been struggling with finding a new place to move in... stratifiedkfold Machine Learning Part 7: Cross Validation Updated: October 24, 2017 Hello there, so I’m here again to bring you the 7th post on my Machine Learning tutorial series. Today I will talk about Cross Validation, a term that you ... supervised Machine Learning Part 2: Types of Learning Updated: September 22, 2017 So here we are again, to continue what we dared to begin: challenging Machine Learning. support vector machine Machine Learning Part 10: Linear Support Vector Machine Updated: November 05, 2017 Hi guys! It’s been while since my last tutorial post about Regularization. And today, as I promised, I’m gonna talk about one supervised learning algorithm... svm Machine Learning Part 10: Linear Support Vector Machine Updated: November 05, 2017 Hi guys! It’s been while since my last tutorial post about Regularization. And today, as I promised, I’m gonna talk about one supervised learning algorithm... tensorflow What I talk when I talk about Tensorflow Updated: November 02, 2018 Some of my collegues, as well as many of my readers told me that they had problems using Tensorflow for their projects. Something like this: Tensorflow Implementation Note: Installing Tensorflow and Keras on Windows Updated: May 09, 2017 Hello everyone, it’s been a long long while, hasn’t it? I was busy fulfilling my job and literally kept away from my blog. But hey, if this takes any longe... text generator Creating A Text Generator Using Recurrent Neural Network Updated: November 14, 2017 Hello guys, it’s been another while since my last post, and I hope you’re all doing well with your own projects. I’ve been kept busy with my own stuff, too... train_test_split Machine Learning Part 7: Cross Validation Updated: October 24, 2017 Hello there, so I’m here again to bring you the 7th post on my Machine Learning tutorial series. Today I will talk about Cross Validation, a term that you ... training Creating A Language Translation Model Using Sequence To Sequence Learning Approach Updated: December 20, 2017 Hello guys. It’s been quite a long while since my last blog post. It may sound like an excuse, but I’ve been struggling with finding a new place to move in... Creating A Text Generator Using Recurrent Neural Network Updated: November 14, 2017 Hello guys, it’s been another while since my last post, and I hope you’re all doing well with your own projects. I’ve been kept busy with my own stuff, too... Training With Your Own Dataset on Caffe Updated: November 09, 2017 Hi, everyone! Welcome back to my Machine Learning page today. I have been playing around with Caffe for a while, and as you already knew, I made a couple o... Tensorflow Implementation Note: Installing Tensorflow and Keras on Windows Updated: May 09, 2017 Hello everyone, it’s been a long long while, hasn’t it? I was busy fulfilling my job and literally kept away from my blog. But hey, if this takes any longe... translator model Creating A Language Translation Model Using Sequence To Sequence Learning Approach Updated: December 20, 2017 Hello guys. It’s been quite a long while since my last blog post. It may sound like an excuse, but I’ve been struggling with finding a new place to move in... tutorials What I talk when I talk about Tensorflow Updated: November 02, 2018 Some of my collegues, as well as many of my readers told me that they had problems using Tensorflow for their projects. Something like this: underfit Machine Learning Part 5: Underfitting and Overfitting Problems Updated: October 12, 2017 Here we are again, in the fifth post of Machine Learning tutorial series. Today I will talk about two common problems you may face in Machine Learning: Und... unsupervised Machine Learning Part 2: Types of Learning Updated: September 22, 2017 So here we are again, to continue what we dared to begin: challenging Machine Learning.",
    "tags": "",
    "url": "/mahaveer0suthar.github.io/tags/"
  },{
    "title": "Machine Learning Tutorials",
    "text": "{% include base_path %} {% for post in site.posts %} {% if post.categories contains 'Tutorial' %} {% include archive-single.html %} {% endif %} {% endfor %}",
    "tags": "",
    "url": "/mahaveer0suthar.github.io/tutorials/"
  }]};
